{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Llama model's performance across the different datasets reveals interesting patterns in how it handles narrative classification tasks. When trained on the Climate Change (CC) dataset alone, the model achieved its strongest results with a final accuracy of 0.625 and an F1 score of 0.665. This suggests that the model was able to effectively learn and distinguish between different climate change-related narratives.\n",
    "\n",
    "However, when trained on the full combined dataset, which included both climate change and Ukraine-related narratives, the model's performance decreased notably, achieving an accuracy of 0.316 and an F1 score of 0.431. This decline in performance tells us something important about how Llama handles increasing narrative complexity. The lower metrics on the combined dataset likely indicate that the model struggled to maintain clear boundaries between similar narrative types when dealing with a broader context spanning multiple domains.\n",
    "\n",
    "The difference in performance between the focused CC dataset and the combined dataset highlights a fundamental challenge in narrative classification: as the number and variety of possible narratives increase, the task of distinguishing between them becomes exponentially more complex. This is particularly relevant when narratives from different domains might share similar linguistic patterns or rhetorical structures, making it harder for the model to make clean distinctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import torch\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from huggingface_hub import login\n",
    "\n",
    "from model import initialize_model, setup_peft\n",
    "from data_utils import prepare_data, get_predictions_batch, prepare_data_for_model, ensure_model_on_device\n",
    "from trainer import train_model\n",
    "from debug_utils import debug_misclassifications\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training():\n",
    "    try:\n",
    "        # Login to Hugging Face\n",
    "        login('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')\n",
    "\n",
    "        # Check CUDA availability\n",
    "        print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "        # Set paths\n",
    "        def find_repo_root():\n",
    "            current = os.getcwd()\n",
    "            while current != os.path.dirname(current):\n",
    "                if os.path.exists(os.path.join(current, '.git')):\n",
    "                    return current\n",
    "                current = os.path.dirname(current)\n",
    "            raise Exception(\"No .git directory found - repository root could not be determined\")\n",
    "\n",
    "        # Set paths using repository root\n",
    "        repo_root = find_repo_root()\n",
    "        code_path = os.path.join(repo_root, \"code\")\n",
    "        current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "        output_dir = os.path.join(code_path, \"models\", f\"llama_{current_date}\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Load data from code directory\n",
    "        print(\"\\nLoading datasets...\")\n",
    "        print(f\"Repository root: {repo_root}\")\n",
    "        print(f\"Looking for data files in: {code_path}\")\n",
    "        input_file_full = os.path.join(code_path, \"df_normalized.csv\")\n",
    "        input_file_ua = os.path.join(code_path, \"df_normalized_ua.csv\")\n",
    "        input_file_cc = os.path.join(code_path, \"df_normalized_cc.csv\")\n",
    "\n",
    "        df_normalized = pd.read_csv(input_file_full)\n",
    "        df_normalized_ua = pd.read_csv(input_file_ua)\n",
    "        df_normalized_cc = pd.read_csv(input_file_cc)\n",
    "\n",
    "        # Model configuration\n",
    "        model_name = \"openlm-research/open_llama_3b\"\n",
    "        \n",
    "        return {\n",
    "            'output_dir': output_dir,\n",
    "            'current_date': current_date,\n",
    "            'model_name': model_name,\n",
    "            'df_normalized': df_normalized,\n",
    "            'df_normalized_ua': df_normalized_ua,\n",
    "            'df_normalized_cc': df_normalized_cc\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in setup: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        wandb.finish()\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_model(model, dataset, tokenizer, label_mapping, dataset_type=\"Training\"):\n",
    "    \"\"\"Run debug analysis on model predictions\"\"\"\n",
    "    try:\n",
    "        # Set up model and device\n",
    "        model, device = ensure_model_on_device(model)\n",
    "        print(f\"\\nAnalyzing {dataset_type} dataset...\")\n",
    "        \n",
    "        # Prepare texts\n",
    "        texts = dataset['tokens_normalized'].apply(\n",
    "            lambda x: ' '.join(x) if isinstance(x, list) else x\n",
    "        ).tolist()\n",
    "        \n",
    "        true_labels = torch.tensor([\n",
    "            label_mapping[get_narrative_key(eval(n)[0] if isinstance(n, str) else n[0])]\n",
    "            for n in dataset['narrative_subnarrative_pairs']\n",
    "        ]).to(device)\n",
    "        \n",
    "        print(f\"Total samples: {len(texts)}\")\n",
    "        \n",
    "        # Get predictions in batches\n",
    "        batch_size = 8\n",
    "        predictions = []\n",
    "        confidences = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            batch_preds, batch_confs = get_predictions_batch(model, batch_texts, tokenizer, device)\n",
    "            predictions.append(batch_preds)\n",
    "            confidences.append(batch_confs)\n",
    "        \n",
    "        # Concatenate and move to CPU\n",
    "        predictions = torch.cat(predictions).cpu().numpy()\n",
    "        confidences = torch.cat(confidences).cpu().numpy()\n",
    "        true_labels = true_labels.cpu().numpy()\n",
    "        \n",
    "        # Track misclassifications\n",
    "        misclassifications = []\n",
    "        for idx, (pred, true, conf) in enumerate(zip(predictions, true_labels, confidences)):\n",
    "            if pred != true:\n",
    "                misclassifications.append({\n",
    "                    'text': texts[idx][:200],\n",
    "                    'predicted': pred,\n",
    "                    'actual': true,\n",
    "                    'confidence': conf,\n",
    "                    'dataset_type': dataset_type\n",
    "                })\n",
    "        \n",
    "        # Create DataFrame and display results\n",
    "        misclass_df = pd.DataFrame(misclassifications)\n",
    "        print(f\"\\nTotal misclassifications: {len(misclass_df)}\")\n",
    "        print(f\"Accuracy: {1 - len(misclass_df)/len(texts):.4f}\")\n",
    "        \n",
    "        if len(misclass_df) > 0:\n",
    "            print(\"\\nMisclassification distribution:\")\n",
    "            print(misclass_df.groupby(['actual', 'predicted']).size().unstack(fill_value=0))\n",
    "            \n",
    "            print(\"\\nSample misclassifications:\")\n",
    "            for i, row in misclass_df.head().iterrows():\n",
    "                print(f\"\\nExample {i+1}:\")\n",
    "                print(f\"Text: {row['text']}\")\n",
    "                print(f\"Predicted: {row['predicted']}, Actual: {row['actual']}\")\n",
    "                print(f\"Confidence: {row['confidence']:.4f}\")\n",
    "        \n",
    "        return misclass_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in debug analysis: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_dataset(df, model_name, output_dir, current_date, dataset_name):\n",
    "    \"\"\"Train model on a single dataset and return results\"\"\"\n",
    "    try:\n",
    "        # Create dataset-specific output directory\n",
    "        dataset_output_dir = os.path.join(output_dir, f\"{dataset_name}_{current_date}\")\n",
    "        os.makedirs(dataset_output_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nTraining on {dataset_name} dataset...\")\n",
    "        \n",
    "        # Initialize wandb run for this dataset\n",
    "        wandb.init(project=\"llama-classification\", \n",
    "                  name=f\"llama-classification-{dataset_name}-{current_date}\",\n",
    "                  reinit=True)\n",
    "        \n",
    "        # Prepare data\n",
    "        train_dataset, val_dataset, tokenizer, label_mapping, num_labels = prepare_data(\n",
    "            df, model_name, dataset_output_dir\n",
    "        )\n",
    "\n",
    "        # Initialize and setup model\n",
    "        print(\"\\nInitializing model...\")\n",
    "        model = initialize_model(model_name, num_labels)\n",
    "        model = setup_peft(model)\n",
    "\n",
    "        # Train model\n",
    "        trainer = train_model(model, train_dataset, val_dataset, dataset_output_dir, \n",
    "                            current_date, dataset_name)\n",
    "\n",
    "        # Evaluate model\n",
    "        print(\"\\nEvaluating model...\")\n",
    "        results = trainer.evaluate()\n",
    "        \n",
    "        print(f\"\\nEvaluation results for {dataset_name} dataset:\")\n",
    "        for metric, value in results.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"{metric}: {value}\")\n",
    "\n",
    "        # Save model and tokenizer\n",
    "        print(f\"\\nSaving {dataset_name} model...\")\n",
    "        trainer.save_model(dataset_output_dir)\n",
    "        tokenizer.save_pretrained(dataset_output_dir)\n",
    "\n",
    "        # End wandb run\n",
    "        wandb.finish()\n",
    "        \n",
    "        return {\n",
    "            'results': results,\n",
    "            'model': model,\n",
    "            'tokenizer': tokenizer,\n",
    "            'label_mapping': label_mapping,\n",
    "            'trainer': trainer\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in training {dataset_name} dataset: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        wandb.finish()\n",
    "        raise\n",
    "\n",
    "def debug_dataset(df, model, tokenizer, label_mapping, dataset_name):\n",
    "    \"\"\"Debug misclassifications for a specific dataset\"\"\"\n",
    "    try:\n",
    "        print(f\"\\nDebugging {dataset_name} dataset...\")\n",
    "        misclass_df = debug_misclassifications(\n",
    "            df, model, tokenizer, label_mapping, dataset_type=dataset_name\n",
    "        )\n",
    "        \n",
    "        # Save misclassifications to CSV\n",
    "        output_file = f\"misclassifications_{dataset_name}.csv\"\n",
    "        misclass_df.to_csv(output_file, index=False)\n",
    "        print(f\"\\nMisclassification results saved to {output_file}\")\n",
    "        \n",
    "        return misclass_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in debugging {dataset_name} dataset: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "def train_and_debug_all_datasets(setup_dict):\n",
    "    \"\"\"Train and debug models for all datasets\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Train and debug full dataset\n",
    "    print(\"\\n=== Processing Full Dataset ===\")\n",
    "    full_results = train_single_dataset(\n",
    "        setup_dict['df_normalized'],\n",
    "        setup_dict['model_name'],\n",
    "        setup_dict['output_dir'],\n",
    "        setup_dict['current_date'],\n",
    "        'full'\n",
    "    )\n",
    "    full_misclass = debug_dataset(\n",
    "        setup_dict['df_normalized'],\n",
    "        full_results['model'],\n",
    "        full_results['tokenizer'],\n",
    "        full_results['label_mapping'],\n",
    "        'full'\n",
    "    )\n",
    "    results['full'] = {'training': full_results, 'debugging': full_misclass}\n",
    "    \n",
    "    # Train and debug UA dataset\n",
    "    print(\"\\n=== Processing UA Dataset ===\")\n",
    "    ua_results = train_single_dataset(\n",
    "        setup_dict['df_normalized_ua'],\n",
    "        setup_dict['model_name'],\n",
    "        setup_dict['output_dir'],\n",
    "        setup_dict['current_date'],\n",
    "        'ua'\n",
    "    )\n",
    "    ua_misclass = debug_dataset(\n",
    "        setup_dict['df_normalized_ua'],\n",
    "        ua_results['model'],\n",
    "        ua_results['tokenizer'],\n",
    "        ua_results['label_mapping'],\n",
    "        'ua'\n",
    "    )\n",
    "    results['ua'] = {'training': ua_results, 'debugging': ua_misclass}\n",
    "    \n",
    "    # Train and debug CC dataset\n",
    "    print(\"\\n=== Processing CC Dataset ===\")\n",
    "    cc_results = train_single_dataset(\n",
    "        setup_dict['df_normalized_cc'],\n",
    "        setup_dict['model_name'],\n",
    "        setup_dict['output_dir'],\n",
    "        setup_dict['current_date'],\n",
    "        'cc'\n",
    "    )\n",
    "    cc_misclass = debug_dataset(\n",
    "        setup_dict['df_normalized_cc'],\n",
    "        cc_results['model'],\n",
    "        cc_results['tokenizer'],\n",
    "        cc_results['label_mapping'],\n",
    "        'cc'\n",
    "    )\n",
    "    results['cc'] = {'training': cc_results, 'debugging': cc_misclass}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_dict = setup_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua_results = train_single_dataset(\n",
    "    setup_dict['df_normalized_ua'],\n",
    "    setup_dict['model_name'],\n",
    "    setup_dict['output_dir'],\n",
    "    setup_dict['current_date'],\n",
    "    'ua'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua_misclass = debug_dataset(\n",
    "    setup_dict['df_normalized_ua'],\n",
    "    ua_results['model'],\n",
    "    ua_results['tokenizer'],\n",
    "    ua_results['label_mapping'],\n",
    "    'ua'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_results, cc_model, cc_tokenizer, cc_label_mapping = train_cc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train full dataset\n",
    "results, model, tokenizer, label_mapping = train_full()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
