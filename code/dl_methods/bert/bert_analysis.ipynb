{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# BERT Training for Narrative Classification\n",
    "\n",
    "This notebook demonstrates how to train a BERT model for narrative classification using the modular code structure."
   ],
   "id": "747bf415d51cf63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "# Import our custom modules\n",
    "from dataset import prepare_data, CustomDataset\n",
    "from model import predict, initialize_model\n",
    "from trainer import train_bert\n",
    "from modules.utils import debug_misclassifications, setup_logging"
   ],
   "id": "a9b0a8f8447bcd24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "setup_logging()\n",
    "# Create logs directory if it doesn't exist\n",
    "logs_dir = os.path.join(os.getcwd(), \"code\", \"logs\")\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "# Setup logging with specified directory\n",
    "log_filename = os.path.join(\n",
    "    logs_dir, f\"preprocessing_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    ")\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_filename), logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "\n",
    "# Get logger\n",
    "logger = logging.getLogger(__name__)"
   ],
   "id": "eeabde8a9fa0badf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Processing Summary Class\n",
    "\n",
    "First, let's define our ProcessingSummary class to track and display results."
   ],
   "id": "2c7ab501b4838871"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ProcessingSummary:\n",
    "    def __init__(self):\n",
    "        self.start_time = datetime.now()\n",
    "        self.steps_completed = []\n",
    "        self.ml_results = {}\n",
    "        self.document_stats = {}\n",
    "\n",
    "    def add_step(self, step_name, details=None):\n",
    "        step = {\n",
    "            \"name\": step_name,\n",
    "            \"timestamp\": datetime.now().strftime(\"%H:%M:%S\"),\n",
    "            \"details\": details,\n",
    "        }\n",
    "        self.steps_completed.append(step)\n",
    "\n",
    "    def add_ml_result(self, model_type, metrics):\n",
    "        self.ml_results[model_type] = metrics\n",
    "\n",
    "    def display_summary(self):\n",
    "        duration = datetime.now() - self.start_time\n",
    "        minutes = int(duration.total_seconds() // 60)\n",
    "        seconds = int(duration.total_seconds() % 60)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"{'PROCESSING SUMMARY':^80}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        print(\"\\nGENERAL INFORMATION\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Total Processing Time: {minutes}m {seconds}s\")\n",
    "        print(f\"Steps Completed: {len(self.steps_completed)}\")\n",
    "\n",
    "        print(\"\\nDOCUMENT STATISTICS\")\n",
    "        print(\"-\" * 80)\n",
    "        for key, value in self.document_stats.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"{key.replace('_', ' ').title()}: {value:.2f}\")\n",
    "            else:\n",
    "                print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "        print(\"\\nPROCESSING TIMELINE\")\n",
    "        print(\"-\" * 80)\n",
    "        for step in self.steps_completed:\n",
    "            print(f\"\\n[{step['timestamp']}] {step['name']}\")\n",
    "            if step.get(\"details\"):\n",
    "                for key, value in step[\"details\"].items():\n",
    "                    print(f\"  └─ {key}: {value}\")\n",
    "\n",
    "        if self.ml_results:\n",
    "            print(\"\\nML RESULTS\")\n",
    "            print(\"-\" * 80)\n",
    "            for model, metrics in self.ml_results.items():\n",
    "                print(f\"\\n{model}:\")\n",
    "                for metric, value in metrics.items():\n",
    "                    if isinstance(value, float):\n",
    "                        print(f\"  └─ {metric}: {value:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"  └─ {metric}: {value}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "\n",
    "# Initialize processing summary\n",
    "summary = ProcessingSummary()"
   ],
   "id": "ec9a1e39c867961b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Set Up Paths and Load Data\n",
    "\n",
    "Define paths and load the preprocessed data files."
   ],
   "id": "955458f3b49dfdca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define paths\n",
    "base_path = os.path.join(os.getcwd(), \"..\", \"..\")\n",
    "output_dir = os.path.join(base_path, \"outputs\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load preprocessed data\n",
    "try:\n",
    "    df_normalized = pd.read_csv(os.path.join(base_path, \"df_normalized.csv\"))\n",
    "    df_normalized_ua = pd.read_csv(os.path.join(base_path, \"df_normalized_ua.csv\"))\n",
    "    df_normalized_cc = pd.read_csv(os.path.join(base_path, \"df_normalized_cc.csv\"))\n",
    "\n",
    "    # Update summary\n",
    "    summary.document_stats = {\n",
    "        \"total_documents\": len(df_normalized),\n",
    "        \"ua_documents\": len(df_normalized_ua),\n",
    "        \"cc_documents\": len(df_normalized_cc),\n",
    "    }\n",
    "\n",
    "    summary.add_step(\n",
    "        \"Data Loading\",\n",
    "        {\n",
    "            \"total_documents\": len(df_normalized),\n",
    "            \"ua_documents\": len(df_normalized_ua),\n",
    "            \"cc_documents\": len(df_normalized_cc),\n",
    "        },\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading data: {str(e)}\")\n",
    "    raise"
   ],
   "id": "d140a69960ad1492"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training Options\n",
    "\n",
    "Define functions to handle different training scenarios."
   ],
   "id": "24c076a62260a746"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_ml_choice():\n",
    "    print(\"\\nSelect processing option:\")\n",
    "    print(\"1. Train BERT on all data\")\n",
    "    print(\"2. Train BERT on UA data only\")\n",
    "    print(\"3. Train BERT on CC data only\")\n",
    "    print(\"4. Run all BERT training variations\")\n",
    "    print(\"5. Skip training\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"\\nEnter your choice (1-5): \"))\n",
    "            if 1 <= choice <= 5:\n",
    "                return choice\n",
    "            print(\"Please enter a number between 1 and 5.\")\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number.\")\n",
    "\n",
    "\n",
    "def run_selected_ml(\n",
    "    choice, df_normalized, df_normalized_ua, df_normalized_cc, base_path, summary\n",
    "):\n",
    "    if choice == 1:\n",
    "        logger.info(\"Starting BERT training on full dataset...\")\n",
    "        training_results = train_bert(df_normalized, base_path)\n",
    "        summary.add_ml_result(\"BERT (Full Dataset)\", training_results)\n",
    "        logger.info(\n",
    "            f\"BERT training on full data completed. Results: {training_results}\"\n",
    "        )\n",
    "\n",
    "    elif choice == 2:\n",
    "        logger.info(\"Starting BERT training on UA dataset...\")\n",
    "        training_results_ua = train_bert(df_normalized_ua, base_path)\n",
    "        summary.add_ml_result(\"BERT (UA Dataset)\", training_results_ua)\n",
    "        logger.info(\n",
    "            f\"BERT training on UA data completed. Results: {training_results_ua}\"\n",
    "        )\n",
    "    elif choice == 3:\n",
    "        logger.info(\"Starting BERT training on CC dataset...\")\n",
    "        training_results_cc = train_bert(df_normalized_cc, base_path)\n",
    "        summary.add_ml_result(\"BERT (CC Dataset)\", training_results_cc)\n",
    "        logger.info(\n",
    "            f\"BERT training on CC data completed. Results: {training_results_cc}\"\n",
    "        )\n",
    "\n",
    "    elif choice == 4:\n",
    "        logger.info(\"Starting BERT training on all variations...\")\n",
    "\n",
    "        training_results = train_bert(df_normalized, base_path)\n",
    "        summary.add_ml_result(\"BERT (Full Dataset)\", training_results)\n",
    "        logger.info(\n",
    "            f\"BERT training on full data completed. Results: {training_results}\"\n",
    "        )\n",
    "\n",
    "        training_results_ua = train_bert(df_normalized_ua, base_path)\n",
    "        summary.add_ml_result(\"BERT (UA Dataset)\", training_results_ua)\n",
    "        logger.info(\n",
    "            f\"BERT training on UA data completed. Results: {training_results_ua}\"\n",
    "        )\n",
    "\n",
    "        training_results_cc = train_bert(df_normalized_cc, base_path)\n",
    "        summary.add_ml_result(\"BERT (CC Dataset)\", training_results_cc)\n",
    "        logger.info(\n",
    "            f\"BERT training on CC data completed. Results: {training_results_cc}\"\n",
    "        )"
   ],
   "id": "7c3b064f91545006"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model Analysis and Debugging\n",
    "\n",
    "Analyze model performance and debug misclassifications."
   ],
   "id": "ad721ba2a8daa8f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get the path to the latest trained model\n",
    "current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "model_path = os.path.join(base_path, f\"models/bert_20250113\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "from model import load_model_and_tokenizer\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(model_path)\n",
    "\n",
    "# Load label mapping\n",
    "import json\n",
    "\n",
    "with open(os.path.join(model_path, \"label_mapping.json\"), \"r\") as f:\n",
    "    label_mapping = json.load(f)\n",
    "\n",
    "# Analyze misclassifications\n",
    "misclassifications = debug_misclassifications(\n",
    "    dataset=df_normalized,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    label_mapping=label_mapping,\n",
    "    dataset_type=\"Training\",\n",
    ")\n",
    "\n",
    "# Save misclassification analysis\n",
    "misclassifications_path = os.path.join(\n",
    "    output_dir, f\"analysis/misclassifications_{current_date}.csv\"\n",
    ")\n",
    "os.makedirs(os.path.dirname(misclassifications_path), exist_ok=True)\n",
    "misclassifications.to_csv(misclassifications_path, index=False)\n",
    "\n",
    "print(\"\\nMisclassified Examples:\")\n",
    "display(misclassifications)\n",
    "print(f\"\\nAnalysis saved to: {misclassifications_path}\")"
   ],
   "id": "2329dd04ee091a20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Description of Deep Leaning baseline and used methods\n",
    "\n",
    "As Deep Learning model we used BERT. We have trained the model using labeled data and measured the performance of the model using the following metrics:\n",
    "* Accuracy\n",
    "* Recall,\n",
    "* Precision\n",
    "* F1 Score\n",
    "\n",
    "Before training the model, we have normalized and tokenized the data.\n",
    "1. Normalization and tokenization: Cleaning and tokenizing the narratives, so that the model can be trained.\n",
    "2. Label mapping: Creating a mapping between all unique class labels and integers to meet BERTs' requirements for training\n",
    "\n",
    "For the models training and testing, we used a 80% training / 20% testing split.\n",
    "By splitting the narratives, we followed different approaches to be able to evaluate the differences between them:\n",
    "* Handling both, Ukraine War and Climate Change narratives in one dataframe, so that the model could learn from all the data\n",
    "* Splitting narratives into two dataframes, containing only Ukraine War narratives or only Climate Change narratives each\n",
    "* Using stratification to improve the distribution of classes between training and testing sets."
   ],
   "id": "843dcb6da5f5cac4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Analysis\n",
    "\n",
    "When the narratives were split by topic, the metrics for Climate Change improved, while the metrics for the Ukraine War worsened.\n",
    "Having all metrics of 0.5135 during the approach with all dataframes at the same time, it is a moderate performance.\n",
    "\n",
    "After that, we used only Climate Change narratives and all resulting metrics got much better: 0.7143.\n",
    "Using only Ukraine War narratives, we received 0.4167 for all metrics, which is the worst result, although it is the topic with the highest amount of narratives that we had for training.\n",
    "\n",
    "### Analyzing single classes\n",
    "\n",
    "To be able to analyze differences in prediction of different classes, we analyzed the confusion matrices for each class individually.\n",
    "\n",
    "For both topics, we received a similar distribution for the confusion matrix of the class *Other*. Again for both topics, the model predicted class \"Other\" correctly as positive. It also predicted other classes to be from class *Other*."
   ],
   "id": "10d44eb3ee460a47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images = [\"cc_other.png\", \"cc_all.png\", \"ua_other.png\", \"ua_all.png\"]\n",
    "labels = [\n",
    "    \"Climate Change Class Other\",\n",
    "    \"Climate Change all remaining classes\",\n",
    "    \"Ukraine War class Other\",\n",
    "    \"Ukraine War all remaining classes\",\n",
    "]\n",
    "img_base_path = \"../../../info/screens/\"\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "for i, (img_name, label) in enumerate(zip(images, labels)):\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    img_path = img_base_path + \"/\" + img_name\n",
    "    img = plt.imread(img_path)\n",
    "    ax[row, col].imshow(img)\n",
    "    ax[row, col].axis(\"off\")\n",
    "    ax[row, col].set_title(label)\n",
    "plt.show()"
   ],
   "id": "e293fb94c42bd50a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Climate Change\n",
    "\n",
    "For Climate Change narratives, we received for class *Other* the following Confusion Matrix:\n",
    "[[0, 4], [0, 10]]\n",
    "\n",
    "Calculating the metrics for this specific class:\n",
    "* Accuracy: 71,43 %\n",
    "* Precision: 71,43 %\n",
    "* Recall: 100%\n",
    "* F1 Score: 83,33 %\n",
    "\n",
    "For all other classes, we received the following equal Confusion Matrix:\n",
    "[[13, 0], [1, 0]]\n",
    "* Accuracy: 92,86 %\n",
    "* Precision: 0 %\n",
    "* Recall: 0%\n",
    "* F1 Score: 0 %\n",
    "\n",
    "\n",
    "#### Ukraine War\n",
    "\n",
    "For Ukraine War narratives, we received for class *Other* the following Confusion Matrix:\n",
    "[[0, 14], [0, 10]]\n",
    "\n",
    "Calculating the metrics for this specific class:\n",
    "* Accuracy: 41,7 %\n",
    "* Precision: 41,7 %\n",
    "* Recall: 100%\n",
    "* F1 Score: 58,8%\n",
    "\n",
    "For all other classes, we received the following equal Confusion Matrix:\n",
    "[[23, 0], [1, 0]]\n",
    "* Accuracy: 95,8%\n",
    "* Precision: 0%\n",
    "* Recall: 0%\n",
    "* F1 Score: 0%\n",
    "\n",
    "\n",
    "For both approaches, we can see that all classes except of *Other* have no counts for *True Positives*, but a very high count for *True Negatives*. On the hand, we have the class *Other*, where all samples of class *Other* are classified correctly as *Other*, but still we have almost as many or even more narratives that were classified as *Other*, although they had a different class.\n",
    "\n",
    "From those unequally distributed results we see that BERTs' prediction performance is best for class *Other* and no other class.\n",
    "\n",
    "An issue that could cause that, can be a data imbalance. The class *Other* occurs most frequently in the dataset. From that, the model could have learned to predict especially this class mostly accurately, while it fails for all other classes.\n",
    "\n",
    "## Possible solutions\n",
    "Although we have used stratification to receive a better distribution of all classes, there are still some solutions that we should consider in the next part.\n",
    "\n",
    "Apperently, the class *Other* is overrepresented in the data set. Since we only had less than 200 narratives available, one possible solution to improve the predictions is to use more data.\n",
    "In the context of this task we also have narratives in other languages available that we can use to handle the imbalance.Having more data.\n",
    "\n",
    "The issue could also be that  the class labeling was not carried out cleanly, so that we have the broad class *Other*, where most of the narratives belong to. Specifying the classes more accurate could help to balance the dataset.\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "d7c7bfc9fa92e30b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Qualitative Analysis\n",
    "\n",
    "While finishing up our project, we accidentally deleted one of the result runs from our BERT model. Because of this, we can’t reproduce the predictions the model originally made, which were the basis for this qualitative analysis.\n",
    "\n",
    "This notebook contains our qualitative analysis, and all the outputs are already included in the markdown and code cells exactly as they were when we first ran it. Thus, rerunning the notebook is not possible!\n",
    "\n",
    "Since we had the freedom to choose the format for this analysis, we think this shouldn’t be a big issue as long as the outputs are left as they are."
   ],
   "id": "427ef8894d3faff7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "base_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "label_directory = os.path.join(\n",
    "    base_path, \"models\", \"bert_20250113\", \"label_mapping.json\"\n",
    ")\n",
    "input_file_full = os.path.join(base_path, \"predicted_dataframe.csv\")\n",
    "dataset = pd.read_csv(input_file_full)"
   ],
   "id": "7547c8826923126f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "texts, labels, label_mapping = prepare_data(dataset)\n",
    "print(f\"Sample text: {texts[:3]}\")\n",
    "print(f\"Sample label: {labels[:3]}\")\n",
    "\n",
    "training_results = train_bert(dataset, base_path)\n",
    "print(f\"Training Results: {training_results}\")"
   ],
   "id": "c82f3222049ceb2a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tqdm.pandas()\n",
    "model_path = os.path.join(base_path, \"models\\bert_20250113\")\n",
    "\n",
    "\n",
    "# Preparing predictions\n",
    "def classify_row(row):\n",
    "    try:\n",
    "        predicted_label, _ = predict(row[\"tokens_normalized\"], model_path)\n",
    "        return predicted_label\n",
    "    except Exception as e:\n",
    "        print(f\"Error for row {row.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Predicting\n",
    "dataset[\"predicted_narrative\"] = dataset.progress_apply(classify_row, axis=1)\n",
    "# dataset.to_csv(os.path.join(base_path, \"predicted_dataframe.csv\"), index=False)"
   ],
   "id": "9fa8dbb515571850"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dataset = pd.read_csv(os.path.join(base_path, \"predicted_dataframe.csv\"))\n",
    "dataset.head()"
   ],
   "id": "7d067750a442889"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here, we will pick one class label and analyze why some articles are getting predicted and especially why most of\n",
    "the news articles that are actually assigned to that class are not predicted to be in this class.\n",
    "\n",
    "We will again choose one class for qualitative analysis which has at least one *True Positive* and some *False Negatives* in English and Russian both, that we can compare with each other.\n",
    "\n",
    "### English"
   ],
   "id": "4c81d8dbfc98e6e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "english_dataset = dataset[dataset[\"language\"] == \"EN\"]\n",
    "russian_dataset = dataset[dataset[\"language\"] == \"RU\"]\n",
    "\n",
    "\n",
    "# Backparse both narrative columns\n",
    "def parse_and_normalize(column):\n",
    "    \"\"\"Parse and normalize a column containing dictionaries stored as strings.\"\"\"\n",
    "    return column.apply(\n",
    "        lambda x: (\n",
    "            {k: v for k, v in sorted(ast.literal_eval(x).items())}\n",
    "            if isinstance(x, str)\n",
    "            else x\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "english_dataset.loc[:, \"temp_narrative\"] = parse_and_normalize(\n",
    "    english_dataset[\"temp_narrative\"]\n",
    ")\n",
    "english_dataset.loc[:, \"predicted_narrative\"] = parse_and_normalize(\n",
    "    english_dataset[\"predicted_narrative\"]\n",
    ")\n",
    "\n",
    "# Getting unique classes from both narrative columns\n",
    "all_classes = set(\n",
    "    tuple(sorted(d.items())) for d in english_dataset[\"temp_narrative\"]\n",
    ") | set(tuple(sorted(d.items())) for d in english_dataset[\"predicted_narrative\"])\n",
    "\n",
    "class_summary_list = []\n",
    "\n",
    "# Creating binary labels\n",
    "for target_class in all_classes:\n",
    "    y_true = (\n",
    "        english_dataset[\"temp_narrative\"]\n",
    "        .apply(lambda x: tuple(sorted(x.items())) == target_class)\n",
    "        .astype(int)\n",
    "    )\n",
    "    y_pred = (\n",
    "        english_dataset[\"predicted_narrative\"]\n",
    "        .apply(lambda x: tuple(sorted(x.items())) == target_class)\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "    # Calculating CMs\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tp, fn, fp, tn = (\n",
    "        cm.ravel() if cm.size == 4 else (cm[1, 1], cm[1, 0], cm[0, 1], cm[0, 0])\n",
    "    )\n",
    "\n",
    "    class_summary_list.append(\n",
    "        {\"target_class\": dict(target_class), \"TP\": tp, \"FN\": fn, \"FP\": fp, \"TN\": tn}\n",
    "    )\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(\n",
    "        confusion_matrix=cm, display_labels=[\"Negative\", \"Positive\"]\n",
    "    )\n",
    "    disp.plot(cmap=\"Blues\")\n",
    "    plt.title(f\"Confusion Matrix for Target Class: {dict(target_class)}\")\n",
    "    plt.show()"
   ],
   "id": "8f418c9dfbf4ccf1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We will choose class *narrative: Hidden plots by secret schemes of powerful groups, subnarrative: Climate agenda has hidden motives* for the qualitative analysis of english.\n",
    "\n",
    "We will print out the needed columns of the texts that were predicted with a different label, although they had the label mentioned above. Also, we will print news articles where the model predicted the label correctly to inspect those outputs."
   ],
   "id": "6394234a9092dc7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "analyzing_class = {\n",
    "    \"narrative\": \"Hidden plots by secret schemes of powerful groups\",\n",
    "    \"subnarrative\": \"Climate agenda has hidden motives\",\n",
    "}\n",
    "\n",
    "# Filtering TPs\n",
    "true_positive_english = english_dataset[\n",
    "    (english_dataset[\"predicted_narrative\"] == analyzing_class)\n",
    "    & (english_dataset[\"temp_narrative\"] == analyzing_class)\n",
    "]\n",
    "\n",
    "# Filtering FNs\n",
    "false_negatives_english = english_dataset[\n",
    "    (english_dataset[\"predicted_narrative\"] != analyzing_class)\n",
    "    & (english_dataset[\"temp_narrative\"] == analyzing_class)\n",
    "]\n",
    "\n",
    "# Index for TP or FN\n",
    "print(\"True Positive Indices:\")\n",
    "print(true_positive_english.index.tolist())\n",
    "print(\"\\nFalse Negative Indices:\")\n",
    "print(false_negatives_english.index.tolist())"
   ],
   "id": "b021c9642def01a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### True positive content:",
   "id": "ed5ec8f2429d889a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(true_positive_english.loc[153, \"content\"])",
   "id": "6b9ed16b7fedae04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Reading through this news article, we can see that there are two sentences in the beginning, where the author already starts to point to the direction, the new regulations being met \"under the guise of fighting 'climate change'\". He also says that most dishwasers that are sold today already align with the planned regulations and that the government should focus on other topics, the meaning of the direction the text is focused to gets clear. Those two cases have probably led to the correct assumption, that there are hidden plots by powerful groups (the government).\n",
    "Having the text part \"under the guise\" likely led to the assumption that the climate agenda has hidden motives.\n",
    "\n",
    "We can assume that the texts the model was trained with contained words like \"hidden\", \"climate agenda\" or \"regime\", because those words are all contained in the news article that was predicted correctly by the model. Those words were also used to hold information which likely led the label human classify the text under that label.\n",
    "\n",
    "Further, the sentence, that \"Critics have also poked major holes in the regime´s claim [...]\" also could contribute to the fact that the model predicted especially this narrative-subnarrative pair, as the governments attempt to reduce energy consumption is getting criticized.\n",
    "\n"
   ],
   "id": "c9b53602881e61c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### False negative content:",
   "id": "8fd4b8d0b9ef7c15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(false_negatives_english.loc[104, \"content\"])",
   "id": "ca26152e2babe907"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(false_negatives_english.loc[293, \"content\"])",
   "id": "228c85f201604510"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the first article, there are some sentences that indicate the topic *Climate Change* slightly. Especially the sentence saying \"Caroline van der Plas founded [the movement]\" and the following part about \"nitrogen hoax\", \"climate change lies\" and \"[...] buy up most of the farmers\" should lead the model to predict the actual label, as those words and sentences shall be likely been used in other news articles with that true label for training.\n",
    "\n",
    "Nevertheless, the second part of the article starts getting religious with text parts like \"Accept Jesus Christ as our saviour\" or \"Read the bible, fast and pray\" or \"Amen\". Those words and especially this topic and tone of writing does not anything contribute to the true label. Thus, this drift off the climate change topic could lead the model to assume the text to belong toa different label, especially if there were some religious texts that were predicted with Other-Other or a different label, where religious topics would fit into the label.\n",
    "\n",
    "The second article contains little Climate Change information throughout the whole text, which is understandable if one knows what kind of meetup was in Dubai and why influential people are joining it. Parts like \"Globalist oligarchs met [...]\" or \"Unelected Bond villains who want to decide our fate.\" and \"Mega Rich Elite [...]\" tries to frame the politicians as a powerful group in a pejorative way.\n",
    "Still, the model did not predict this news article as the true label. One reason for that could be, that the article is written in a sarcastic and pejorative way, which could lead the model to predict the label Other-Other, because the climate change content of the text is a bit hidden in political and social concepts that need to be understood."
   ],
   "id": "485dcf1c1cbe2e9c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
