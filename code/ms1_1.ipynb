{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Natural Language Processing Milestone 1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Content\n",
    "1. [Initial Data prep and label representation](#initial-data-prep-and-label-representation)\n",
    "    - [Create a first dataframe](#creating-a-first-dataframe)\n",
    "    - [Extract the label taxonomy](#extract-the-label-taxonomy)\n",
    "    - [Map the labels to the taxonomy](#map-the-labels-in-the-file-to-the-taxonomy)\n",
    "2. [Text segmentation](#text-segmentation)\n",
    "    - [Tokenize sentences and words](#tokenize-sentences-and-words)\n",
    "    - [Find sentences of unusual length](#find-sentences-unusual-length)\n",
    "    - [Handle very short sentences](#handle-short-sentences)\n",
    "    - [Handle very long sentences](#handle-long-sentences)\n",
    "    - [Verify remaining sentences of unusual length](#verify-remaining-unusual-sentences)\n",
    "3. [Text Normalization](#text-normalization)\n",
    "    - [Verify text normalization](#check-normalization)\n",
    "    - [Print the results of the text normalization](#result-printing)\n",
    "4. [CoNNL-U format](#connlu-format)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T22:18:47.324128Z",
     "start_time": "2024-12-11T22:18:17.487628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#%pip install --requirement requirements.txt\n",
    "# When you import something new that is not in the requirements.txt file, please add it to the requirements.txt file and re-run the cell above.\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "from stanza.utils.conll import CoNLL\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "import stanza\n",
    "import torch\n",
    "import nltk\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "##%%capture"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asus9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initial Data Prep and label representation"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Creating a first dataframe\n",
    "containing [filename, content, narrative, subnarrative, topic] for each datapoint\n",
    "- The documents belong to two different topics, ukraine war and climate change\n",
    "- We will make it clear in the dataframe which topic each document belongs to by adding the column \"topic\", which is based on the abbreviations UA or CC in the document filenames\n",
    "    - The reason for this is the implementation decision we made to **build separate models to predict the two topics** instead of one that can predict both"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The dataset has **two ways of categorizing articles** into topics:\n",
    "- The above-mentioned UA and CC in the article files\n",
    "- URW and CC in the label strings which classifies every label for an article (since there can be multiple) \n",
    "    - This means, that theoretically, an article could contain both topics\n",
    "    - However, we will see that almost all datapoints only have labels for the topic that is also in the corresponding article filenames (i.e. if there is CC in the article filename, for almost all datapoints, there would also only be CC labels)\n",
    "    - More details that drove the decision to use the article file topic instead of the label topics are provided further below"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the paths to articles and annotations\n",
    "documents_path = \"../training_data_16_October_release/EN/raw-documents\"\n",
    "annotations_file = \"../training_data_16_October_release/EN/subtask-2-annotations.txt\"\n",
    "\n",
    "# Read the annotations file\n",
    "annotations = pd.read_csv(annotations_file, sep='\\t', header=None, names=['filename', 'narrative', 'subnarrative'])\n",
    "\n",
    "# Remove all occurrences of \"CC: \" and \"URW: \" from narratives and subnarratives\n",
    "annotations['narrative'] = annotations['narrative'].str.replace(r'(CC: |URW: )', '', regex=True)\n",
    "annotations['subnarrative'] = annotations['subnarrative'].str.replace(r'(CC: |URW: )', '', regex=True)\n",
    "\n",
    "# Split the narratives and subnarratives into lists\n",
    "annotations['narrative'] = annotations['narrative'].str.split(';')\n",
    "annotations['subnarrative'] = annotations['subnarrative'].str.split(';')\n",
    "\n",
    "# Initialize a list to store the data\n",
    "data = []\n",
    "\n",
    "# Iterate over the annotations and read the corresponding documents\n",
    "for _, row in annotations.iterrows():\n",
    "    filename = row['filename']\n",
    "    narratives = row['narrative']\n",
    "    subnarratives = row['subnarrative']\n",
    "    \n",
    "    # Read the document content\n",
    "    with open(os.path.join(documents_path, filename), 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Determine the topic based on the filename\n",
    "    topic = \"UA\" if \"UA\" in filename else \"CC\"\n",
    "    \n",
    "    # Append the document content, narratives, subnarratives, and topic to the data list\n",
    "    data.append({\n",
    "        'filename': filename,\n",
    "        'content': content,\n",
    "        'narratives': narratives,\n",
    "        'subnarratives': subnarratives,\n",
    "        'topic': topic\n",
    "    })\n",
    "\n",
    "# Convert the data list to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- There seems to be redundant information in the narratives and subnarratives column, since entries in the subnarrative column is structured in the way *\"narrative:subnarrative\"* -> we could therefore get rid of some redundant information\n",
    "- We first check if the narrative information in the subnarrative column is exactly the same as in the narrative column\n",
    "- If so, we will remove the narrative information from the subnarrative column"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define a function to check if the subnarrative starts with the narrative in a given row\n",
    "def check_pattern(row):\n",
    "    narratives = row['narratives']\n",
    "    subnarratives = row['subnarratives']\n",
    "    \n",
    "    for narrative, subnarrative in zip(narratives, subnarratives):\n",
    "        if not subnarrative.startswith(narrative + \":\"):\n",
    "            return (row.name, narrative, subnarrative)\n",
    "    return None\n",
    "\n",
    "# Apply the function to each row and collect the results\n",
    "pattern_check_results = df.apply(check_pattern, axis=1)\n",
    "\n",
    "# Filter out the rows where the pattern does not hold\n",
    "problematic_rows = pattern_check_results[pattern_check_results.notnull()]\n",
    "\n",
    "# Set display options to avoid truncation\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Display the problematic rows\n",
    "print(\"Problematic rows where the pattern does not hold:\")\n",
    "print(problematic_rows)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- The only exception from the pattern is *narrative other, subnarrative other* cases \n",
    "- Since there is no redundancy in those cases and there are no other exceptions from the pattern, we can delete the narratives before the \":\" in the subnarratives column"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to remove redundant narratives from subnarratives\n",
    "def remove_redundant_narratives(row):\n",
    "    narratives = row['narratives']\n",
    "    subnarratives = row['subnarratives']\n",
    "    \n",
    "    cleaned_subnarratives = []\n",
    "    for narrative, subnarrative in zip(narratives, subnarratives):\n",
    "        if subnarrative.startswith(narrative + \":\"):\n",
    "            cleaned_subnarrative = subnarrative[len(narrative) + 1:].strip()\n",
    "            cleaned_subnarratives.append(cleaned_subnarrative)\n",
    "        else:\n",
    "            cleaned_subnarratives.append(subnarrative)\n",
    "    \n",
    "    return cleaned_subnarratives\n",
    "\n",
    "# Apply the function to each row to clean the subnarratives\n",
    "df['subnarratives'] = df.apply(remove_redundant_narratives, axis=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(\"Updated DataFrame with cleaned subnarratives:\")\n",
    "df.head()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Currently, a subnarrative is assigned to a narrative by the order in the list in the respective column\n",
    "- We change this to a better assignment using lists containing dictionaries where each contains a \"narrative\" and \"subnarrative\" key\n",
    "- After this, we only have a single column for the labels \"narrative_subnarrative_pairs\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to create narrative-subnarrative pairs in a single column\n",
    "def create_narrative_subnarrative_pairs(row):\n",
    "    narratives = row['narratives']\n",
    "    subnarratives = row['subnarratives']\n",
    "    \n",
    "    pairs = []\n",
    "    for narrative, subnarrative in zip(narratives, subnarratives):\n",
    "        if subnarrative.startswith(narrative + \":\"):\n",
    "            cleaned_subnarrative = subnarrative[len(narrative) + 1:].strip()\n",
    "        else:\n",
    "            cleaned_subnarrative = subnarrative\n",
    "        pairs.append({'narrative': narrative, 'subnarrative': cleaned_subnarrative})\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Apply the function to each row to create narrative-subnarrative pairs\n",
    "df['narrative_subnarrative_pairs'] = df.apply(create_narrative_subnarrative_pairs, axis=1)\n",
    "\n",
    "# Drop the original narratives and subnarratives columns if no longer needed\n",
    "df = df.drop(columns=['narratives', 'subnarratives'])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(\"Updated DataFrame with narrative-subnarrative pairs:\")\n",
    "df.head()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Extract the label taxonomy\n",
    "- The competition creators provide a complete taxonomy of labels for each of the two topics\n",
    "- We create a templete for each label taxonomy (for each of the two topics) from the subtask 2 pdf file\n",
    "- We can use this to encode the labels of the datapoints in the dataset\n",
    "- We do this by assigning an index to every possible class (narrative-subnarrative pair) to numerically represent the labels for each document"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Define the path to the PDF file\n",
    "pdf_path = \"../info/subtask2_NARRATIVE-TAXONOMIES.pdf\"\n",
    "\n",
    "# Open the PDF file\n",
    "pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "# Function to extract text from a specific page\n",
    "def extract_text_from_page(page_number):\n",
    "    page = pdf_document.load_page(page_number)\n",
    "    text = page.get_text(\"text\")\n",
    "    return text\n",
    "\n",
    "# Extract text from the relevant pages\n",
    "ukraine_war_text = extract_text_from_page(0)  # First page contains Ukraine War taxonomy\n",
    "climate_change_text = extract_text_from_page(1)  # Second page contains Climate Change taxonomy\n",
    "\n",
    "# Function to parse the taxonomy text and create a DataFrame\n",
    "def parse_taxonomy(text):\n",
    "    lines = text.split('\\n')\n",
    "    # Exclude the last two lines\n",
    "    lines = lines[:-3]\n",
    "    data = []\n",
    "    current_narrative = None\n",
    "    for line in lines:\n",
    "        if line.strip() == \"\":\n",
    "            continue\n",
    "        if line.startswith(\"-\"):  # Subnarrative\n",
    "            subnarrative = line.strip(\"- \").strip()\n",
    "            data.append({'narrative': current_narrative, 'subnarrative': subnarrative})\n",
    "        else:  # Narrative\n",
    "            if current_narrative and not any(d['narrative'] == current_narrative for d in data):\n",
    "                # Add the narrative itself as subnarrative if it has no subnarratives\n",
    "                data.append({'narrative': current_narrative, 'subnarrative': current_narrative})\n",
    "            current_narrative = line.strip()\n",
    "            if current_narrative == \"Other\":\n",
    "                data.append({'narrative': \"Other\", 'subnarrative': \"Other\"})\n",
    "    # Handle the last narrative if it has no subnarratives\n",
    "    if current_narrative and not any(d['narrative'] == current_narrative for d in data):\n",
    "        data.append({'narrative': current_narrative, 'subnarrative': 'Other'})\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.sort_values(by='narrative', ascending=True).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Parse the taxonomies and create DataFrames\n",
    "ukraine_war_df = parse_taxonomy(ukraine_war_text)\n",
    "climate_change_df = parse_taxonomy(climate_change_text)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "ukraine_war_df.head(50)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "climate_change_df.head(50)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Handle some edge cases in the taxonomy as instructed in the task description (see https://propaganda.math.unipd.it/semeval2025task10/index.html)\n",
    "- E.g. if a narrative is identified but no subnarrative applies, the subnarrative is \"Other\". We apply this to all rows in the taxonomy dataframe except for the narratives \"Other\" and \"Hidden plots by secret schemes of powerful groups\" since those are already present (in one of the two dataframes) \n",
    "- After manually adding the \"Other\", \"Hidden plots by secret schemes of powerful groups\" combination in the climate change taxonomy, all possible combinations are be covered\n",
    "    - We do this because the \"Hidden plots by secret schemes of powerful groups\" - \"Other\" combination is already present in the UA taxonomy but not in the CC taxonomy"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to add \"Other\" subnarrative for each narrative group, excluding specific narratives\n",
    "def add_other_subnarrative(df):\n",
    "    additional_rows = []\n",
    "    unique_narratives = df['narrative'].unique()\n",
    "    for narrative in unique_narratives:\n",
    "        if narrative not in [\"Other\", \"Hidden plots by secret schemes of powerful groups\"]:\n",
    "            additional_rows.append({'narrative': narrative, 'subnarrative': 'Other'})\n",
    "    additional_df = pd.DataFrame(additional_rows)\n",
    "    return pd.concat([df, additional_df], ignore_index=True)\n",
    "\n",
    "# Function to sort the DataFrame and add an index column\n",
    "def sort_and_index_df(df):\n",
    "    df = df.sort_values(by=['narrative', 'subnarrative']).reset_index(drop=True)\n",
    "    df['index'] = df.index + 1\n",
    "    df = df[['index', 'narrative', 'subnarrative']]\n",
    "    return df\n",
    "\n",
    "# Add \"Other\" subnarrative to each DataFrame\n",
    "ukraine_war_df = add_other_subnarrative(ukraine_war_df)\n",
    "climate_change_df = add_other_subnarrative(climate_change_df)\n",
    "\n",
    "# Manually add the specific row to the climate change DataFrame\n",
    "climate_change_df = pd.concat([climate_change_df, pd.DataFrame([{'narrative': 'Hidden plots by secret schemes of powerful groups', 'subnarrative': 'Other'}])], ignore_index=True)\n",
    "\n",
    "# Sort and add index column to each DataFrame\n",
    "ukraine_war_df = sort_and_index_df(ukraine_war_df)\n",
    "climate_change_df = sort_and_index_df(climate_change_df)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "ukraine_war_df.head(55)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "climate_change_df.head(58)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Map the labels in the file to the taxonomy"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- In the next step, we want to add another column to our \"df\" that combines the narrative_subnarrative_pairs column with the information in our taxonomy dataframes\n",
    "- Firstly, the column \"topic\" tells us what taxonomy df should be applied (UA for ukraine war and CC for climate change)\n",
    "- Then, theoretically, the dictionaries should exactly correspond to a given row in one of the two taxonomy dataframes\n",
    "- We check if every narrative-subnarrative pair of every row in the dataset can be mapped to its row in the taxonomy and if so, add a column to the dataframe that contains the indices of the targets."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a mapping of narrative-subnarrative pairs to their indices\n",
    "def create_mapping(df):\n",
    "    mapping = {}\n",
    "    for _, row in df.iterrows():\n",
    "        key = (row['narrative'], row['subnarrative'])\n",
    "        mapping[key] = row['index']\n",
    "    return mapping\n",
    "\n",
    "ukraine_war_mapping = create_mapping(ukraine_war_df)\n",
    "climate_change_mapping = create_mapping(climate_change_df)\n",
    "\n",
    "# Function to check the mapping and add indices to the DataFrame\n",
    "def add_target_indices(row, ukraine_war_mapping, climate_change_mapping):\n",
    "    pairs = row['narrative_subnarrative_pairs']\n",
    "    topic = row['topic']\n",
    "    indices = []\n",
    "    \n",
    "    if topic == \"UA\":\n",
    "        mapping = ukraine_war_mapping\n",
    "    elif topic == \"CC\":\n",
    "        mapping = climate_change_mapping\n",
    "    else:\n",
    "        return None  # Invalid topic\n",
    "    \n",
    "    for pair in pairs:\n",
    "        key = (pair['narrative'], pair['subnarrative'])\n",
    "        if key in mapping:\n",
    "            indices.append(mapping[key])\n",
    "        else:\n",
    "            return (row.name, key)  # Mapping does not exist\n",
    "    \n",
    "    return indices\n",
    "\n",
    "# Apply the function to each row and collect the results\n",
    "df['target_indices'] = df.apply(add_target_indices, axis=1, args=(ukraine_war_mapping, climate_change_mapping))\n",
    "\n",
    "# Filter out the rows where the mapping does not exist\n",
    "problematic_rows = df[df['target_indices'].apply(lambda x: isinstance(x, tuple))]\n",
    "\n",
    "# Display the problematic rows\n",
    "print(\"Problematic rows where the mapping does not exist:\")\n",
    "print(problematic_rows)\n",
    "\n",
    "# Display the first few problematic rows for inspection\n",
    "if not problematic_rows.empty:\n",
    "    print(\"First few problematic rows:\")\n",
    "    for index, row in problematic_rows.iterrows():\n",
    "        print(f\"Row index: {index}, Problematic pair: {row['target_indices']}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "len(problematic_rows)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- There are two datapoints/rows in the dataset where the mapping does not work, the indices 65 and 143\n",
    "- With a closer look, we see what the problem is: All but the two rows are EITHER ukraine war OR climate change\n",
    "- In the two rows, this is not the case (remember that our implementation relies on classifying whole articles as a topic, i.e. all the labels must be from the taxonomy of that topic)\n",
    "\n",
    "Below we check the annotation file for rows where there are mixed topics in the labels."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# Define the path to the annotations file\n",
    "annotations_file = \"../training_data_16_October_release/EN/subtask-2-annotations.txt\"\n",
    "\n",
    "# Read the annotations file\n",
    "annotations = pd.read_csv(annotations_file, sep='\\t', header=None, names=['filename', 'narrative', 'subnarrative'])\n",
    "\n",
    "# Initialize a list to store the line numbers with both \"CC\" and \"URW\"\n",
    "mixed_topic_lines = []\n",
    "\n",
    "# Iterate through each row and check for mixed topics\n",
    "for index, row in annotations.iterrows():\n",
    "    narrative = row['narrative']\n",
    "    subnarrative = row['subnarrative']\n",
    "    \n",
    "    # Check if both \"CC\" and \"URW\" are present in either narrative or subnarrative\n",
    "    if (\"CC: \" in narrative and \"URW: \" in narrative) or (\"CC: \" in subnarrative and \"URW: \" in subnarrative):\n",
    "        mixed_topic_lines.append(index + 1)  # Adding 1 to index to match line numbers\n",
    "\n",
    "# Print the line numbers with mixed topics\n",
    "print(\"Lines with both 'CC' and 'URW' present:\")\n",
    "print(mixed_topic_lines)\n",
    "\n",
    "# Print the total count of such lines\n",
    "print(\"Total number of lines with mixed topics:\", len(mixed_topic_lines))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- This shows us that line 66 in the annotations file (index 65 in the dataset) has labels from both taxonomies\n",
    "- Line 144 in the annotation file on the other hand (Index 143) has UA in the filename but CC in all the labels, which we assume to be an error in the dataset\n",
    "- We deal with this by dropping the two rows from the dataset"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_short = df.drop([65, 143])"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Conclusion for initial data prep and label representation\n",
    "- We now have a dataframe containing all relevant data, including which topic an article belongs to, the article content (in raw form up until now) and the labels (narrative and subnarrative combinations) in text form and in numerical form\n",
    "- We removed 2 problematic datapoints and are left with 198\n",
    "- While the label classes are basically ordinally encoded right now, one-hot encoding would be more suitable since there is no ordinal relationship for the class labels\n",
    "- We can now easily change the encoding which we will do in Milestone 2. The current implementation also easily supports differentiating the topics to train separate models using the \"topic\" column"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Text Segmentation <a class=\"anchor\" id=\"text-segmentation\"></a>"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a class=\"anchor\" id=\"tokenize-sentences-and-words\"></a>\n",
    "Now we handle the content of the articles. Currently, each entry in our dataframe has a single plain string that contains the whole article.\n",
    "\n",
    "Let's start by splitting it into sentences and words."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def tokenize(df):\n",
    "    df['tokens'] = None\n",
    "    for i, row in df.iterrows():\n",
    "        # split the content into sentences\n",
    "        sentences = nltk.sent_tokenize(row['content'])\n",
    "        # tokenize each sentence\n",
    "        tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "        df.at[i, 'tokens'] = tokens\n",
    "    return df\n",
    "\n",
    "df_short = tokenize(df_short)\n",
    "#df_short.head()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a class=\"anchor\" id=\"find-sentences-unusual-length\"></a>\n",
    "To uncover potential errors, let us check for and handle sentences of unusual length."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to find sentences of unusual length\n",
    "def find_unusual_length_sentences(df, min_length=3, max_length=130):\n",
    "    unusual_sentences = []\n",
    "    for i, row in df.iterrows():\n",
    "        for j, sentence in enumerate(row['tokens']):\n",
    "            if len(sentence) < min_length or len(sentence) > max_length:\n",
    "                # also store the previous and next sentences for context\n",
    "                prev_sentence = row['tokens'][j-1] if j > 0 else None\n",
    "                next_sentence = row['tokens'][j+1] if j < len(row['tokens']) - 1 else None\n",
    "                unusual_sentences.append({\n",
    "                    'row_index': i, # for later handling of the unusual sentences\n",
    "                    'sentence_index': j, # for later handling of the unusual sentences\n",
    "                    'sentence': sentence,\n",
    "                    'previous': prev_sentence,\n",
    "                    'next': next_sentence\n",
    "                })\n",
    "    return unusual_sentences\n",
    "\n",
    "# Find sentences with less than 3 words or more than 130 words\n",
    "unusual_sentences = find_unusual_length_sentences(df_short)\n",
    "\n",
    "print(f\"There are {len(unusual_sentences)} sentences of unusual length.\")\n",
    "\n",
    "# Display the unusual sentences\n",
    "for entry in unusual_sentences:\n",
    "    print(f\"Sentence length: {len(entry[\"sentence\"])}, Sentence: {' '.join(entry[\"sentence\"])}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We find multiple very short and very large sentences.\n",
    "\n",
    "<a class=\"anchor\" id=\"handle-short-sentences\"></a>\n",
    "#### Handling very short sentences.\n",
    "\n",
    "The sentences of size 1 all consist of non-meaningful characters. Therefore we can drop them directly."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to drop unusual sentences of a specific length from the DataFrame\n",
    "def drop_sentences_of_length(df, unusual_sentences, length):\n",
    "    # Create a copy of the list to iterate over\n",
    "    for entry in unusual_sentences[:]:\n",
    "        if len(entry['sentence']) == length:\n",
    "            row_index = entry['row_index']\n",
    "            sentence_index = entry['sentence_index']\n",
    "            # Check if the sentence index is within the valid range\n",
    "            if 0 <= sentence_index < len(df.at[row_index, 'tokens']):\n",
    "                # Drop from DataFrame\n",
    "                df.at[row_index, 'tokens'].pop(sentence_index)\n",
    "                # Drop from unusual_sentences list\n",
    "                unusual_sentences.remove(entry)\n",
    "                print(\"dropped: \", entry['sentence'])\n",
    "    return df\n",
    "\n",
    "# Drop sentences of length 1\n",
    "print(f\"There are {len(unusual_sentences)} sentences of unusual length before dropping sentences of length 1.\")\n",
    "df_short = drop_sentences_of_length(df_short, unusual_sentences, length=1)\n",
    "print(f\"There are {len(unusual_sentences)} sentences of unusual length after dropping sentences of length 1.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The sentences of size 2 might make sense. Let's have a look at their context."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Display sentences of length 2 with their preceding and following sentences\n",
    "for entry in unusual_sentences:\n",
    "\n",
    "    print(f\"(Previous) {' '.join(entry['previous']) if entry['previous'] else 'None'}\")\n",
    "    print(f\"(Idx {entry['row_index']}, {entry['sentence_index']}, ListIdx {unusual_sentences.index(entry)}) {' '.join(entry['sentence'])}\")\n",
    "    if entry['next']:\n",
    "        print(f\"(Next) {' '.join(entry['next'])}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We find, that the sentences of size 2 are of different types.\n",
    " - Words belonging to the previous or following sentence, but are split by punctation errors (e.g. \"Mild. Tonight: Rain slowly returns ...\") -> merge manually \n",
    " - Words at the end of an document (e.g. \"Watch:\") -> drop\n",
    " - Valid sentences (e.g. \"Why?\") -> valid, keep\n",
    " - Section numerations (e.g. \"2. (paragraph)\") -> valid, keep (might be helpful for the model to understand the text, since they give a structure)\n",
    " \n",
    "\n",
    "Since the number of such sentences is managable, we can manually decide on each case, whether to keep, merge or drop it."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# function to merge specific sentences with either the previous or next sentence\n",
    "def merge_specific_sentence(df, row_idx, sentence_idx, direction):\n",
    "    \n",
    "    # List of sentences of the specified row\n",
    "    sentences = df.at[row_idx, 'tokens']\n",
    "    \n",
    "    # Merge based on the direction\n",
    "    if direction == 'previous':\n",
    "        merged_sentence = sentences[sentence_idx - 1] + sentences[sentence_idx]\n",
    "        sentences[sentence_idx - 1] = merged_sentence\n",
    "        del sentences[sentence_idx]\n",
    "        \n",
    "    elif direction == 'next':\n",
    "        merged_sentence = sentences[sentence_idx] + sentences[sentence_idx + 1]\n",
    "        sentences[sentence_idx] = merged_sentence\n",
    "        del sentences[sentence_idx + 1]\n",
    "    \n",
    "    # Update the row in the dataframe\n",
    "    df.at[row_idx, 'tokens'] = sentences\n",
    "\n",
    "\n",
    "print(f\"There are {len(unusual_sentences)} sentences of unusual length before merging some sentences of length 2.\")\n",
    "\n",
    "merge_specific_sentence(df_short, 87, 6, 'previous')  # Merge \"Drones .\" with previous\n",
    "merge_specific_sentence(df_short, 88, 0, 'next')      # Merge \"U.K .\" with next\n",
    "merge_specific_sentence(df_short, 127, 23, 'next')    # Merge \"Mild .\" with next\n",
    "merge_specific_sentence(df_short, 136, 3, 'next')     # Merge \"Gov .\" with next\n",
    "\n",
    "# drop them from the list of unusual sentences\n",
    "unusual_sentences = [i for j, i in enumerate(unusual_sentences) if j not in [6,7,12,13]]\n",
    "\n",
    "print(f\"There are {len(unusual_sentences)} sentences of unusual length after merging some sentences of length 2.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# function to drop specific sentences from the dataframe\n",
    "def drop_sentence_by_indices(df, row_idx, sentence_idx):\n",
    "    \n",
    "    # copy list of sentences of the specified row\n",
    "    sentences = df.at[row_idx, 'tokens']\n",
    "        \n",
    "    if 0 <= sentence_idx < len(sentences):\n",
    "        # drop sentence\n",
    "        del sentences[sentence_idx]\n",
    "        # update the dataframe\n",
    "        df.at[row_idx, 'tokens'] = sentences\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(f\"Invalid sentence index {sentence_idx} for row {row_idx}\")\n",
    "\n",
    "\n",
    "print(f\"There are {len(unusual_sentences)} sentences of unusual length before dropping some sentences of length 2.\")\n",
    "\n",
    "drop_sentence_by_indices(df_short, 55, 10)\n",
    "drop_sentence_by_indices(df_short, 80, 21)\n",
    "\n",
    "# drop them from the list of unusual sentences\n",
    "unusual_sentences = [i for j, i in enumerate(unusual_sentences) if j not in [2, 5]]\n",
    "\n",
    "print(f\"There are {len(unusual_sentences)} sentences of unusual length after dropping some sentences of length 2.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Handling very long sentences <a class=\"anchor\" id=\"handle-long-sentences\"></a>\n",
    "\n",
    "There are several very long sentences. Looking at the data, we see that some of them are infact correctly split and complete single sentences. Others, however, are in fact multiple sentences stored as one, because the splitting did not work correctly. Let's manually split them."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# function to manually replace a unsplitted sequence of sentences with the manually correctly splitted sentences\n",
    "def manually_split_sentence(idx_row, idx_sentence, splitted_sentence):\n",
    "    df_short.at[idx_row, 'tokens'] = df_short.at[idx_row, 'tokens'][:idx_sentence] + splitted_sentence + df_short.at[6, 'tokens'][idx_sentence+1:]\n",
    "\n",
    "\n",
    "print(f\"There are {len(unusual_sentences)} sentences of unusual length before manually splitting some too long sentences.\")\n",
    "\n",
    "manually_split_sentence(6,6,[\n",
    "    [\"Jens\", \"Stoltenberg\", \"(\", \"pictured\", \")\", \",\", \"the\", \"13th\", \"secretary\", \"general\", \"of\", \"NATO\", \",\", \"revealed\", \"there\", \"were\", \"live\", \"discussions\", \"among\", \"members\", \"about\", \"removing\", \"missiles\", \"from\", \"storage\", \"and\", \"putting\", \"them\", \"on\", \"standby\", \".\"],\n",
    "    [\"A\", \"Netherlands\", \"'\", \"Air\", \"Force\", \"F-16\", \"jetfighter\", \"takes\", \"part\", \"in\", \"the\", \"NATO\", \"exercise\", \"as\", \"part\", \"of\", \"the\", \"NATO\", \"Air\", \"Policing\", \"mission\", \".\"],\n",
    "    [\"The\", \"head\", \"of\", \"Kyiv\", \"'s\", \"national\", \"security\", \"council\", \"said\", \"Putin\", \"could\", \"demand\", \"a\", \"tactical\", \"nuclear\", \"weapon\", \"be\", \"used\", \"if\", \"Russia\", \"'s\", \"army\", \"is\", \"beaten\", \"in\", \"Ukraine\", \".\"],\n",
    "    [\"Russian\", \"soldiers\", \"load\", \"a\", \"Iskander-M\", \"short-range\", \"ballistic\", \"missile\", \"launcher\", \"at\", \"a\", \"firing\", \"position\", \"as\", \"part\", \"of\", \"a\", \"Russian\", \"military\", \"drill\", \"intended\", \"to\", \"train\", \"the\", \"troops\", \"in\", \"using\", \"tactical\", \"nuclear\", \"weapons\", \".\"],\n",
    "    [\"Meanwhile\", \",\", \"Mr\", \"Stoltenberg\", \"warned\", \"in\", \"Brussels\", \"of\", \"the\", \"threat\", \"from\", \"China\", \",\", \"adding\", \"that\", \"nuclear\", \"transparency\", \"should\", \"form\", \"the\", \"basis\", \"of\", \"NATO\", \"'s\", \"nuclear\", \"strategy\", \"to\", \"prepare\", \"the\", \"alliance\", \"for\", \"the\", \"dangers\", \"of\", \"the\", \"world\", \".\"]\n",
    "])\n",
    "\n",
    "manually_split_sentence(46,12,[\n",
    "    [\"“\", \"The\", \"Complaint\", \"alleged\", \"that\", \"several\", \"of\", \"the\", \"Vietnamese\", \"orphans\", \"brought\", \"to\", \"the\", \"United\", \"States\", \"under\", \"Operation\", \"Babylift\", \"stated\", \"they\", \"are\", \"not\", \"orphans\", \"and\", \"that\", \"they\", \"wish\", \"to\", \"return\", \"to\", \"Vietnam\", \".\", \"”\"],\n",
    "    [\"A\", \"statement\", \"issued\", \"on\", \"April\", \"4\", \",\", \"1975\", \",\", \"by\", \"“\", \"professors\", \"of\", \"ethics\", \"and\", \"religion\", \",\", \"”\", \"pointed\", \"out\", \"that\", \"many\", \"“\", \"of\", \"the\", \"children\", \"are\", \"not\", \"orphans\", \";\", \"their\", \"parents\", \"or\", \"relatives\", \"may\", \"still\", \"be\", \"alive\", \",\", \"although\", \"displaced\", \",\", \"in\", \"Vietnam\", \"…\", \"The\", \"Vietnamese\", \"children\", \"should\", \"be\", \"allowed\", \"to\", \"stay\", \"in\", \"Vietnam\", \"where\", \"they\", \"belong\", \".\", \"”\"],\n",
    "    [\"The\", \"operation\", \"was\", \"celebrated\", \"by\", \"the\", \"corporate\", \"media\", \"and\", \"“\", \"Hollywood\", \"’\", \"s\", \"celebrity\", \"elite\", \"…\", \"[\", \"and\", \",\", \"as\", \"a\", \"propaganda\", \"event\", \"]\", \"generated\", \"a\", \"spectacle\", \"of\", \"celebration\", \"and\", \"emphasized\", \"that\", \"the\", \"babies\", \"were\", \"more\", \"than\", \"just\", \"average\", \"orphans\", \",\", \"”\", \"writes\", \"US\", \"History\", \"Scene\", \".\"]\n",
    "])\n",
    "\n",
    "manually_split_sentence(73,8,[\n",
    "    [\"But\", \"he\", \"noted\", \"that\", \"“\", \"even\", \"as\", \"the\", \"Russians\", \"have\", \"gained\", \"territory\", \",\", \"they\", \"do\", \"it\", \"at\", \"a\", \"pretty\", \"big\", \"cost\", \"in\", \"number\", \"of\", \"casualties\", \",\", \"like\", \"in\", \"personnel\", \",\", \"but\", \"also\", \"in\", \"number\", \"of\", \"pieces\", \"of\", \"equipment\", \"that\", \"are\", \"being\", \"taken\", \"out.\", \"”\"],\n",
    "    [\"Austin\", \"said\", \"in\", \"his\", \"remarks\", \"Tuesday\", \"that\", \"“\", \"Russia\", \"has\", \"paid\", \"a\", \"staggering\", \"cost\", \"for\", \"(\", \"President\", \"Vladimir\", \")\", \"Putin\", \"’\", \"s\", \"imperial\", \"dreams\", \"”\", \",\", \"using\", \"“\", \"up\", \"to\", \"$\", \"211\", \"billion\", \"to\", \"equip\", \",\", \"deploy\", \",\", \"maintain\", \",\", \"and\", \"sustain\", \"its\", \"imperial\", \"aggression\", \"against\", \"Ukraine.\", \"”\"],\n",
    "    [\"“\", \"At\", \"least\", \"315,000\", \"Russian\", \"troops\", \"have\", \"been\", \"killed\", \"or\", \"wounded\", \"”\", \"since\", \"Russia\", \"launched\", \"its\", \"all-out\", \"invasion\", \"of\", \"Ukraine\", \"in\", \"2022\", \",\", \"Austin\", \"said\", \".\"],\n",
    "    [\"Austin\", \"added\", \"that\", \"Ukraine\", \"has\", \"also\", \"“\", \"sunk\", \",\", \"destroyed\", \",\", \"or\", \"damaged\", \"some\", \"20\", \"medium-to-large\", \"Russian\", \"navy\", \"vessels.\", \"”\"],\n",
    "    [\"The\", \"sinkings\", \"have\", \"been\", \"an\", \"embarrassment\", \"for\", \"Moscow\", \"and\", \"Russian\", \"state\", \"media\", \"confirmed\", \"Tuesday\", \"that\", \"the\", \"country\", \"had\", \"replaced\", \"the\", \"head\", \"of\", \"its\", \"navy\", \".\"]\n",
    "])\n",
    "\n",
    "manually_split_sentence(77,13,[\n",
    "    [\"Подробнее\", \"на\", \"РБК\", \":\", \"https\", \":\", \"//www.rbc.ru/politics/14/06/2023/6489e6f39a794778d61881b4\", \".\"],\n",
    "    [\"The\", \"picture\", \"of\", \"widening\", \"war\", \"is\", \"beginning\", \"to\", \"form\", \":\", \"Professor\", \"Sergey\", \"Karaganov\", \",\", \"honorary\", \"chairman\", \"of\", \"Russia\", \"’\", \"s\", \"Council\", \"on\", \"Foreign\", \"and\", \"Defense\", \"Policy\", \",\", \"and\", \"academic\", \"supervisor\", \"at\", \"the\", \"School\", \"of\", \"International\", \"Economics\", \"and\", \"Foreign\", \"Affairs\", \"Higher\", \"School\", \"of\", \"Economics\", \"(\", \"HSE\", \")\", \"in\", \"Moscow\", \".\"],\n",
    "    [\"Sergey\", \"Karaganov\", \":\", \"By\", \"using\", \"its\", \"nuclear\", \"weapons\", \",\", \"Russia\", \"could\", \"save\", \"humanity\", \"from\", \"a\", \"global\", \"catastrophe\", \".\"],\n",
    "    [\"A\", \"tough\", \"but\", \"necessary\", \"decision\", \"would\", \"likely\", \"force\", \"the\", \"West\", \"to\", \"back\", \"off\", \",\", \"enabling\", \"an\", \"earlier\", \"end\", \"to\", \"the\", \"Ukraine\", \"crisis\", \"and\", \"preventing\", \"it\", \"from\", \"expanding\", \"to\", \"other\", \"states\", \".\"],\n",
    "    [\"Karaganov\", \"’\", \"s\", \"description\", \"of\", \"the\", \"Western\", \"World\", \"as\", \"“\", \"anti-human\", \"ideologies\", \":\", \"the\", \"denial\", \"of\", \"family\", \",\", \"homeland\", \",\", \"history\", \",\", \"love\", \"between\", \"men\", \"and\", \"women\", \",\", \"faith\", \",\", \"service\", \"to\", \"higher\", \"ideals\", \",\", \"everything\", \"that\", \"is\", \"human\", \",\", \"”\", \"shows\", \"a\", \"rising\", \"realization\", \"that\", \"Russia\", \"sees\", \"itself\", \"confronted\", \"by\", \"a\", \"Satanic\", \"force\", \"that\", \"must\", \"be\", \"destroyed\", \".\"]\n",
    "])\n",
    "\n",
    "manually_split_sentence(147,7,[\n",
    "    [\"At\", \"the\", \"same\", \"time\", \",\", \"the\", \"official\", \"claimed\", \"that\", \"the\", \"danger\", \"of\", \"Kiev\", \"using\", \"a\", \"‘\", \"dirty\", \"bomb\", \"’\", \"remains\", \"“\", \"very\", \"high\", \",\", \"”\", \"and\", \"that\", \"Ukraine\", \"“\", \"has\", \"the\", \"opportunity\", \"”\", \"and\", \"“\", \"has\", \"every\", \"reason\", \"to\", \"use\", \"it\", \".\"],\n",
    "    [\"Earlier\", \"on\", \"Tuesday\", \",\", \"in\", \"a\", \"letter\", \"to\", \"UN\", \"Secretary-General\", \"Antonio\", \"Guterres\", \",\", \"the\", \"Russian\", \"mission\", \"’\", \"s\", \"head\", \",\", \"Vassily\", \"Nebenzia\", \",\", \"said\", \"that\", \"Moscow\", \"would\", \"consider\", \"the\", \"use\", \"of\", \"a\", \"‘\", \"dirty\", \"bomb\", \"’\", \"by\", \"Ukraine\", \"“\", \"an\", \"act\", \"of\", \"nuclear\", \"terrorism\", \".\", \"”\"],\n",
    "    [\"Meanwhile\", \",\", \"Ukrainian\", \"Foreign\", \"Minister\", \"Dmitry\", \"Kuleba\", \"earlier\", \"called\", \"the\", \"Russian\", \"allegations\", \"“\", \"as\", \"absurd\", \"as\", \"they\", \"are\", \"dangerous\", \".\", \"”\"],\n",
    "    [\"He\", \"also\", \"noted\", \"that\", \"“\", \"Russians\", \"often\", \"accuse\", \"others\", \"of\", \"what\", \"they\", \"plan\", \"themselves\", \".\", \"”\"],\n",
    "    [\"On\", \"Tuesday\", \",\", \"the\", \"minister\", \"revealed\", \"that\", \"Ukraine\", \"had\", \"invited\", \"IAEA\", \"inspectors\", \"to\", \"come\", \"and\", \"to\", \"“\", \"prove\", \"that\", \"Ukraine\", \"has\", \"neither\", \"any\", \"dirty\", \"bombs\", \"nor\", \"plans\", \"to\", \"develop\", \"them\", \".\", \"”\"],\n",
    "    [\"“\", \"Good\", \"cooperation\", \"with\", \"IAEA\", \"and\", \"partners\", \"allows\", \"us\", \"to\", \"foil\", \"Russia\", \"’\", \"s\", \"‘\", \"dirty\", \"bomb\", \"’\", \"disinfo\", \"campaign\", \",\", \"”\", \"Kuleba\", \"said\", \".\"]\n",
    "])\n",
    "\n",
    "manually_split_sentence(154,5,[\n",
    "    [\"WHO\", \"Tedros\", \"describes\", \"Disease\", \"X\", \"as\", \"a\", \"blueprint\", \"at\", \"a\", \"panel\", \"discussion\", \"at\", \"WEF24\", \"—\", \"Tamara\", \"Ugolini\", \"🇨🇦\", \"(\", \"@\", \"TamaraUgo\", \")\", \"January\", \"17\", \",\", \"2024\", \".\"],\n",
    "    [\"He\", \"says\", \"that\", \"COVID\", \"was\", \"the\", \"first\", \"Disease\", \"X\", \"and\", \"we\", \"“\", \"need\", \"a\", \"placeholder\", \"for\", \"diseases\", \"we\", \"don\", \"’\", \"t\", \"know\", \",\", \"”\", \"including\", \"dedication\", \"to\", \"private\", \"sector\", \"drug\", \"research\", \"and\", \"development\", \".\"],\n",
    "    [\"Disease\", \"X\", \"serves\", \"as\", \"a\", \"“\", \"placeholder\", \"for\", \"the\", \"diseases\", \"we\", \"don\", \"’\", \"t\", \"know\", \",\", \"”\", \"and\", \"it\", \"begins\", \"with\", \"private-sector\", \"research\", \"and\", \"development\", \"to\", \"test\", \"drugs\", \"and\", \"“\", \"other\", \"things\", \".\", \"”\"],\n",
    "    [\"Tedros\", \"stressed\", \"that\", \"the\", \"next\", \"pandemic\", \"is\", \"“\", \"not\", \"a\", \"matter\", \"of\", \"if\", \",\", \"but\", \"rather\", \"when\", \",\", \"”\", \"while\", \"noting\", \"that\", \"COVID-19\", \"was\", \"the\", \"original\", \"Disease\", \"X\", \",\", \"in\", \"which\", \"they\", \"were\", \"able\", \"to\", \"facilitate\", \"the\", \"Pandemic\", \"Fund\", \"in\", \"partnership\", \"with\", \"the\", \"World\", \"Bank\", \".\"]\n",
    "])\n",
    "\n",
    "# drop them from the list of unusual sentences\n",
    "unusual_sentences = [i for j, i in enumerate(unusual_sentences) if j not in [0, 1, 2, 3, 8, 9]]\n",
    "\n",
    "print(f\"There are {len(unusual_sentences)} sentences of unusual length after manually splitting some too long sentences.\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Validating fixed unusual-length-sentences <a class=\"anchor\" id=\"verify-remaining-unusual-sentences\"></a>\n",
    "\n",
    "We update the unusual sentences and print them. We find that all unusually short and long sentences that still occur, are valid and ment to be kept."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Find sentences with less than 3 words or more than 100 words\n",
    "unusual_sentences = find_unusual_length_sentences(df_short)\n",
    "\n",
    "print(f\"After handling, there are {len(unusual_sentences)} unusual sentences left.\")\n",
    "\n",
    "# Display the unusual sentences\n",
    "for entry in unusual_sentences:\n",
    "    print(f\"Sentence length: {len(entry[\"sentence\"])}, (Idx {entry['row_index']}, {entry['sentence_index']}), Sentence: {' '.join(entry[\"sentence\"])}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that the text is correctly segmentated into sentences and words, we can proceed with text normalization."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Text Normalization  <a class=\"anchor\" id=\"text-normalization\"></a>"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "  \n",
    "Text normalization is the process of transforming text into a standard format, which typically involves:\n",
    "\n",
    "- Converting text to lowercase\n",
    "- Removing punctuation\n",
    "- Removing stopwords\n",
    "- Removing special characters and numbers\n",
    "- Lemmatization or stemming\n",
    "\n",
    "This process helps in reducing the complexity of the text and making it more uniform for further analysis or processing.\n",
    "\n",
    "We will implement text normalization in the next steps."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(torch.version.cuda)  # Shows CUDA version if available\n",
    "print(torch.cuda.is_available())  # Checks if CUDA is available\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check if a NVIDIA Graphics card is installed (and the nessesary CUDA packages) to be used later for lemmatization because with just the CPU it was taking around 15min every time. "
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def text_normalization(df, column_name):\n",
    "    \"\"\"\n",
    "    Text normalization with optimized batch processing for nested lists of tokens\n",
    "    with proper abbreviation handling\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    nlp = stanza.Pipeline('en',\n",
    "                         processors='tokenize,lemma',\n",
    "                         device=device,\n",
    "                         use_gpu=True,\n",
    "                         batch_size=4096,\n",
    "                         tokenize_batch_size=4096,\n",
    "                         tokenize_pretokenized=True,\n",
    "                         download_method=None\n",
    "                         )\n",
    "    \n",
    "    # Depending on the graphics card the batch size can be adjusted\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Common abbreviations and their normalized forms\n",
    "    abbreviations = {\n",
    "        'p.m.': 'pm',\n",
    "        'a.m.': 'am',\n",
    "        'e.g.': 'eg',\n",
    "        'i.e.': 'ie',\n",
    "        'etc.': 'etc',\n",
    "        'vs.': 'vs',\n",
    "        'mr.': 'mr',\n",
    "        'mrs.': 'mrs',\n",
    "        'dr.': 'dr',\n",
    "        'prof.': 'prof',\n",
    "        'u.s.': 'us',\n",
    "        'u.k.': 'uk',\n",
    "        'n.y.': 'ny',\n",
    "        'l.a.': 'la',\n",
    "        'st.': 'st',\n",
    "        'inc.': 'inc',\n",
    "        'ltd.': 'ltd',\n",
    "        'co.': 'co',\n",
    "        'corp.': 'corp',\n",
    "        'avg.': 'avg',\n",
    "        'approx.': 'approx'\n",
    "    }\n",
    "    \n",
    "    def normalize_token(token):\n",
    "        \"\"\"Normalize a single token\"\"\"\n",
    "        if not isinstance(token, str):\n",
    "            return ''\n",
    "            \n",
    "        # Convert to lowercase first\n",
    "        token = token.lower().strip()\n",
    "        \n",
    "        # Check if it's an abbreviation\n",
    "        if token in abbreviations:\n",
    "            return abbreviations[token]\n",
    "            \n",
    "        # Remove special characters and numbers for non-abbreviations\n",
    "        token = re.sub(r'[^a-z]', '', token)\n",
    "        \n",
    "        return token\n",
    "    \n",
    "    def clean_text(nested_tokens):\n",
    "        \"\"\"Clean and preprocess nested list of tokens\"\"\"\n",
    "        if not isinstance(nested_tokens, list):\n",
    "            return []\n",
    "        \n",
    "        cleaned_tokens = []\n",
    "        for sentence in nested_tokens:\n",
    "            if isinstance(sentence, list):\n",
    "                for token in sentence:\n",
    "                    # Normalize the token\n",
    "                    normalized = normalize_token(token)\n",
    "                    # Check if token is not empty and not a stopword\n",
    "                    if normalized and normalized not in stop_words:\n",
    "                        cleaned_tokens.append(normalized)\n",
    "        \n",
    "        return cleaned_tokens\n",
    "    \n",
    "    def process_text(tokens):\n",
    "        \"\"\"Process a single text through Stanza\"\"\"\n",
    "        try:\n",
    "            if not tokens:\n",
    "                return []\n",
    "            # Join tokens into a single string for processing\n",
    "            text = ' '.join(tokens)\n",
    "            doc = nlp(text)\n",
    "            # Extract lemmas and filter stopwords\n",
    "            lemmas = []\n",
    "            for sent in doc.sentences:\n",
    "                for word in sent.words:\n",
    "                    lemma = word.lemma.lower()\n",
    "                    # Check if the original token was an abbreviation\n",
    "                    if lemma not in stop_words:\n",
    "                        lemmas.append(lemma)\n",
    "            return lemmas\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def process_batch(batch_tokens):\n",
    "        \"\"\"Process a batch of nested token lists\"\"\"\n",
    "        results = []\n",
    "        for tokens in batch_tokens:\n",
    "            # Clean and flatten tokens\n",
    "            cleaned_tokens = clean_text(tokens)\n",
    "            # Process cleaned tokens\n",
    "            normalized = process_text(cleaned_tokens)\n",
    "            # Ensure all tokens are properly normalized\n",
    "            normalized = [normalize_token(token) for token in normalized if normalize_token(token)]\n",
    "            results.append(normalized)\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    # Process in batches\n",
    "    batch_size = 50\n",
    "    normalized_tokens = []\n",
    "    total_batches = (len(df) + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"Starting processing of {len(df)} rows in {total_batches} batches\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(df), batch_size), desc=\"Normalizing text\"):\n",
    "        batch_df = df.iloc[i:i + batch_size]\n",
    "        batch_tokens = batch_df[column_name].tolist()\n",
    "        \n",
    "        # Print sample of first batch for debugging\n",
    "        if i == 0:\n",
    "            print(\"\\nSample processing:\")\n",
    "            sample_tokens = batch_tokens[0][:5] if batch_tokens else []  # First 5 tokens of first row\n",
    "            print(f\"Original tokens: {sample_tokens}\")\n",
    "            cleaned = clean_text([sample_tokens])\n",
    "            print(f\"Cleaned tokens: {cleaned}\")\n",
    "            normalized = process_batch([sample_tokens])\n",
    "            print(f\"Normalized tokens: {normalized[0]}\\n\")\n",
    "        \n",
    "        normalized_batch = process_batch(batch_tokens)\n",
    "        normalized_tokens.extend(normalized_batch)\n",
    "    \n",
    "    print(f\"\\nProcessing completed. Total normalized entries: {len(normalized_tokens)}\")\n",
    "    print(f\"Non-empty normalized entries: {sum(1 for tokens in normalized_tokens if tokens)}\")\n",
    "    \n",
    "    # Final check to ensure no punctuation or special characters remain\n",
    "    final_tokens = []\n",
    "    for tokens in normalized_tokens:\n",
    "        cleaned = [token for token in tokens if token and not any(char in string.punctuation for char in token)]\n",
    "        final_tokens.append(cleaned)\n",
    "    \n",
    "    # Update DataFrame with normalized tokens\n",
    "    df_normalized = df.copy()\n",
    "    df_normalized[f'{column_name}_normalized'] = final_tokens\n",
    "    \n",
    "    return df_normalized"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Verifying of text nomalization  <a class=\"anchor\" id=\"check-normalization\"></a>"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "def check_text_normalization(df):\n",
    "    \"\"\"\n",
    "    Validates if text normalization has been applied correctly\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing original and normalized tokens\n",
    "    \n",
    "    Returns:\n",
    "    dict: Validation results with detailed statistics and examples of any issues found\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'overall_status': 'PASS',\n",
    "        'tests': {},\n",
    "        'statistics': {},\n",
    "        'issues_found': {},\n",
    "        'sample_issues': {}\n",
    "    }\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def is_lowercase(text):\n",
    "        return text.islower()\n",
    "    \n",
    "    def contains_punctuation(text):\n",
    "        return any(char in string.punctuation for char in text)\n",
    "    \n",
    "    def contains_numbers(text):\n",
    "        return bool(re.search(r'\\d', text))\n",
    "    \n",
    "    def contains_special_chars(text):\n",
    "        return bool(re.search(r'[^a-zA-Z\\s]', text))\n",
    "    \n",
    "    def is_stopword(text):\n",
    "        return text in stop_words\n",
    "    \n",
    "    # Initialize counters for statistics\n",
    "    stats = {\n",
    "        'total_original_tokens': 0,\n",
    "        'total_normalized_tokens': 0,\n",
    "        'uppercase_found': 0,\n",
    "        'punctuation_found': 0,\n",
    "        'numbers_found': 0,\n",
    "        'special_chars_found': 0,\n",
    "        'stopwords_found': 0\n",
    "    }\n",
    "    \n",
    "    # Initialize issue tracking\n",
    "    issues = {\n",
    "        'uppercase_tokens': [],\n",
    "        'punctuation_tokens': [],\n",
    "        'number_tokens': [],\n",
    "        'special_char_tokens': [],\n",
    "        'stopword_tokens': []\n",
    "    }\n",
    "    \n",
    "    # Check each row\n",
    "    for idx, row in df.iterrows():\n",
    "        if 'tokens_normalized' not in row:\n",
    "            results['overall_status'] = 'FAIL'\n",
    "            results['tests']['normalization_column_exists'] = False\n",
    "            return results\n",
    "        \n",
    "        normalized_tokens = row['tokens_normalized']\n",
    "        \n",
    "        if not isinstance(normalized_tokens, list):\n",
    "            continue\n",
    "            \n",
    "        # Check each normalized token\n",
    "        for token in normalized_tokens:\n",
    "            stats['total_normalized_tokens'] += 1\n",
    "            \n",
    "            # Check for uppercase\n",
    "            if not is_lowercase(token):\n",
    "                stats['uppercase_found'] += 1\n",
    "                if len(issues['uppercase_tokens']) < 5:\n",
    "                    issues['uppercase_tokens'].append((idx, token))\n",
    "            \n",
    "            # Check for punctuation\n",
    "            if contains_punctuation(token):\n",
    "                stats['punctuation_found'] += 1\n",
    "                if len(issues['punctuation_tokens']) < 5:\n",
    "                    issues['punctuation_tokens'].append((idx, token))\n",
    "            \n",
    "            # Check for numbers\n",
    "            if contains_numbers(token):\n",
    "                stats['numbers_found'] += 1\n",
    "                if len(issues['number_tokens']) < 5:\n",
    "                    issues['number_tokens'].append((idx, token))\n",
    "            \n",
    "            # Check for special characters\n",
    "            if contains_special_chars(token):\n",
    "                stats['special_chars_found'] += 1\n",
    "                if len(issues['special_char_tokens']) < 5:\n",
    "                    issues['special_char_tokens'].append((idx, token))\n",
    "            \n",
    "            # Check for stopwords\n",
    "            if is_stopword(token):\n",
    "                stats['stopwords_found'] += 1\n",
    "                if len(issues['stopword_tokens']) < 5:\n",
    "                    issues['stopword_tokens'].append((idx, token))\n",
    "    \n",
    "    # Calculate pass/fail for each test\n",
    "    tests = {\n",
    "        'uppercase_test': stats['uppercase_found'] == 0,\n",
    "        'punctuation_test': stats['punctuation_found'] == 0,\n",
    "        'numbers_test': stats['numbers_found'] == 0,\n",
    "        'special_chars_test': stats['special_chars_found'] == 0,\n",
    "        'stopwords_test': stats['stopwords_found'] == 0\n",
    "    }\n",
    "    \n",
    "    # Update overall status\n",
    "    if not all(tests.values()):\n",
    "        results['overall_status'] = 'FAIL'\n",
    "    \n",
    "    # Calculate percentages for statistics\n",
    "    total_tokens = stats['total_normalized_tokens']\n",
    "    if total_tokens > 0:\n",
    "        stats.update({\n",
    "            'uppercase_percentage': (stats['uppercase_found'] / total_tokens) * 100,\n",
    "            'punctuation_percentage': (stats['punctuation_found'] / total_tokens) * 100,\n",
    "            'numbers_percentage': (stats['numbers_found'] / total_tokens) * 100,\n",
    "            'special_chars_percentage': (stats['special_chars_found'] / total_tokens) * 100,\n",
    "            'stopwords_percentage': (stats['stopwords_found'] / total_tokens) * 100\n",
    "        })\n",
    "    \n",
    "    # Compile results\n",
    "    results['tests'] = tests\n",
    "    results['statistics'] = stats\n",
    "    results['issues_found'] = {k: len(v) for k, v in issues.items()}\n",
    "    results['sample_issues'] = issues\n",
    "    \n",
    "    # Print summary report\n",
    "    print(\"\\nText Normalization Validation Report\")\n",
    "    print(\"====================================\")\n",
    "    print(f\"Overall Status: {results['overall_status']}\")\n",
    "    print(\"\\nTest Results:\")\n",
    "    for test, passed in tests.items():\n",
    "        print(f\"- {test}: {'PASS' if passed else 'FAIL'}\")\n",
    "    \n",
    "    print(\"\\nStatistics:\")\n",
    "    print(f\"- Total normalized tokens: {stats['total_normalized_tokens']}\")\n",
    "    if total_tokens > 0:\n",
    "        print(f\"- Uppercase tokens: {stats['uppercase_found']} ({stats['uppercase_percentage']:.2f}%)\")\n",
    "        print(f\"- Tokens with punctuation: {stats['punctuation_found']} ({stats['punctuation_percentage']:.2f}%)\")\n",
    "        print(f\"- Tokens with numbers: {stats['numbers_found']} ({stats['numbers_percentage']:.2f}%)\")\n",
    "        print(f\"- Tokens with special characters: {stats['special_chars_found']} ({stats['special_chars_percentage']:.2f}%)\")\n",
    "        print(f\"- Stopwords found: {stats['stopwords_found']} ({stats['stopwords_percentage']:.2f}%)\")\n",
    "    \n",
    "    if results['overall_status'] == 'FAIL':\n",
    "        print(\"\\nSample Issues Found:\")\n",
    "        for issue_type, samples in issues.items():\n",
    "            if samples:\n",
    "                print(f\"\\n{issue_type}:\")\n",
    "                for idx, token in samples:\n",
    "                    print(f\"- Row {idx}: '{token}'\")\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Print the results  <a class=\"anchor\" id=\"result-printing\"></a>"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "# Run the normalization\n",
    "df_short = text_normalization(df_short, 'tokens')\n",
    "\n",
    "# Verify the results\n",
    "print(\"\\nResults verification:\")\n",
    "print(\"Sample of normalized tokens (first 3 rows):\")\n",
    "print(df_short['tokens_normalized'].head(3))\n",
    "\n",
    "\n",
    "results = check_text_normalization(df_short)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### CoNNL-U format  <a class=\"anchor\" id=\"#connlu-format\"></a>\n",
    "Now we will write a function to store the data in CoNLL-U format to ensure *reproducibility* and *platform independency*. Ten text parts will be written in one file in CoNNL-U format. \n",
    "If the execution of a cell is aborted, the currently open file will be closed."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse')\n",
    "\n",
    "def convert_to_connlu(dataframe, column_name):\n",
    "    \"\"\"\n",
    "    Function that takes a dataframe and converts each row in a CoNNL-U file. Each file consists of ten text parts\n",
    "    in the CoNNL-U format.\n",
    "    \"\"\"\n",
    "    output_dir = os.path.join(\"..\", \"CoNLL\")\n",
    "    \n",
    "    file_index = 1\n",
    "    sentence_count = 0\n",
    "    total_sentences = 0\n",
    "    output_file = os.path.join(output_dir, f\"output_{file_index}.conllu\")\n",
    "    \n",
    "    # Converting text parts to CoNNL-U format and closing files again. \n",
    "    try:        \n",
    "        f = open(output_file, \"w\", encoding=\"utf-8\")\n",
    "    \n",
    "        for idx, row in dataframe.iterrows():\n",
    "            sentence = \" \".join(row[f\"{column_name}_normalized\"])\n",
    "            doc = nlp(sentence)\n",
    "            CoNLL.write_doc2conll(doc, f)\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            sentence_count += 1\n",
    "            total_sentences += 1\n",
    "            \n",
    "            # Closing file after ten converted text parts to one CoNNL-U file\n",
    "            if sentence_count >= 10:\n",
    "                f.close()\n",
    "                file_index += 1\n",
    "                output_file = os.path.join(output_dir, f\"output_{file_index}.conllu\")\n",
    "                f = open(output_file, \"w\", encoding=\"utf-8\")\n",
    "                sentence_count = 0 \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nStopped running. Closing open files...\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error happened {e}\")\n",
    "    # If the cell gets aborted, any open file is\n",
    "    # closed so that there is no remaining open file when the cell is stopped.\n",
    "    finally:\n",
    "        if not f.closed:\n",
    "            f.close()\n",
    "        \n",
    "    created_files = len([name for name in os.listdir(output_dir) if name.startswith('output') and name.endswith('.conllu')])\n",
    "    print(f\"\\nTotal sentences processed: {total_sentences}\")\n",
    "    print(f\"Total files created: {created_files}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we call the function *convert_to_connlu* on our dataframe *df_short* to receive files in CoNNL-U format consisting of ten text parts per file."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#only run if the conll files are not already created\n",
    "#convert_to_connlu(df_short, 'tokens')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Creating function to load tokens in a dataframe, keeping the CoNNL-u format in columns."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T22:30:02.113362Z",
     "start_time": "2024-12-11T22:30:01.849059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_conllu_to_dataframe(directory):\n",
    "    \"\"\"\n",
    "    Function that takes the directory string where the CoNNL-u files are located and loops though each files, extracting each word with\n",
    "    its corresponding CoNNL-u features. At the end, all tokens including its features are stored in a dataframe. Each word is assigned a word_id.\n",
    "    Each single narrative is assigned a narrative_id\n",
    "    :returns df\n",
    "    \"\"\"\n",
    "    columns = ['narrative_id', 'word_id', 'form', 'lemma', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc']\n",
    "    data = []\n",
    "    narrative_id = 0\n",
    "\n",
    "    # Looping though all files\n",
    "    for filename in sorted(os.listdir(directory)):\n",
    "        if filename.endswith('.conllu'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r') as f:\n",
    "                word_id = 0\n",
    "                consecutive_comments = 0\n",
    "\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "\n",
    "                    # Checking for comment lines, indicated by two hashtags. If new narrative starts, increment narrative_id\n",
    "                    if line.startswith('#'):\n",
    "                        consecutive_comments += 1\n",
    "                        if consecutive_comments == 2:\n",
    "                            narrative_id += 1\n",
    "                            word_id = 0\n",
    "                        continue\n",
    "                    else:\n",
    "                        consecutive_comments = 0\n",
    "\n",
    "                    # Continuing for empty strokes\n",
    "                    if not line:\n",
    "                        continue\n",
    "\n",
    "                    parts = line.split('\\t')\n",
    "                    if len(parts) == 10:\n",
    "                        word_id += 1\n",
    "                        row = [narrative_id, word_id] + parts[1:]\n",
    "                        data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df\n",
    "\n",
    "# Extract all narratives from all CoNNL-u files to dataframe df_connlu\n",
    "directory_path = \"../CoNLL\"\n",
    "df_connlu = extract_conllu_to_dataframe(\"../CoNLL\")"
   ],
   "outputs": [],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
