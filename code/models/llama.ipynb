{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1+cu121\n",
      "CUDA Available: True\n",
      "GPU Device: NVIDIA L40S\n",
      "GPU Memory: 47.81 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import wandb\n",
    "from transformers import LlamaTokenizer, LlamaForSequenceClassification, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import sentencepiece\n",
    "from tqdm.notebook import tqdm\n",
    "from huggingface_hub import login\n",
    "login('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')\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "from datasets import Dataset\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check CUDA availability\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1694 entries, 0 to 1693\n",
      "Data columns (total 8 columns):\n",
      " #   Column                        Non-Null Count  Dtype \n",
      "---  ------                        --------------  ----- \n",
      " 0   filename                      1694 non-null   object\n",
      " 1   language                      1694 non-null   object\n",
      " 2   content                       1694 non-null   object\n",
      " 3   topic                         1694 non-null   object\n",
      " 4   narrative_subnarrative_pairs  1694 non-null   object\n",
      " 5   target_indices                1694 non-null   object\n",
      " 6   tokens                        1694 non-null   object\n",
      " 7   tokens_normalized             1694 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 106.0+ KB\n",
      "None\n",
      "\n",
      "Number of records: 1694\n",
      "\n",
      "UA Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1175 entries, 0 to 1174\n",
      "Data columns (total 8 columns):\n",
      " #   Column                        Non-Null Count  Dtype \n",
      "---  ------                        --------------  ----- \n",
      " 0   filename                      1175 non-null   object\n",
      " 1   language                      1175 non-null   object\n",
      " 2   content                       1175 non-null   object\n",
      " 3   topic                         1175 non-null   object\n",
      " 4   narrative_subnarrative_pairs  1175 non-null   object\n",
      " 5   target_indices                1175 non-null   object\n",
      " 6   tokens                        1175 non-null   object\n",
      " 7   tokens_normalized             1175 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 73.6+ KB\n",
      "None\n",
      "\n",
      "Number of UA records: 1175\n",
      "\n",
      "CC Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 519 entries, 0 to 518\n",
      "Data columns (total 8 columns):\n",
      " #   Column                        Non-Null Count  Dtype \n",
      "---  ------                        --------------  ----- \n",
      " 0   filename                      519 non-null    object\n",
      " 1   language                      519 non-null    object\n",
      " 2   content                       519 non-null    object\n",
      " 3   topic                         519 non-null    object\n",
      " 4   narrative_subnarrative_pairs  519 non-null    object\n",
      " 5   target_indices                519 non-null    object\n",
      " 6   tokens                        519 non-null    object\n",
      " 7   tokens_normalized             519 non-null    object\n",
      "dtypes: object(8)\n",
      "memory usage: 32.6+ KB\n",
      "None\n",
      "\n",
      "Number of CC records: 519\n",
      "\n",
      "Sample row from full dataset:\n",
      "filename                                                         EN_CC_100013.txt\n",
      "language                                                                       EN\n",
      "content                         Bill Gates Says He Is â€˜The Solutionâ€™ To Climat...\n",
      "topic                                                                          CC\n",
      "narrative_subnarrative_pairs    [{'narrative': 'Criticism of climate movement'...\n",
      "target_indices                                                               [14]\n",
      "tokens                          [['Bill', 'Gates', 'Says', 'He', 'Is', 'â€˜', 'T...\n",
      "tokens_normalized               ['bill', 'gate', 'say', 'solution', 'climate',...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "base_path = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Load preprocessed data\n",
    "input_file_full = os.path.join(base_path, \"df_normalized.csv\")\n",
    "df_normalized = pd.read_csv(input_file_full)\n",
    "df = pd.read_csv(input_file_full)\n",
    "\n",
    "input_file_ua = os.path.join(base_path, \"df_normalized_ua.csv\")\n",
    "df_normalized_ua = pd.read_csv(input_file_ua)\n",
    "\n",
    "input_file_cc = os.path.join(base_path, \"df_normalized_cc.csv\")\n",
    "df_normalized_cc = pd.read_csv(input_file_cc)\n",
    "\n",
    "# Display dataset information\n",
    "print(\"\\nFull Dataset Info:\")\n",
    "print(df_normalized.info())\n",
    "print(f\"\\nNumber of records: {len(df_normalized)}\")\n",
    "\n",
    "print(\"\\nUA Dataset Info:\")\n",
    "print(df_normalized_ua.info())\n",
    "print(f\"\\nNumber of UA records: {len(df_normalized_ua)}\")\n",
    "\n",
    "print(\"\\nCC Dataset Info:\")\n",
    "print(df_normalized_cc.info())\n",
    "print(f\"\\nNumber of CC records: {len(df_normalized_cc)}\")\n",
    "\n",
    "# Display sample rows\n",
    "print(\"\\nSample row from full dataset:\")\n",
    "print(df_normalized.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom Dataset for loading Llama input data\"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        \n",
    "        # Debug info\n",
    "        print(f\"Dataset created with {len(self.labels)} samples\")\n",
    "        print(f\"Label distribution: {pd.Series(self.labels).value_counts()}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"Compute evaluation metrics\"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"micro\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    # Computing confusion matrix per class\n",
    "    unique_classes = np.unique(labels)\n",
    "    cm_per_class = {}\n",
    "    \n",
    "    for class_idx in unique_classes:\n",
    "        binary_labels = (labels == class_idx).astype(int)\n",
    "        binary_preds = (preds == class_idx).astype(int)\n",
    "        cm = confusion_matrix(binary_labels, binary_preds)\n",
    "        cm_per_class[f\"Class_{class_idx}\"] = cm.tolist()\n",
    "        \n",
    "        # Print per-class metrics for debugging\n",
    "        print(f\"\\nMetrics for Class {class_idx}:\")\n",
    "        print(f\"Confusion Matrix:\\n{cm}\")\n",
    "        class_precision = precision_recall_fscore_support(binary_labels, binary_preds, average='binary')[0]\n",
    "        class_recall = precision_recall_fscore_support(binary_labels, binary_preds, average='binary')[1]\n",
    "        print(f\"Precision: {class_precision:.4f}\")\n",
    "        print(f\"Recall: {class_recall:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'confusion_matrix': cm_per_class\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_narrative_key(narrative_dict):\n",
    "    \"\"\"Extract key from narrative dictionary for classification\"\"\"\n",
    "    if isinstance(narrative_dict, str):\n",
    "        narrative_dict = eval(narrative_dict)\n",
    "    return narrative_dict['narrative']  # or you could use narrative_dict['subnarrative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_classification_head(model, train_dataset, tokenizer, output_dir):\n",
    "    \"\"\"Pre-train the classification head before fine-tuning the full model\"\"\"\n",
    "    try:\n",
    "        print(\"\\nPre-training classification head...\")\n",
    "        \n",
    "        # Freeze all layers except classification head\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"score\" not in name:  # Freeze everything except score layer\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        # Print trainable parameters\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        all_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"Trainable parameters for head pre-training: {trainable_params:,} ({trainable_params/all_params:.2%} of total)\")\n",
    "        \n",
    "        # Training arguments for head pre-training\n",
    "        head_training_args = TrainingArguments(\n",
    "            output_dir=os.path.join(output_dir, \"head_pretraining\"),\n",
    "            num_train_epochs=1,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            learning_rate=1e-3,\n",
    "            warmup_ratio=0.1,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=50,\n",
    "            save_strategy=\"no\",\n",
    "            logging_dir=os.path.join(output_dir, \"head_logs\"),\n",
    "            logging_steps=10,\n",
    "            remove_unused_columns=False,\n",
    "            report_to=\"wandb\"\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer just for head\n",
    "        head_trainer = Trainer(\n",
    "            model=model,\n",
    "            args=head_training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "        \n",
    "        # Train only the head\n",
    "        head_trainer.train()\n",
    "        \n",
    "        print(\"\\nClassification head pre-training completed\")\n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in classification head pre-training: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "openlm-research/open_llama_7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_llama(df, base_path, model_name=\"openlm-research/open_llama_7b\"):\n",
    "    \"\"\"Train Llama model with classification head pre-training\"\"\"\n",
    "    try:\n",
    "        current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "        \n",
    "        # Create output directories\n",
    "        output_dir = os.path.join(base_path, f\"models/llama_{current_date}\")\n",
    "        log_dir = os.path.join(base_path, f\"logs/llama_{current_date}\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "        # Initialize wandb\n",
    "        wandb.init(project=\"llama-classification\", name=f\"llama-classification-{current_date}\")\n",
    "\n",
    "        # Create narrative mapping\n",
    "        print(\"\\nCreating narrative mapping...\")\n",
    "        narratives = df['narrative_subnarrative_pairs'].apply(\n",
    "            lambda x: eval(x)[0] if isinstance(x, str) else x[0]\n",
    "        ).tolist()\n",
    "        \n",
    "        unique_narratives = set(get_narrative_key(n) for n in narratives)\n",
    "        label_mapping = {narrative: idx for idx, narrative in enumerate(sorted(unique_narratives))}\n",
    "        \n",
    "        print(f\"Number of unique narratives: {len(unique_narratives)}\")\n",
    "\n",
    "        # Prepare data\n",
    "        df_train, df_val = train_test_split(df, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Process texts and labels\n",
    "        train_texts = df_train['tokens_normalized'].apply(\n",
    "            lambda x: ' '.join(x) if isinstance(x, list) else x\n",
    "        ).tolist()\n",
    "        val_texts = df_val['tokens_normalized'].apply(\n",
    "            lambda x: ' '.join(x) if isinstance(x, list) else x\n",
    "        ).tolist()\n",
    "        \n",
    "        train_labels = [\n",
    "            label_mapping[get_narrative_key(eval(n)[0] if isinstance(n, str) else n[0])]\n",
    "            for n in df_train['narrative_subnarrative_pairs']\n",
    "        ]\n",
    "        val_labels = [\n",
    "            label_mapping[get_narrative_key(eval(n)[0] if isinstance(n, str) else n[0])]\n",
    "            for n in df_val['narrative_subnarrative_pairs']\n",
    "        ]\n",
    "\n",
    "        # Initialize tokenizer\n",
    "        print(\"\\nInitializing tokenizer...\")\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "        # Tokenize texts\n",
    "        train_encodings = tokenizer(\n",
    "            train_texts, \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            max_length=512\n",
    "        )\n",
    "        val_encodings = tokenizer(\n",
    "            val_texts, \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            max_length=512\n",
    "        )\n",
    "\n",
    "        # Create datasets\n",
    "        train_dataset = Dataset.from_dict({\n",
    "            'input_ids': train_encodings['input_ids'],\n",
    "            'attention_mask': train_encodings['attention_mask'],\n",
    "            'labels': train_labels\n",
    "        })\n",
    "        val_dataset = Dataset.from_dict({\n",
    "            'input_ids': val_encodings['input_ids'],\n",
    "            'attention_mask': val_encodings['attention_mask'],\n",
    "            'labels': val_labels\n",
    "        })\n",
    "\n",
    "        # Setup quantization configuration\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "\n",
    "        print(\"\\nInitializing model...\")\n",
    "        model = LlamaForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=len(label_mapping),\n",
    "            torch_dtype=torch.float16,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map='auto'\n",
    "        )\n",
    "\n",
    "        # Prepare model for k-bit training\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "        # Pre-train classification head\n",
    "        print(\"\\nStarting classification head pre-training...\")\n",
    "        model = initialize_classification_head(model, train_dataset, tokenizer, output_dir)\n",
    "\n",
    "        # Configure LoRA\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            inference_mode=False,\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "            bias=\"none\",\n",
    "        )\n",
    "\n",
    "        # Get PEFT model\n",
    "        print(\"\\nApplying LoRA adapters...\")\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "        # Training arguments for full model fine-tuning\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            run_name=f\"llama-classification-run-{current_date}\",\n",
    "            num_train_epochs=3,\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=4,\n",
    "            learning_rate=2e-4,\n",
    "            warmup_ratio=0.03,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_dir=log_dir,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model='eval_loss',\n",
    "            greater_is_better=False,\n",
    "            logging_steps=10,\n",
    "            gradient_accumulation_steps=2,\n",
    "            gradient_checkpointing=True,\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "            remove_unused_columns=False,\n",
    "            report_to=\"wandb\"\n",
    "        )\n",
    "\n",
    "        # Initialize trainer for full model fine-tuning\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "        # Train full model\n",
    "        print(\"\\nStarting full model fine-tuning...\")\n",
    "        trainer.train()\n",
    "\n",
    "        # Evaluate model\n",
    "        print(\"\\nEvaluating model...\")\n",
    "        results = trainer.evaluate()\n",
    "        \n",
    "        # Save model and tokenizer\n",
    "        print(\"\\nSaving model...\")\n",
    "        trainer.save_model(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "        # End wandb run\n",
    "        wandb.finish()\n",
    "\n",
    "        return results, model, tokenizer, label_mapping\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Llama training: {str(e)}\")\n",
    "        wandb.finish()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_misclassifications(dataset, model, tokenizer, label_mapping, dataset_type=\"Training\"):\n",
    "    \"\"\"Debug misclassified examples with detailed output and proper device handling\"\"\"\n",
    "    try:\n",
    "        print(f\"\\nAnalyzing misclassifications in {dataset_type} dataset...\")\n",
    "        \n",
    "        # Determine device\n",
    "        device = model.device\n",
    "        print(f\"Model is on device: {device}\")\n",
    "        \n",
    "        # Prepare data\n",
    "        texts = dataset['tokens_normalized'].apply(\n",
    "            lambda x: ' '.join(x) if isinstance(x, list) else x\n",
    "        ).tolist()\n",
    "        \n",
    "        true_labels = [\n",
    "            label_mapping[get_narrative_key(eval(n)[0] if isinstance(n, str) else n[0])]\n",
    "            for n in dataset['narrative_subnarrative_pairs']\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nTotal samples to analyze: {len(texts)}\")\n",
    "\n",
    "        # Get predictions in batches to manage memory\n",
    "        batch_size = 8\n",
    "        predictions = []\n",
    "        confidences = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            # Tokenize batch\n",
    "            encodings = tokenizer(\n",
    "                batch_texts, \n",
    "                truncation=True, \n",
    "                padding=True, \n",
    "                max_length=512, \n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Move encodings to same device as model\n",
    "            encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**encodings)\n",
    "                batch_preds = outputs.logits.argmax(-1)\n",
    "                batch_confs = torch.softmax(outputs.logits, dim=-1).max(dim=-1)[0]\n",
    "                \n",
    "                # Move predictions back to CPU\n",
    "                predictions.extend(batch_preds.cpu().numpy())\n",
    "                confidences.extend(batch_confs.cpu().numpy())\n",
    "\n",
    "        # Track misclassifications\n",
    "        misclassifications = []\n",
    "        for idx, (pred, true, conf) in enumerate(zip(predictions, true_labels, confidences)):\n",
    "            if pred != true:\n",
    "                misclassifications.append({\n",
    "                    'text': texts[idx][:200],  # First 200 chars for brevity\n",
    "                    'predicted': pred,\n",
    "                    'actual': true,\n",
    "                    'confidence': conf,\n",
    "                    'dataset_type': dataset_type\n",
    "                })\n",
    "\n",
    "        # Create DataFrame and display results\n",
    "        misclass_df = pd.DataFrame(misclassifications)\n",
    "        \n",
    "        print(f\"\\nTotal misclassifications: {len(misclass_df)}\")\n",
    "        print(f\"Accuracy: {1 - len(misclass_df)/len(texts):.4f}\")\n",
    "        \n",
    "        if len(misclass_df) > 0:\n",
    "            print(\"\\nMisclassification distribution:\")\n",
    "            print(misclass_df.groupby(['actual', 'predicted']).size().unstack(fill_value=0))\n",
    "            \n",
    "            print(\"\\nSample misclassifications:\")\n",
    "            for i, row in misclass_df.head().iterrows():\n",
    "                print(f\"\\nExample {i+1}:\")\n",
    "                print(f\"Text: {row['text']}\")\n",
    "                print(f\"Predicted: {row['predicted']}, Actual: {row['actual']}\")\n",
    "                print(f\"Confidence: {row['confidence']:.4f}\")\n",
    "        \n",
    "        return misclass_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in debugging misclassifications: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select dataset for training:\n",
      "1. Full dataset\n",
      "2. UA dataset\n",
      "3. CC dataset\n",
      "\n",
      "Training on CC dataset...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/nlp_Backpropagandists_2024/code/models/wandb/run-20250109_223407-sv3fx4pq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/backpropagandists/llama-classification/runs/sv3fx4pq' target=\"_blank\">llama-classification-20250109</a></strong> to <a href='https://wandb.ai/backpropagandists/llama-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/backpropagandists/llama-classification' target=\"_blank\">https://wandb.ai/backpropagandists/llama-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/backpropagandists/llama-classification/runs/sv3fx4pq' target=\"_blank\">https://wandb.ai/backpropagandists/llama-classification/runs/sv3fx4pq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating narrative mapping...\n",
      "Number of unique narratives: 11\n",
      "\n",
      "Initializing tokenizer...\n",
      "\n",
      "Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 22:34:10,106 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22f6924e4344c428b61936b029e063c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at openlm-research/open_llama_7b and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying LoRA adapters...\n",
      "trainable params: 16,822,272 || all params: 6,624,210,944 || trainable%: 0.2540\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='156' max='156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [156/156 16:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.893900</td>\n",
       "      <td>1.736891</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>{'Class_0': [[42, 22], [1, 39]], 'Class_1': [[103, 0], [1, 0]], 'Class_3': [[96, 0], [8, 0]], 'Class_4': [[100, 0], [4, 0]], 'Class_5': [[88, 4], [11, 1]], 'Class_6': [[97, 0], [7, 0]], 'Class_7': [[103, 0], [1, 0]], 'Class_8': [[96, 0], [8, 0]], 'Class_9': [[60, 22], [6, 16]], 'Class_10': [[103, 0], [1, 0]]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.812100</td>\n",
       "      <td>1.574796</td>\n",
       "      <td>0.548077</td>\n",
       "      <td>0.548077</td>\n",
       "      <td>0.548077</td>\n",
       "      <td>0.548077</td>\n",
       "      <td>{'Class_0': [[53, 11], [5, 35]], 'Class_1': [[103, 0], [1, 0]], 'Class_3': [[94, 2], [8, 0]], 'Class_4': [[95, 5], [2, 2]], 'Class_5': [[80, 12], [5, 7]], 'Class_6': [[95, 2], [7, 0]], 'Class_7': [[103, 0], [1, 0]], 'Class_8': [[95, 1], [8, 0]], 'Class_9': [[70, 12], [9, 13]], 'Class_10': [[102, 1], [1, 0]]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.616500</td>\n",
       "      <td>1.765414</td>\n",
       "      <td>0.528846</td>\n",
       "      <td>0.528846</td>\n",
       "      <td>0.528846</td>\n",
       "      <td>0.528846</td>\n",
       "      <td>{'Class_0': [[49, 15], [4, 36]], 'Class_1': [[103, 0], [1, 0]], 'Class_3': [[93, 3], [8, 0]], 'Class_4': [[98, 2], [3, 1]], 'Class_5': [[81, 11], [8, 4]], 'Class_6': [[94, 3], [7, 0]], 'Class_7': [[103, 0], [1, 0]], 'Class_8': [[96, 0], [8, 0]], 'Class_9': [[68, 14], [8, 14]], 'Class_10': [[103, 0], [1, 0]]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Class 0:\n",
      "Confusion Matrix:\n",
      "[[42 22]\n",
      " [ 1 39]]\n",
      "Precision: 0.6393\n",
      "Recall: 0.9750\n",
      "\n",
      "Metrics for Class 1:\n",
      "Confusion Matrix:\n",
      "[[103   0]\n",
      " [  1   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 3:\n",
      "Confusion Matrix:\n",
      "[[96  0]\n",
      " [ 8  0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 4:\n",
      "Confusion Matrix:\n",
      "[[100   0]\n",
      " [  4   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 5:\n",
      "Confusion Matrix:\n",
      "[[88  4]\n",
      " [11  1]]\n",
      "Precision: 0.2000\n",
      "Recall: 0.0833\n",
      "\n",
      "Metrics for Class 6:\n",
      "Confusion Matrix:\n",
      "[[97  0]\n",
      " [ 7  0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 7:\n",
      "Confusion Matrix:\n",
      "[[103   0]\n",
      " [  1   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 8:\n",
      "Confusion Matrix:\n",
      "[[96  0]\n",
      " [ 8  0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 9:\n",
      "Confusion Matrix:\n",
      "[[60 22]\n",
      " [ 6 16]]\n",
      "Precision: 0.4211\n",
      "Recall: 0.7273\n",
      "\n",
      "Metrics for Class 10:\n",
      "Confusion Matrix:\n",
      "[[103   0]\n",
      " [  1   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Class 0:\n",
      "Confusion Matrix:\n",
      "[[53 11]\n",
      " [ 5 35]]\n",
      "Precision: 0.7609\n",
      "Recall: 0.8750\n",
      "\n",
      "Metrics for Class 1:\n",
      "Confusion Matrix:\n",
      "[[103   0]\n",
      " [  1   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 3:\n",
      "Confusion Matrix:\n",
      "[[94  2]\n",
      " [ 8  0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 4:\n",
      "Confusion Matrix:\n",
      "[[95  5]\n",
      " [ 2  2]]\n",
      "Precision: 0.2857\n",
      "Recall: 0.5000\n",
      "\n",
      "Metrics for Class 5:\n",
      "Confusion Matrix:\n",
      "[[80 12]\n",
      " [ 5  7]]\n",
      "Precision: 0.3684\n",
      "Recall: 0.5833\n",
      "\n",
      "Metrics for Class 6:\n",
      "Confusion Matrix:\n",
      "[[95  2]\n",
      " [ 7  0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 7:\n",
      "Confusion Matrix:\n",
      "[[103   0]\n",
      " [  1   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 8:\n",
      "Confusion Matrix:\n",
      "[[95  1]\n",
      " [ 8  0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 9:\n",
      "Confusion Matrix:\n",
      "[[70 12]\n",
      " [ 9 13]]\n",
      "Precision: 0.5200\n",
      "Recall: 0.5909\n",
      "\n",
      "Metrics for Class 10:\n",
      "Confusion Matrix:\n",
      "[[102   1]\n",
      " [  1   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Class 0:\n",
      "Confusion Matrix:\n",
      "[[49 15]\n",
      " [ 4 36]]\n",
      "Precision: 0.7059\n",
      "Recall: 0.9000\n",
      "\n",
      "Metrics for Class 1:\n",
      "Confusion Matrix:\n",
      "[[103   0]\n",
      " [  1   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 3:\n",
      "Confusion Matrix:\n",
      "[[93  3]\n",
      " [ 8  0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 4:\n",
      "Confusion Matrix:\n",
      "[[98  2]\n",
      " [ 3  1]]\n",
      "Precision: 0.3333\n",
      "Recall: 0.2500\n",
      "\n",
      "Metrics for Class 5:\n",
      "Confusion Matrix:\n",
      "[[81 11]\n",
      " [ 8  4]]\n",
      "Precision: 0.2667\n",
      "Recall: 0.3333\n",
      "\n",
      "Metrics for Class 6:\n",
      "Confusion Matrix:\n",
      "[[94  3]\n",
      " [ 7  0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 7:\n",
      "Confusion Matrix:\n",
      "[[103   0]\n",
      " [  1   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 8:\n",
      "Confusion Matrix:\n",
      "[[96  0]\n",
      " [ 8  0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 9:\n",
      "Confusion Matrix:\n",
      "[[68 14]\n",
      " [ 8 14]]\n",
      "Precision: 0.5000\n",
      "Recall: 0.6364\n",
      "\n",
      "Metrics for Class 10:\n",
      "Confusion Matrix:\n",
      "[[103   0]\n",
      " [  1   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Class 0:\n",
      "Confusion Matrix:\n",
      "[[53 11]\n",
      " [ 5 35]]\n",
      "Precision: 0.7609\n",
      "Recall: 0.8750\n",
      "\n",
      "Metrics for Class 1:\n",
      "Confusion Matrix:\n",
      "[[103   0]\n",
      " [  1   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 3:\n",
      "Confusion Matrix:\n",
      "[[94  2]\n",
      " [ 8  0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 4:\n",
      "Confusion Matrix:\n",
      "[[95  5]\n",
      " [ 2  2]]\n",
      "Precision: 0.2857\n",
      "Recall: 0.5000\n",
      "\n",
      "Metrics for Class 5:\n",
      "Confusion Matrix:\n",
      "[[80 12]\n",
      " [ 5  7]]\n",
      "Precision: 0.3684\n",
      "Recall: 0.5833\n",
      "\n",
      "Metrics for Class 6:\n",
      "Confusion Matrix:\n",
      "[[95  2]\n",
      " [ 7  0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 7:\n",
      "Confusion Matrix:\n",
      "[[103   0]\n",
      " [  1   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 8:\n",
      "Confusion Matrix:\n",
      "[[95  1]\n",
      " [ 8  0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 9:\n",
      "Confusion Matrix:\n",
      "[[70 12]\n",
      " [ 9 13]]\n",
      "Precision: 0.5200\n",
      "Recall: 0.5909\n",
      "\n",
      "Metrics for Class 10:\n",
      "Confusion Matrix:\n",
      "[[102   1]\n",
      " [  1   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>â–…â–ˆâ–â–ˆ</td></tr><tr><td>eval/f1</td><td>â–…â–ˆâ–â–ˆ</td></tr><tr><td>eval/loss</td><td>â–‡â–â–ˆâ–</td></tr><tr><td>eval/precision</td><td>â–…â–ˆâ–â–ˆ</td></tr><tr><td>eval/recall</td><td>â–…â–ˆâ–â–ˆ</td></tr><tr><td>eval/runtime</td><td>â–â–ˆâ–‡â–ˆ</td></tr><tr><td>eval/samples_per_second</td><td>â–ˆâ–â–‚â–</td></tr><tr><td>eval/steps_per_second</td><td>â–ˆâ–â–‚â–</td></tr><tr><td>train/epoch</td><td>â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–ˆâ–„â–ƒâ–…â–„â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–…â–ƒâ–â–‚</td></tr><tr><td>train/learning_rate</td><td>â–ˆâ–ˆâ–‡â–‡â–†â–†â–…â–…â–„â–„â–ƒâ–ƒâ–‚â–â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–‡â–†â–†â–…â–ƒâ–„â–„â–ƒâ–ƒâ–‚â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.54808</td></tr><tr><td>eval/f1</td><td>0.54808</td></tr><tr><td>eval/loss</td><td>1.5748</td></tr><tr><td>eval/precision</td><td>0.54808</td></tr><tr><td>eval/recall</td><td>0.54808</td></tr><tr><td>eval/runtime</td><td>28.1762</td></tr><tr><td>eval/samples_per_second</td><td>3.691</td></tr><tr><td>eval/steps_per_second</td><td>0.923</td></tr><tr><td>total_flos</td><td>2.483391893078016e+16</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>156</td></tr><tr><td>train/grad_norm</td><td>22.21198</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.6165</td></tr><tr><td>train_loss</td><td>2.16584</td></tr><tr><td>train_runtime</td><td>1016.922</td></tr><tr><td>train_samples_per_second</td><td>1.224</td></tr><tr><td>train_steps_per_second</td><td>0.153</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">llama-classification-20250109</strong> at: <a href='https://wandb.ai/backpropagandists/llama-classification/runs/sv3fx4pq' target=\"_blank\">https://wandb.ai/backpropagandists/llama-classification/runs/sv3fx4pq</a><br> View project at: <a href='https://wandb.ai/backpropagandists/llama-classification' target=\"_blank\">https://wandb.ai/backpropagandists/llama-classification</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250109_223407-sv3fx4pq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose dataset to train on\n",
    "print(\"Select dataset for training:\")\n",
    "print(\"1. Full dataset\")\n",
    "print(\"2. UA dataset\")\n",
    "print(\"3. CC dataset\")\n",
    "choice = input(\"Enter your choice (1-3): \")\n",
    "\n",
    "if choice == \"1\":\n",
    "    print(\"\\nTraining on full dataset...\")\n",
    "    results, model, tokenizer, label_mapping = train_llama(df_normalized, base_path)\n",
    "elif choice == \"2\":\n",
    "    print(\"\\nTraining on UA dataset...\")\n",
    "    results, model, tokenizer, label_mapping = train_llama(df_normalized_ua, base_path)\n",
    "else:\n",
    "    print(\"\\nTraining on CC dataset...\")\n",
    "    results, model, tokenizer, label_mapping = train_llama(df_normalized_cc, base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing misclassifications in Training dataset...\n",
      "Model is on device: cuda:0\n",
      "\n",
      "Total samples to analyze: 519\n",
      "\n",
      "Total misclassifications: 91\n",
      "Accuracy: 0.8247\n",
      "\n",
      "Misclassification distribution:\n",
      "predicted  0   2   3   4   5   6   8   9   10\n",
      "actual                                       \n",
      "0           0   0   0   2   4   0   0   1   1\n",
      "1           1   0   0   0   1   0   0   0   0\n",
      "2           1   0   0   0   1   0   0   0   0\n",
      "3           1   0   0   3   6   0   1   5   0\n",
      "4           2   0   1   0   3   0   0   0   0\n",
      "5           2   1   0   2   0   0   0   3   0\n",
      "6           7   0   1   4   3   0   0   0   0\n",
      "7           0   0   0   1   0   0   0   1   0\n",
      "8           2   0   0   2   1   1   0   5   0\n",
      "9          10   0   1   2   2   2   1   0   0\n",
      "10          2   0   0   0   0   0   0   1   0\n",
      "\n",
      "Sample misclassifications:\n",
      "\n",
      "Example 1:\n",
      "Text: ['bill', 'gate', 'say', 'solution', 'climate', 'change', 'ok', 'four', 'private', 'jet', 'bill', 'gate', 'right', 'fly', 'around', 'world', 'private', 'jet', 'normal', 'person', 'force', 'live', 'minu\n",
      "Predicted: 9, Actual: 3\n",
      "Confidence: 0.6975\n",
      "\n",
      "Example 2:\n",
      "Text: ['new', 'paper', 'make', 'increase', 'tropical', 'cyclone', 'frequency', 'claim', 'contradicted', 'two', 'year', 'ago', 'noaa', 'noaa', 'july', 'headline', 'research', 'global', 'warming', 'contribute\n",
      "Predicted: 0, Actual: 10\n",
      "Confidence: 0.4533\n",
      "\n",
      "Example 3:\n",
      "Text: ['greta', 'thunb', 'arrest', 'london', 'protest', 'course', 'script', 'attend', 'protest', 'get', 'arrest', 'sad', 'pathetic', 'figure', 'serve', 'agenda', 'elite', 'claim', 'oppose', 'greta', 'thunb'\n",
      "Predicted: 9, Actual: 3\n",
      "Confidence: 0.6793\n",
      "\n",
      "Example 4:\n",
      "Text: ['armistice', 'day', 'empire', 'name', 'change', 'catastrophe', 'follow', 'congress', 'rename', 'armistice', 'day', 'veteran', 'day', 'state', 'reason', 'remember', 'generation', 'veteran', 'veteran',\n",
      "Predicted: 8, Actual: 9\n",
      "Confidence: 0.3626\n",
      "\n",
      "Example 5:\n",
      "Text: ['media', 'weather', 'anomaly', 'propagandist', 'bbc', 'much', 'evident', 'discomfort', 'bbc', 'force', 'acknowledge', 'obvious', 'week', 'uk', 'weather', 'unsettled', 'widespread', 'rain', 'cool', 't\n",
      "Predicted: 3, Actual: 6\n",
      "Confidence: 0.3160\n"
     ]
    }
   ],
   "source": [
    "# Debug misclassifications\n",
    "if choice == \"1\":\n",
    "    misclass_df = debug_misclassifications(df_normalized, model, tokenizer, label_mapping)\n",
    "elif choice == \"2\":\n",
    "    misclass_df = debug_misclassifications(df_normalized_ua, model, tokenizer, label_mapping)\n",
    "else:\n",
    "    misclass_df = debug_misclassifications(df_normalized_cc,model, tokenizer, label_mapping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
