{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "CUDA Available: True\n",
      "GPU Device: NVIDIA GeForce RTX 2080 Ti\n",
      "GPU Memory: 11.81 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import wandb\n",
    "from transformers import LlamaTokenizer, LlamaForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import sentencepiece\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check CUDA availability\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1694 entries, 0 to 1693\n",
      "Data columns (total 8 columns):\n",
      " #   Column                        Non-Null Count  Dtype \n",
      "---  ------                        --------------  ----- \n",
      " 0   filename                      1694 non-null   object\n",
      " 1   language                      1694 non-null   object\n",
      " 2   content                       1694 non-null   object\n",
      " 3   topic                         1694 non-null   object\n",
      " 4   narrative_subnarrative_pairs  1694 non-null   object\n",
      " 5   target_indices                1694 non-null   object\n",
      " 6   tokens                        1694 non-null   object\n",
      " 7   tokens_normalized             1694 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 106.0+ KB\n",
      "None\n",
      "\n",
      "Number of records: 1694\n",
      "\n",
      "UA Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1175 entries, 0 to 1174\n",
      "Data columns (total 8 columns):\n",
      " #   Column                        Non-Null Count  Dtype \n",
      "---  ------                        --------------  ----- \n",
      " 0   filename                      1175 non-null   object\n",
      " 1   language                      1175 non-null   object\n",
      " 2   content                       1175 non-null   object\n",
      " 3   topic                         1175 non-null   object\n",
      " 4   narrative_subnarrative_pairs  1175 non-null   object\n",
      " 5   target_indices                1175 non-null   object\n",
      " 6   tokens                        1175 non-null   object\n",
      " 7   tokens_normalized             1175 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 73.6+ KB\n",
      "None\n",
      "\n",
      "Number of UA records: 1175\n",
      "\n",
      "CC Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 519 entries, 0 to 518\n",
      "Data columns (total 8 columns):\n",
      " #   Column                        Non-Null Count  Dtype \n",
      "---  ------                        --------------  ----- \n",
      " 0   filename                      519 non-null    object\n",
      " 1   language                      519 non-null    object\n",
      " 2   content                       519 non-null    object\n",
      " 3   topic                         519 non-null    object\n",
      " 4   narrative_subnarrative_pairs  519 non-null    object\n",
      " 5   target_indices                519 non-null    object\n",
      " 6   tokens                        519 non-null    object\n",
      " 7   tokens_normalized             519 non-null    object\n",
      "dtypes: object(8)\n",
      "memory usage: 32.6+ KB\n",
      "None\n",
      "\n",
      "Number of CC records: 519\n",
      "\n",
      "Sample row from full dataset:\n",
      "filename                                                         EN_CC_100013.txt\n",
      "language                                                                       EN\n",
      "content                         Bill Gates Says He Is â€˜The Solutionâ€™ To Climat...\n",
      "topic                                                                          CC\n",
      "narrative_subnarrative_pairs    [{'narrative': 'Criticism of climate movement'...\n",
      "target_indices                                                               [14]\n",
      "tokens                          [['Bill', 'Gates', 'Says', 'He', 'Is', 'â€˜', 'T...\n",
      "tokens_normalized               ['bill', 'gate', 'say', 'solution', 'climate',...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "base_path = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Load preprocessed data\n",
    "input_file_full = os.path.join(base_path, \"df_normalized.csv\")\n",
    "df_normalized = pd.read_csv(input_file_full)\n",
    "df = pd.read_csv(input_file_full)\n",
    "\n",
    "input_file_ua = os.path.join(base_path, \"df_normalized_ua.csv\")\n",
    "df_normalized_ua = pd.read_csv(input_file_ua)\n",
    "\n",
    "input_file_cc = os.path.join(base_path, \"df_normalized_cc.csv\")\n",
    "df_normalized_cc = pd.read_csv(input_file_cc)\n",
    "\n",
    "# Display dataset information\n",
    "print(\"\\nFull Dataset Info:\")\n",
    "print(df_normalized.info())\n",
    "print(f\"\\nNumber of records: {len(df_normalized)}\")\n",
    "\n",
    "print(\"\\nUA Dataset Info:\")\n",
    "print(df_normalized_ua.info())\n",
    "print(f\"\\nNumber of UA records: {len(df_normalized_ua)}\")\n",
    "\n",
    "print(\"\\nCC Dataset Info:\")\n",
    "print(df_normalized_cc.info())\n",
    "print(f\"\\nNumber of CC records: {len(df_normalized_cc)}\")\n",
    "\n",
    "# Display sample rows\n",
    "print(\"\\nSample row from full dataset:\")\n",
    "print(df_normalized.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom Dataset for loading Llama input data\"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        \n",
    "        # Debug info\n",
    "        print(f\"Dataset created with {len(self.labels)} samples\")\n",
    "        print(f\"Label distribution: {pd.Series(self.labels).value_counts()}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"Compute evaluation metrics\"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"micro\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    # Computing confusion matrix per class\n",
    "    unique_classes = np.unique(labels)\n",
    "    cm_per_class = {}\n",
    "    \n",
    "    for class_idx in unique_classes:\n",
    "        binary_labels = (labels == class_idx).astype(int)\n",
    "        binary_preds = (preds == class_idx).astype(int)\n",
    "        cm = confusion_matrix(binary_labels, binary_preds)\n",
    "        cm_per_class[f\"Class_{class_idx}\"] = cm.tolist()\n",
    "        \n",
    "        # Print per-class metrics for debugging\n",
    "        print(f\"\\nMetrics for Class {class_idx}:\")\n",
    "        print(f\"Confusion Matrix:\\n{cm}\")\n",
    "        class_precision = precision_recall_fscore_support(binary_labels, binary_preds, average='binary')[0]\n",
    "        class_recall = precision_recall_fscore_support(binary_labels, binary_preds, average='binary')[1]\n",
    "        print(f\"Precision: {class_precision:.4f}\")\n",
    "        print(f\"Recall: {class_recall:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'confusion_matrix': cm_per_class\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_narrative_key(narrative_dict):\n",
    "    \"\"\"Extract key from narrative dictionary for classification\"\"\"\n",
    "    if isinstance(narrative_dict, str):\n",
    "        narrative_dict = eval(narrative_dict)\n",
    "    return narrative_dict['narrative']  # or you could use narrative_dict['subnarrative']\n",
    "\n",
    "def train_llama(df, base_path, model_name=\"openlm-research/open_llama_7b\"):\n",
    "    \"\"\"Train Llama model with detailed debugging outputs\"\"\"\n",
    "    try:\n",
    "        current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "        \n",
    "        # Create output directories\n",
    "        output_dir = os.path.join(base_path, f\"models/llama_{current_date}\")\n",
    "        log_dir = os.path.join(base_path, f\"logs/llama_{current_date}\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nOutput directory: {output_dir}\")\n",
    "        print(f\"Log directory: {log_dir}\")\n",
    "\n",
    "        # Initialize wandb\n",
    "        wandb.init(project=\"llama-classification\", name=f\"llama-classification-{current_date}\")\n",
    "\n",
    "        # Create narrative mapping\n",
    "        print(\"\\nCreating narrative mapping...\")\n",
    "        narratives = df['narrative_subnarrative_pairs'].apply(\n",
    "            lambda x: eval(x)[0] if isinstance(x, str) else x[0]\n",
    "        ).tolist()\n",
    "        \n",
    "        # Extract unique narratives (using main narrative or subnarrative)\n",
    "        unique_narratives = set(get_narrative_key(n) for n in narratives)\n",
    "        label_mapping = {narrative: idx for idx, narrative in enumerate(sorted(unique_narratives))}\n",
    "        \n",
    "        print(f\"Number of unique narratives: {len(unique_narratives)}\")\n",
    "        print(\"\\nSample narrative mappings:\")\n",
    "        for i, (narrative, idx) in enumerate(list(label_mapping.items())[:5]):\n",
    "            print(f\"{idx}: {narrative}\")\n",
    "\n",
    "        # Save label mapping\n",
    "        with open(os.path.join(output_dir, \"label_mapping.json\"), 'w') as f:\n",
    "            json.dump(label_mapping, f, indent=2)\n",
    "\n",
    "        # Prepare data\n",
    "        print(\"\\nPreparing data for training...\")\n",
    "        df_train, df_val = train_test_split(df, test_size=0.2, random_state=42)\n",
    "        \n",
    "        print(f\"Training set size: {len(df_train)}\")\n",
    "        print(f\"Validation set size: {len(df_val)}\")\n",
    "\n",
    "        # Process texts and labels\n",
    "        train_texts = df_train['tokens_normalized'].apply(\n",
    "            lambda x: ' '.join(x) if isinstance(x, list) else x\n",
    "        ).tolist()\n",
    "        val_texts = df_val['tokens_normalized'].apply(\n",
    "            lambda x: ' '.join(x) if isinstance(x, list) else x\n",
    "        ).tolist()\n",
    "        \n",
    "        # Convert narratives to labels using the main narrative\n",
    "        train_labels = [\n",
    "            label_mapping[get_narrative_key(eval(n)[0] if isinstance(n, str) else n[0])]\n",
    "            for n in df_train['narrative_subnarrative_pairs']\n",
    "        ]\n",
    "        val_labels = [\n",
    "            label_mapping[get_narrative_key(eval(n)[0] if isinstance(n, str) else n[0])]\n",
    "            for n in df_val['narrative_subnarrative_pairs']\n",
    "        ]\n",
    "\n",
    "        print(\"\\nSample processed text:\")\n",
    "        print(train_texts[0][:200])\n",
    "        \n",
    "        print(\"\\nLabel distribution in training set:\")\n",
    "        print(pd.Series(train_labels).value_counts())\n",
    "\n",
    "        # Initialize tokenizer\n",
    "        print(\"\\nInitializing tokenizer...\")\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "        print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "        print(f\"Padding token: {tokenizer.pad_token}\")\n",
    "        print(f\"EOS token: {tokenizer.eos_token}\")\n",
    "\n",
    "        # Tokenize texts\n",
    "        print(\"\\nTokenizing texts...\")\n",
    "        train_encodings = tokenizer(\n",
    "            train_texts, \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        val_encodings = tokenizer(\n",
    "            val_texts, \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        print(\"\\nEncoding shapes:\")\n",
    "        for key, val in train_encodings.items():\n",
    "            print(f\"Training {key}: {val.shape}\")\n",
    "\n",
    "        # Create datasets\n",
    "        train_dataset = CustomDataset(train_encodings, train_labels)\n",
    "        val_dataset = CustomDataset(val_encodings, val_labels)\n",
    "\n",
    "        # Initialize model\n",
    "        print(\"\\nInitializing model...\")\n",
    "        model = LlamaForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=len(label_mapping),\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        \n",
    "        if model.config.pad_token_id is None:\n",
    "            model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        \n",
    "        print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            run_name=f\"llama-classification-run-{current_date}\",\n",
    "            num_train_epochs=3,\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=4,\n",
    "            learning_rate=2e-5,\n",
    "            warmup_ratio=0.1,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_dir=log_dir,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model='eval_loss',\n",
    "            greater_is_better=False,\n",
    "            logging_steps=10,\n",
    "            fp16=True,\n",
    "            gradient_accumulation_steps=4,\n",
    "            gradient_checkpointing=True\n",
    "        )\n",
    "\n",
    "        # Initialize trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "        # Train model\n",
    "        print(\"\\nStarting training...\")\n",
    "        trainer.train()\n",
    "\n",
    "        # Evaluate model\n",
    "        print(\"\\nEvaluating model...\")\n",
    "        results = trainer.evaluate()\n",
    "        \n",
    "        print(\"\\nEvaluation results:\")\n",
    "        for metric, value in results.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"{metric}: {value}\")\n",
    "\n",
    "        # Save model and tokenizer\n",
    "        trainer.save_model(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        print(f\"\\nModel and tokenizer saved to {output_dir}\")\n",
    "\n",
    "        # End wandb run\n",
    "        wandb.finish()\n",
    "\n",
    "        return results, model, tokenizer, label_mapping\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Llama training: {str(e)}\")\n",
    "        wandb.finish()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_misclassifications(dataset, model, tokenizer, label_mapping, dataset_type=\"Training\"):\n",
    "    \"\"\"Debug misclassified examples with detailed output\"\"\"\n",
    "    try:\n",
    "        print(f\"\\nAnalyzing misclassifications in {dataset_type} dataset...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        texts = dataset['tokens_normalized'].apply(\n",
    "            lambda x: ' '.join(x) if isinstance(x, list) else x\n",
    "        ).tolist()\n",
    "        \n",
    "        true_labels = [\n",
    "            label_mapping[get_narrative_key(eval(n)[0] if isinstance(n, str) else n[0])]\n",
    "            for n in dataset['narrative_subnarrative_pairs']\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nTotal samples to analyze: {len(texts)}\")\n",
    "\n",
    "        # Get predictions\n",
    "        print(\"\\nGenerating predictions...\")\n",
    "        encodings = tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encodings)\n",
    "            predictions = outputs.logits.argmax(-1)\n",
    "\n",
    "        # Track misclassifications\n",
    "        misclassifications = []\n",
    "        for idx, (pred, true) in enumerate(zip(predictions, true_labels)):\n",
    "            if pred != true:\n",
    "                misclassifications.append({\n",
    "                    'text': texts[idx][:200],\n",
    "                    'predicted': pred.item(),\n",
    "                    'actual': true,\n",
    "                    'confidence': torch.softmax(outputs.logits[idx], dim=0)[pred].item(),\n",
    "                    'dataset_type': dataset_type\n",
    "                })\n",
    "\n",
    "        # Create DataFrame and display results\n",
    "        misclass_df = pd.DataFrame(misclassifications)\n",
    "        \n",
    "        print(f\"\\nTotal misclassifications: {len(misclass_df)}\")\n",
    "        print(f\"Accuracy: {1 - len(misclass_df)/len(texts):.4f}\")\n",
    "        \n",
    "        print(\"\\nMisclassification distribution:\")\n",
    "        print(misclass_df.groupby(['actual', 'predicted']).size().unstack(fill_value=0))\n",
    "        \n",
    "        return misclass_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in debugging misclassifications: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select dataset for training:\n",
      "1. Full dataset\n",
      "2. UA dataset\n",
      "3. CC dataset\n",
      "\n",
      "Training on CC dataset...\n",
      "\n",
      "Output directory: c:\\Users\\krona\\OneDrive - TU Wien\\TU Wien\\1. Semester\\NLP\\nlp_Backpropagandists_2024\\code\\models/llama_20250109\n",
      "Log directory: c:\\Users\\krona\\OneDrive - TU Wien\\TU Wien\\1. Semester\\NLP\\nlp_Backpropagandists_2024\\code\\logs/llama_20250109\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\krona\\OneDrive - TU Wien\\TU Wien\\1. Semester\\NLP\\nlp_Backpropagandists_2024\\code\\models\\wandb\\run-20250109_224037-piqwion0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/backpropagandists/llama-classification/runs/piqwion0' target=\"_blank\">llama-classification-20250109</a></strong> to <a href='https://wandb.ai/backpropagandists/llama-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/backpropagandists/llama-classification' target=\"_blank\">https://wandb.ai/backpropagandists/llama-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/backpropagandists/llama-classification/runs/piqwion0' target=\"_blank\">https://wandb.ai/backpropagandists/llama-classification/runs/piqwion0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating narrative mapping...\n",
      "Number of unique narratives: 11\n",
      "\n",
      "Sample narrative mappings:\n",
      "0: Amplifying Climate Fears\n",
      "1: Climate change is beneficial\n",
      "2: Controversy about green technologies\n",
      "3: Criticism of climate movement\n",
      "4: Criticism of climate policies\n",
      "\n",
      "Preparing data for training...\n",
      "Training set size: 415\n",
      "Validation set size: 104\n",
      "\n",
      "Sample processed text:\n",
      "['eleio', 'autrquica', 'poder', 'ignorar', 'agenda', 'climtica', 'efeito', 'altera', 'climtica', 'j', 'fazer', 'parte', 'dia', 'dia', 'brasileiro', 'h', 'algum', 'tempo', 'maio', 'po', 'comoveuse', 'd\n",
      "\n",
      "Label distribution in training set:\n",
      "0     176\n",
      "9      82\n",
      "5      60\n",
      "4      30\n",
      "3      21\n",
      "6      21\n",
      "8      10\n",
      "10      7\n",
      "2       6\n",
      "1       1\n",
      "7       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Initializing tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 32000\n",
      "Padding token: </s>\n",
      "EOS token: </s>\n",
      "\n",
      "Tokenizing texts...\n",
      "\n",
      "Encoding shapes:\n",
      "Training input_ids: torch.Size([415, 512])\n",
      "Training attention_mask: torch.Size([415, 512])\n",
      "Dataset created with 415 samples\n",
      "Label distribution: 0     176\n",
      "9      82\n",
      "5      60\n",
      "4      30\n",
      "3      21\n",
      "6      21\n",
      "8      10\n",
      "10      7\n",
      "2       6\n",
      "1       1\n",
      "7       1\n",
      "Name: count, dtype: int64\n",
      "Dataset created with 104 samples\n",
      "Label distribution: 0     40\n",
      "9     22\n",
      "5     12\n",
      "3      8\n",
      "8      8\n",
      "6      7\n",
      "4      4\n",
      "1      1\n",
      "10     1\n",
      "7      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [03:54<00:00, 117.41s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.25it/s]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at openlm-research/open_llama_7b and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\krona\\.conda\\envs\\llama\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 6,607,388,672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/78 [00:00<?, ?it/s]C:\\Users\\krona\\AppData\\Local\\Temp\\ipykernel_42696\\3963993035.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in Llama training: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 11.00 GiB of which 0 bytes is free. Of the allocated memory 24.43 GiB is allocated by PyTorch, and 1023.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">llama-classification-20250109</strong> at: <a href='https://wandb.ai/backpropagandists/llama-classification/runs/piqwion0' target=\"_blank\">https://wandb.ai/backpropagandists/llama-classification/runs/piqwion0</a><br> View project at: <a href='https://wandb.ai/backpropagandists/llama-classification' target=\"_blank\">https://wandb.ai/backpropagandists/llama-classification</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250109_224037-piqwion0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 11.00 GiB of which 0 bytes is free. Of the allocated memory 24.43 GiB is allocated by PyTorch, and 1023.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining on CC dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m     results, model, tokenizer, label_mapping \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_llama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_normalized_cc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 154\u001b[0m, in \u001b[0;36mtrain_llama\u001b[1;34m(df, base_path, model_name)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 154\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\krona\\.conda\\envs\\llama\\lib\\site-packages\\transformers\\trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\krona\\.conda\\envs\\llama\\lib\\site-packages\\transformers\\trainer.py:2524\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2517\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2518\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2520\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2521\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2522\u001b[0m )\n\u001b[0;32m   2523\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2524\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2527\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2528\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2529\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2530\u001b[0m ):\n\u001b[0;32m   2531\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2532\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\krona\\.conda\\envs\\llama\\lib\\site-packages\\transformers\\trainer.py:3687\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3685\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3687\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3688\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[0;32m   3689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\krona\\.conda\\envs\\llama\\lib\\site-packages\\accelerate\\accelerator.py:2244\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2243\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2245\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\krona\\.conda\\envs\\llama\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\krona\\.conda\\envs\\llama\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\krona\\.conda\\envs\\llama\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[1;32mc:\\Users\\krona\\.conda\\envs\\llama\\lib\\site-packages\\torch\\autograd\\function.py:307\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m     )\n\u001b[0;32m    306\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[1;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\krona\\.conda\\envs\\llama\\lib\\site-packages\\torch\\utils\\checkpoint.py:304\u001b[0m, in \u001b[0;36mCheckpointFunction.backward\u001b[1;34m(ctx, *args)\u001b[0m\n\u001b[0;32m    300\u001b[0m     device_autocast_ctx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\n\u001b[0;32m    301\u001b[0m         device_type\u001b[38;5;241m=\u001b[39mctx\u001b[38;5;241m.\u001b[39mdevice_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mctx\u001b[38;5;241m.\u001b[39mdevice_autocast_kwargs\n\u001b[0;32m    302\u001b[0m     ) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mis_autocast_available(ctx\u001b[38;5;241m.\u001b[39mdevice_type) \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext()\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad(), device_autocast_ctx, torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mctx\u001b[38;5;241m.\u001b[39mcpu_autocast_kwargs):  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdetached_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    307\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (outputs,)\n",
      "File \u001b[1;32mc:\\Users\\krona\\.conda\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\krona\\.conda\\envs\\llama\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\krona\\.conda\\envs\\llama\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:657\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    655\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m    656\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m--> 657\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mresidual\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\n\u001b[0;32m    659\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 11.00 GiB of which 0 bytes is free. Of the allocated memory 24.43 GiB is allocated by PyTorch, and 1023.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Choose dataset to train on\n",
    "print(\"Select dataset for training:\")\n",
    "print(\"1. Full dataset\")\n",
    "print(\"2. UA dataset\")\n",
    "print(\"3. CC dataset\")\n",
    "choice = input(\"Enter your choice (1-3): \")\n",
    "\n",
    "if choice == \"1\":\n",
    "    print(\"\\nTraining on full dataset...\")\n",
    "    results, model, tokenizer, label_mapping = train_llama(df_normalized, base_path)\n",
    "elif choice == \"2\":\n",
    "    print(\"\\nTraining on UA dataset...\")\n",
    "    results, model, tokenizer, label_mapping = train_llama(df_normalized_ua, base_path)\n",
    "else:\n",
    "    print(\"\\nTraining on CC dataset...\")\n",
    "    results, model, tokenizer, label_mapping = train_llama(df_normalized_cc, base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug misclassifications\n",
    "if choice == \"1\":\n",
    "    misclass_df = debug_misclassifications(df_normalized, model, tokenizer, label_mapping)\n",
    "elif choice == \"2\":\n",
    "    misclass_df = debug_misclassifications(df_normalized_ua, model, tokenizer, label_mapping)\n",
    "else:\n",
    "    misclass_df = debug_misclassifications(df_normalized_cc,model, tokenizer, label_mapping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
