{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1+cu121\n",
      "CUDA Available: True\n",
      "GPU Device: NVIDIA L40S\n",
      "GPU Memory: 47.81 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import wandb\n",
    "from transformers import LlamaTokenizer, LlamaForSequenceClassification, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import sentencepiece\n",
    "from tqdm.notebook import tqdm\n",
    "from huggingface_hub import login\n",
    "login('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')\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "from datasets import Dataset\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check CUDA availability\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1694 entries, 0 to 1693\n",
      "Data columns (total 8 columns):\n",
      " #   Column                        Non-Null Count  Dtype \n",
      "---  ------                        --------------  ----- \n",
      " 0   filename                      1694 non-null   object\n",
      " 1   language                      1694 non-null   object\n",
      " 2   content                       1694 non-null   object\n",
      " 3   topic                         1694 non-null   object\n",
      " 4   narrative_subnarrative_pairs  1694 non-null   object\n",
      " 5   target_indices                1694 non-null   object\n",
      " 6   tokens                        1694 non-null   object\n",
      " 7   tokens_normalized             1694 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 106.0+ KB\n",
      "None\n",
      "\n",
      "Number of records: 1694\n",
      "\n",
      "UA Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1175 entries, 0 to 1174\n",
      "Data columns (total 8 columns):\n",
      " #   Column                        Non-Null Count  Dtype \n",
      "---  ------                        --------------  ----- \n",
      " 0   filename                      1175 non-null   object\n",
      " 1   language                      1175 non-null   object\n",
      " 2   content                       1175 non-null   object\n",
      " 3   topic                         1175 non-null   object\n",
      " 4   narrative_subnarrative_pairs  1175 non-null   object\n",
      " 5   target_indices                1175 non-null   object\n",
      " 6   tokens                        1175 non-null   object\n",
      " 7   tokens_normalized             1175 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 73.6+ KB\n",
      "None\n",
      "\n",
      "Number of UA records: 1175\n",
      "\n",
      "CC Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 519 entries, 0 to 518\n",
      "Data columns (total 8 columns):\n",
      " #   Column                        Non-Null Count  Dtype \n",
      "---  ------                        --------------  ----- \n",
      " 0   filename                      519 non-null    object\n",
      " 1   language                      519 non-null    object\n",
      " 2   content                       519 non-null    object\n",
      " 3   topic                         519 non-null    object\n",
      " 4   narrative_subnarrative_pairs  519 non-null    object\n",
      " 5   target_indices                519 non-null    object\n",
      " 6   tokens                        519 non-null    object\n",
      " 7   tokens_normalized             519 non-null    object\n",
      "dtypes: object(8)\n",
      "memory usage: 32.6+ KB\n",
      "None\n",
      "\n",
      "Number of CC records: 519\n",
      "\n",
      "Sample row from full dataset:\n",
      "filename                                                         EN_CC_100013.txt\n",
      "language                                                                       EN\n",
      "content                         Bill Gates Says He Is ‘The Solution’ To Climat...\n",
      "topic                                                                          CC\n",
      "narrative_subnarrative_pairs    [{'narrative': 'Criticism of climate movement'...\n",
      "target_indices                                                               [14]\n",
      "tokens                          [['Bill', 'Gates', 'Says', 'He', 'Is', '‘', 'T...\n",
      "tokens_normalized               ['bill', 'gate', 'say', 'solution', 'climate',...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "base_path = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Load preprocessed data\n",
    "input_file_full = os.path.join(base_path, \"df_normalized.csv\")\n",
    "df_normalized = pd.read_csv(input_file_full)\n",
    "df = pd.read_csv(input_file_full)\n",
    "\n",
    "input_file_ua = os.path.join(base_path, \"df_normalized_ua.csv\")\n",
    "df_normalized_ua = pd.read_csv(input_file_ua)\n",
    "\n",
    "input_file_cc = os.path.join(base_path, \"df_normalized_cc.csv\")\n",
    "df_normalized_cc = pd.read_csv(input_file_cc)\n",
    "\n",
    "# Display dataset information\n",
    "print(\"\\nFull Dataset Info:\")\n",
    "print(df_normalized.info())\n",
    "print(f\"\\nNumber of records: {len(df_normalized)}\")\n",
    "\n",
    "print(\"\\nUA Dataset Info:\")\n",
    "print(df_normalized_ua.info())\n",
    "print(f\"\\nNumber of UA records: {len(df_normalized_ua)}\")\n",
    "\n",
    "print(\"\\nCC Dataset Info:\")\n",
    "print(df_normalized_cc.info())\n",
    "print(f\"\\nNumber of CC records: {len(df_normalized_cc)}\")\n",
    "\n",
    "# Display sample rows\n",
    "print(\"\\nSample row from full dataset:\")\n",
    "print(df_normalized.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom Dataset for loading Llama input data\"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        \n",
    "        # Debug info\n",
    "        print(f\"Dataset created with {len(self.labels)} samples\")\n",
    "        print(f\"Label distribution: {pd.Series(self.labels).value_counts()}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"Compute evaluation metrics\"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"micro\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    # Computing confusion matrix per class\n",
    "    unique_classes = np.unique(labels)\n",
    "    cm_per_class = {}\n",
    "    \n",
    "    for class_idx in unique_classes:\n",
    "        binary_labels = (labels == class_idx).astype(int)\n",
    "        binary_preds = (preds == class_idx).astype(int)\n",
    "        cm = confusion_matrix(binary_labels, binary_preds)\n",
    "        cm_per_class[f\"Class_{class_idx}\"] = cm.tolist()\n",
    "        \n",
    "        # Print per-class metrics for debugging\n",
    "        print(f\"\\nMetrics for Class {class_idx}:\")\n",
    "        print(f\"Confusion Matrix:\\n{cm}\")\n",
    "        class_precision = precision_recall_fscore_support(binary_labels, binary_preds, average='binary')[0]\n",
    "        class_recall = precision_recall_fscore_support(binary_labels, binary_preds, average='binary')[1]\n",
    "        print(f\"Precision: {class_precision:.4f}\")\n",
    "        print(f\"Recall: {class_recall:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'confusion_matrix': cm_per_class\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_narrative_key(narrative_dict):\n",
    "    \"\"\"Extract key from narrative dictionary for classification\"\"\"\n",
    "    if isinstance(narrative_dict, str):\n",
    "        narrative_dict = eval(narrative_dict)\n",
    "    return narrative_dict['narrative']  # or you could use narrative_dict['subnarrative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_classification_head(model, train_dataset, val_dataset, tokenizer, output_dir):\n",
    "    \"\"\"Pre-train the classification head before fine-tuning the full model\"\"\"\n",
    "    try:\n",
    "        print(\"\\nPre-training classification head...\")\n",
    "        \n",
    "        # Freeze all layers except classification head\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"score\" not in name:  # Freeze everything except score layer\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        # Print trainable parameters\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        all_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"Trainable parameters for head pre-training: {trainable_params:,} ({trainable_params/all_params:.2%} of total)\")\n",
    "        \n",
    "        # Training arguments for head pre-training\n",
    "        head_training_args = TrainingArguments(\n",
    "            output_dir=os.path.join(output_dir, \"head_pretraining\"),\n",
    "            run_name=f\"llama-classification-run-{current_date}\",\n",
    "            num_train_epochs=1,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            learning_rate=1e-3,\n",
    "            warmup_ratio=0.1,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"no\",\n",
    "            logging_dir=os.path.join(output_dir, \"head_logs\"),\n",
    "            logging_steps=10,\n",
    "            remove_unused_columns=False,\n",
    "            report_to=\"wandb\"\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer with validation dataset\n",
    "        head_trainer = Trainer(\n",
    "            model=model,\n",
    "            args=head_training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,  # Added validation dataset\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "        \n",
    "        # Train only the head\n",
    "        head_trainer.train()\n",
    "        \n",
    "        print(\"\\nClassification head pre-training completed\")\n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in classification head pre-training: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "openlm-research/open_llama_7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_llama(df, base_path, model_name=\"openlm-research/open_llama_7b\"):\n",
    "    \"\"\"Train Llama model with classification head pre-training and LoRA fine-tuning\"\"\"\n",
    "    try:\n",
    "        current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "        \n",
    "        # Create output directories\n",
    "        output_dir = os.path.join(base_path, f\"models/llama_{current_date}\")\n",
    "        log_dir = os.path.join(base_path, f\"logs/llama_{current_date}\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "        # Initialize wandb\n",
    "        wandb.init(project=\"llama-classification\", name=f\"llama-classification-{current_date}\")\n",
    "\n",
    "        # Create narrative mapping\n",
    "        print(\"\\nCreating narrative mapping...\")\n",
    "        narratives = df['narrative_subnarrative_pairs'].apply(\n",
    "            lambda x: eval(x)[0] if isinstance(x, str) else x[0]\n",
    "        ).tolist()\n",
    "        \n",
    "        unique_narratives = set(get_narrative_key(n) for n in narratives)\n",
    "        label_mapping = {narrative: idx for idx, narrative in enumerate(sorted(unique_narratives))}\n",
    "        \n",
    "        print(f\"Number of unique narratives: {len(unique_narratives)}\")\n",
    "        print(\"\\nSample narrative mappings:\")\n",
    "        for i, (narrative, idx) in enumerate(list(label_mapping.items())[:5]):\n",
    "            print(f\"{idx}: {narrative}\")\n",
    "\n",
    "        # Save label mapping\n",
    "        with open(os.path.join(output_dir, \"label_mapping.json\"), 'w') as f:\n",
    "            json.dump(label_mapping, f, indent=2)\n",
    "\n",
    "        # Prepare data\n",
    "        print(\"\\nPreparing data splits...\")\n",
    "        df_train, df_val = train_test_split(df, test_size=0.2, random_state=42)\n",
    "        \n",
    "        print(f\"Training set size: {len(df_train)}\")\n",
    "        print(f\"Validation set size: {len(df_val)}\")\n",
    "\n",
    "        # Process texts and labels\n",
    "        train_texts = df_train['tokens_normalized'].apply(\n",
    "            lambda x: ' '.join(x) if isinstance(x, list) else x\n",
    "        ).tolist()\n",
    "        val_texts = df_val['tokens_normalized'].apply(\n",
    "            lambda x: ' '.join(x) if isinstance(x, list) else x\n",
    "        ).tolist()\n",
    "        \n",
    "        train_labels = [\n",
    "            label_mapping[get_narrative_key(eval(n)[0] if isinstance(n, str) else n[0])]\n",
    "            for n in df_train['narrative_subnarrative_pairs']\n",
    "        ]\n",
    "        val_labels = [\n",
    "            label_mapping[get_narrative_key(eval(n)[0] if isinstance(n, str) else n[0])]\n",
    "            for n in df_val['narrative_subnarrative_pairs']\n",
    "        ]\n",
    "\n",
    "        # Initialize tokenizer\n",
    "        print(\"\\nInitializing tokenizer...\")\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "        # Tokenize texts\n",
    "        print(\"\\nTokenizing texts...\")\n",
    "        train_encodings = tokenizer(\n",
    "            train_texts, \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            max_length=512\n",
    "        )\n",
    "        val_encodings = tokenizer(\n",
    "            val_texts, \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            max_length=512\n",
    "        )\n",
    "\n",
    "        # Create datasets\n",
    "        train_dataset = Dataset.from_dict({\n",
    "            'input_ids': train_encodings['input_ids'],\n",
    "            'attention_mask': train_encodings['attention_mask'],\n",
    "            'labels': train_labels\n",
    "        })\n",
    "        val_dataset = Dataset.from_dict({\n",
    "            'input_ids': val_encodings['input_ids'],\n",
    "            'attention_mask': val_encodings['attention_mask'],\n",
    "            'labels': val_labels\n",
    "        })\n",
    "\n",
    "        # Setup quantization configuration\n",
    "        print(\"\\nConfiguring quantization...\")\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "\n",
    "        # Initialize model\n",
    "        print(\"\\nInitializing model...\")\n",
    "        model = LlamaForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=len(label_mapping),\n",
    "            torch_dtype=torch.float16,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map='auto'\n",
    "        )\n",
    "\n",
    "        # Prepare model for k-bit training\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "        # Configure and apply LoRA\n",
    "        print(\"\\nApplying LoRA adapters...\")\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            inference_mode=False,\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "            bias=\"none\",\n",
    "        )\n",
    "        \n",
    "        model = get_peft_model(model, peft_config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "        # Pre-train classification head\n",
    "        print(\"\\nStarting classification head pre-training...\")\n",
    "        \n",
    "        # Freeze LoRA adapters\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'lora' in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Training arguments for head pre-training\n",
    "        head_training_args = TrainingArguments(\n",
    "            output_dir=os.path.join(output_dir, \"head_pretraining\"),\n",
    "            num_train_epochs=1,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            learning_rate=1e-3,\n",
    "            warmup_ratio=0.1,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"no\",\n",
    "            logging_dir=os.path.join(output_dir, \"head_logs\"),\n",
    "            logging_steps=10,\n",
    "            remove_unused_columns=False,\n",
    "            report_to=\"wandb\"\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer for head pre-training\n",
    "        head_trainer = Trainer(\n",
    "            model=model,\n",
    "            args=head_training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "        \n",
    "        # Train classification head\n",
    "        head_trainer.train()\n",
    "\n",
    "        # Unfreeze LoRA adapters for full training\n",
    "        print(\"\\nUnfreezing LoRA adapters for full training...\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'lora' in name:\n",
    "                param.requires_grad = True\n",
    "\n",
    "        # Training arguments for full model fine-tuning\n",
    "        print(\"\\nStarting full model fine-tuning...\")\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            run_name=f\"llama-classification-run-{current_date}\",\n",
    "            num_train_epochs=3,\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=4,\n",
    "            learning_rate=2e-4,\n",
    "            warmup_ratio=0.03,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_dir=log_dir,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model='eval_loss',\n",
    "            greater_is_better=False,\n",
    "            logging_steps=10,\n",
    "            gradient_accumulation_steps=2,\n",
    "            gradient_checkpointing=True,\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "            remove_unused_columns=False,\n",
    "            report_to=\"wandb\"\n",
    "        )\n",
    "\n",
    "        # Initialize trainer for full model fine-tuning\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "        # Train full model\n",
    "        trainer.train()\n",
    "\n",
    "        # Evaluate model\n",
    "        print(\"\\nEvaluating model...\")\n",
    "        results = trainer.evaluate()\n",
    "        \n",
    "        print(\"\\nEvaluation results:\")\n",
    "        for metric, value in results.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"{metric}: {value}\")\n",
    "\n",
    "        # Save model and tokenizer\n",
    "        print(\"\\nSaving model...\")\n",
    "        trainer.save_model(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "        # End wandb run\n",
    "        wandb.finish()\n",
    "\n",
    "        return results, model, tokenizer, label_mapping\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Llama training: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        wandb.finish()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_misclassifications(dataset, model, tokenizer, label_mapping, dataset_type=\"Training\"):\n",
    "    \"\"\"Debug misclassified examples with detailed output and proper device handling\"\"\"\n",
    "    try:\n",
    "        print(f\"\\nAnalyzing misclassifications in {dataset_type} dataset...\")\n",
    "        \n",
    "        # Determine device\n",
    "        device = model.device\n",
    "        print(f\"Model is on device: {device}\")\n",
    "        \n",
    "        # Prepare data\n",
    "        texts = dataset['tokens_normalized'].apply(\n",
    "            lambda x: ' '.join(x) if isinstance(x, list) else x\n",
    "        ).tolist()\n",
    "        \n",
    "        true_labels = [\n",
    "            label_mapping[get_narrative_key(eval(n)[0] if isinstance(n, str) else n[0])]\n",
    "            for n in dataset['narrative_subnarrative_pairs']\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nTotal samples to analyze: {len(texts)}\")\n",
    "\n",
    "        # Get predictions in batches to manage memory\n",
    "        batch_size = 8\n",
    "        predictions = []\n",
    "        confidences = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            # Tokenize batch\n",
    "            encodings = tokenizer(\n",
    "                batch_texts, \n",
    "                truncation=True, \n",
    "                padding=True, \n",
    "                max_length=512, \n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Move encodings to same device as model\n",
    "            encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**encodings)\n",
    "                batch_preds = outputs.logits.argmax(-1)\n",
    "                batch_confs = torch.softmax(outputs.logits, dim=-1).max(dim=-1)[0]\n",
    "                \n",
    "                # Move predictions back to CPU\n",
    "                predictions.extend(batch_preds.cpu().numpy())\n",
    "                confidences.extend(batch_confs.cpu().numpy())\n",
    "\n",
    "        # Track misclassifications\n",
    "        misclassifications = []\n",
    "        for idx, (pred, true, conf) in enumerate(zip(predictions, true_labels, confidences)):\n",
    "            if pred != true:\n",
    "                misclassifications.append({\n",
    "                    'text': texts[idx][:200],  # First 200 chars for brevity\n",
    "                    'predicted': pred,\n",
    "                    'actual': true,\n",
    "                    'confidence': conf,\n",
    "                    'dataset_type': dataset_type\n",
    "                })\n",
    "\n",
    "        # Create DataFrame and display results\n",
    "        misclass_df = pd.DataFrame(misclassifications)\n",
    "        \n",
    "        print(f\"\\nTotal misclassifications: {len(misclass_df)}\")\n",
    "        print(f\"Accuracy: {1 - len(misclass_df)/len(texts):.4f}\")\n",
    "        \n",
    "        if len(misclass_df) > 0:\n",
    "            print(\"\\nMisclassification distribution:\")\n",
    "            print(misclass_df.groupby(['actual', 'predicted']).size().unstack(fill_value=0))\n",
    "            \n",
    "            print(\"\\nSample misclassifications:\")\n",
    "            for i, row in misclass_df.head().iterrows():\n",
    "                print(f\"\\nExample {i+1}:\")\n",
    "                print(f\"Text: {row['text']}\")\n",
    "                print(f\"Predicted: {row['predicted']}, Actual: {row['actual']}\")\n",
    "                print(f\"Confidence: {row['confidence']:.4f}\")\n",
    "        \n",
    "        return misclass_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in debugging misclassifications: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select dataset for training:\n",
      "1. Full dataset\n",
      "2. UA dataset\n",
      "3. CC dataset\n",
      "\n",
      "Training on full dataset...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/nlp_Backpropagandists_2024/code/models/wandb/run-20250109_230304-9ipou16t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/backpropagandists/llama-classification/runs/9ipou16t' target=\"_blank\">llama-classification-20250109</a></strong> to <a href='https://wandb.ai/backpropagandists/llama-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/backpropagandists/llama-classification' target=\"_blank\">https://wandb.ai/backpropagandists/llama-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/backpropagandists/llama-classification/runs/9ipou16t' target=\"_blank\">https://wandb.ai/backpropagandists/llama-classification/runs/9ipou16t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating narrative mapping...\n",
      "Number of unique narratives: 21\n",
      "\n",
      "Sample narrative mappings:\n",
      "0: Amplifying Climate Fears\n",
      "1: Amplifying war-related fears\n",
      "2: Blaming the war on others rather than the invader\n",
      "3: Climate change is beneficial\n",
      "4: Controversy about green technologies\n",
      "\n",
      "Preparing data splits...\n",
      "Training set size: 1355\n",
      "Validation set size: 339\n",
      "\n",
      "Initializing tokenizer...\n",
      "\n",
      "Tokenizing texts...\n",
      "\n",
      "Configuring quantization...\n",
      "\n",
      "Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 23:03:09,944 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9a9670ba164cceb940f7c18a95ee70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at openlm-research/open_llama_7b and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying LoRA adapters...\n",
      "trainable params: 16,863,232 || all params: 6,624,292,864 || trainable%: 0.2546\n",
      "\n",
      "Starting classification head pre-training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170/170 17:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.376200</td>\n",
       "      <td>2.484182</td>\n",
       "      <td>0.247788</td>\n",
       "      <td>0.247788</td>\n",
       "      <td>0.247788</td>\n",
       "      <td>0.247788</td>\n",
       "      <td>{'Class_0': [[254, 49], [10, 26]], 'Class_1': [[309, 7], [22, 1]], 'Class_2': [[316, 6], [17, 0]], 'Class_4': [[338, 0], [1, 0]], 'Class_5': [[335, 0], [4, 0]], 'Class_6': [[332, 4], [3, 0]], 'Class_7': [[311, 7], [18, 3]], 'Class_8': [[223, 66], [27, 23]], 'Class_9': [[292, 11], [36, 0]], 'Class_10': [[334, 0], [5, 0]], 'Class_11': [[334, 1], [4, 0]], 'Class_13': [[330, 2], [7, 0]], 'Class_14': [[333, 0], [6, 0]], 'Class_15': [[222, 59], [31, 27]], 'Class_16': [[336, 0], [3, 0]], 'Class_17': [[267, 37], [31, 4]], 'Class_18': [[337, 1], [1, 0]], 'Class_19': [[323, 2], [14, 0]], 'Class_20': [[321, 3], [15, 0]]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Class 0:\n",
      "Confusion Matrix:\n",
      "[[254  49]\n",
      " [ 10  26]]\n",
      "Precision: 0.3467\n",
      "Recall: 0.7222\n",
      "\n",
      "Metrics for Class 1:\n",
      "Confusion Matrix:\n",
      "[[309   7]\n",
      " [ 22   1]]\n",
      "Precision: 0.1250\n",
      "Recall: 0.0435\n",
      "\n",
      "Metrics for Class 2:\n",
      "Confusion Matrix:\n",
      "[[316   6]\n",
      " [ 17   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 4:\n",
      "Confusion Matrix:\n",
      "[[338   0]\n",
      " [  1   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 5:\n",
      "Confusion Matrix:\n",
      "[[335   0]\n",
      " [  4   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 6:\n",
      "Confusion Matrix:\n",
      "[[332   4]\n",
      " [  3   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 7:\n",
      "Confusion Matrix:\n",
      "[[311   7]\n",
      " [ 18   3]]\n",
      "Precision: 0.3000\n",
      "Recall: 0.1429\n",
      "\n",
      "Metrics for Class 8:\n",
      "Confusion Matrix:\n",
      "[[223  66]\n",
      " [ 27  23]]\n",
      "Precision: 0.2584\n",
      "Recall: 0.4600\n",
      "\n",
      "Metrics for Class 9:\n",
      "Confusion Matrix:\n",
      "[[292  11]\n",
      " [ 36   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 10:\n",
      "Confusion Matrix:\n",
      "[[334   0]\n",
      " [  5   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 11:\n",
      "Confusion Matrix:\n",
      "[[334   1]\n",
      " [  4   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 13:\n",
      "Confusion Matrix:\n",
      "[[330   2]\n",
      " [  7   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 14:\n",
      "Confusion Matrix:\n",
      "[[333   0]\n",
      " [  6   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 15:\n",
      "Confusion Matrix:\n",
      "[[222  59]\n",
      " [ 31  27]]\n",
      "Precision: 0.3140\n",
      "Recall: 0.4655\n",
      "\n",
      "Metrics for Class 16:\n",
      "Confusion Matrix:\n",
      "[[336   0]\n",
      " [  3   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 17:\n",
      "Confusion Matrix:\n",
      "[[267  37]\n",
      " [ 31   4]]\n",
      "Precision: 0.0976\n",
      "Recall: 0.1143\n",
      "\n",
      "Metrics for Class 18:\n",
      "Confusion Matrix:\n",
      "[[337   1]\n",
      " [  1   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 19:\n",
      "Confusion Matrix:\n",
      "[[323   2]\n",
      " [ 14   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 20:\n",
      "Confusion Matrix:\n",
      "[[321   3]\n",
      " [ 15   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unfreezing LoRA adapters for full training...\n",
      "\n",
      "Starting full model fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='507' max='507' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [507/507 55:22, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.038500</td>\n",
       "      <td>2.290623</td>\n",
       "      <td>0.321534</td>\n",
       "      <td>0.321534</td>\n",
       "      <td>0.321534</td>\n",
       "      <td>0.321534</td>\n",
       "      <td>{'Class_0': [[270, 33], [4, 32]], 'Class_1': [[290, 26], [17, 6]], 'Class_2': [[320, 2], [17, 0]], 'Class_4': [[338, 0], [1, 0]], 'Class_5': [[327, 8], [4, 0]], 'Class_6': [[336, 0], [3, 0]], 'Class_7': [[316, 2], [18, 3]], 'Class_8': [[218, 71], [19, 31]], 'Class_9': [[292, 11], [33, 3]], 'Class_10': [[334, 0], [5, 0]], 'Class_11': [[335, 0], [4, 0]], 'Class_13': [[330, 2], [7, 0]], 'Class_14': [[332, 1], [6, 0]], 'Class_15': [[234, 47], [30, 28]], 'Class_16': [[335, 1], [3, 0]], 'Class_17': [[281, 23], [29, 6]], 'Class_18': [[338, 0], [1, 0]], 'Class_19': [[324, 1], [14, 0]], 'Class_20': [[322, 2], [15, 0]]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.600400</td>\n",
       "      <td>2.697214</td>\n",
       "      <td>0.348083</td>\n",
       "      <td>0.348083</td>\n",
       "      <td>0.348083</td>\n",
       "      <td>0.348083</td>\n",
       "      <td>{'Class_0': [[282, 21], [8, 28]], 'Class_1': [[303, 13], [12, 11]], 'Class_2': [[312, 10], [16, 1]], 'Class_4': [[338, 0], [1, 0]], 'Class_5': [[330, 5], [4, 0]], 'Class_6': [[335, 1], [3, 0]], 'Class_7': [[312, 6], [15, 6]], 'Class_8': [[252, 37], [28, 22]], 'Class_9': [[275, 28], [26, 10]], 'Class_10': [[334, 0], [5, 0]], 'Class_11': [[333, 2], [4, 0]], 'Class_13': [[331, 1], [7, 0]], 'Class_14': [[333, 0], [6, 0]], 'Class_15': [[228, 53], [24, 34]], 'Class_16': [[333, 3], [3, 0]], 'Class_17': [[282, 22], [31, 4]], 'Class_18': [[336, 2], [1, 0]], 'Class_19': [[318, 7], [13, 1]], 'Class_20': [[314, 10], [14, 1]]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Class 0:\n",
      "Confusion Matrix:\n",
      "[[270  33]\n",
      " [  4  32]]\n",
      "Precision: 0.4923\n",
      "Recall: 0.8889\n",
      "\n",
      "Metrics for Class 1:\n",
      "Confusion Matrix:\n",
      "[[290  26]\n",
      " [ 17   6]]\n",
      "Precision: 0.1875\n",
      "Recall: 0.2609\n",
      "\n",
      "Metrics for Class 2:\n",
      "Confusion Matrix:\n",
      "[[320   2]\n",
      " [ 17   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 4:\n",
      "Confusion Matrix:\n",
      "[[338   0]\n",
      " [  1   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 5:\n",
      "Confusion Matrix:\n",
      "[[327   8]\n",
      " [  4   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 6:\n",
      "Confusion Matrix:\n",
      "[[336   0]\n",
      " [  3   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 7:\n",
      "Confusion Matrix:\n",
      "[[316   2]\n",
      " [ 18   3]]\n",
      "Precision: 0.6000\n",
      "Recall: 0.1429\n",
      "\n",
      "Metrics for Class 8:\n",
      "Confusion Matrix:\n",
      "[[218  71]\n",
      " [ 19  31]]\n",
      "Precision: 0.3039\n",
      "Recall: 0.6200\n",
      "\n",
      "Metrics for Class 9:\n",
      "Confusion Matrix:\n",
      "[[292  11]\n",
      " [ 33   3]]\n",
      "Precision: 0.2143\n",
      "Recall: 0.0833\n",
      "\n",
      "Metrics for Class 10:\n",
      "Confusion Matrix:\n",
      "[[334   0]\n",
      " [  5   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 11:\n",
      "Confusion Matrix:\n",
      "[[335   0]\n",
      " [  4   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 13:\n",
      "Confusion Matrix:\n",
      "[[330   2]\n",
      " [  7   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 14:\n",
      "Confusion Matrix:\n",
      "[[332   1]\n",
      " [  6   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 15:\n",
      "Confusion Matrix:\n",
      "[[234  47]\n",
      " [ 30  28]]\n",
      "Precision: 0.3733\n",
      "Recall: 0.4828\n",
      "\n",
      "Metrics for Class 16:\n",
      "Confusion Matrix:\n",
      "[[335   1]\n",
      " [  3   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 17:\n",
      "Confusion Matrix:\n",
      "[[281  23]\n",
      " [ 29   6]]\n",
      "Precision: 0.2069\n",
      "Recall: 0.1714\n",
      "\n",
      "Metrics for Class 18:\n",
      "Confusion Matrix:\n",
      "[[338   0]\n",
      " [  1   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 19:\n",
      "Confusion Matrix:\n",
      "[[324   1]\n",
      " [ 14   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 20:\n",
      "Confusion Matrix:\n",
      "[[322   2]\n",
      " [ 15   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Class 0:\n",
      "Confusion Matrix:\n",
      "[[273  30]\n",
      " [  5  31]]\n",
      "Precision: 0.5082\n",
      "Recall: 0.8611\n",
      "\n",
      "Metrics for Class 1:\n",
      "Confusion Matrix:\n",
      "[[299  17]\n",
      " [ 13  10]]\n",
      "Precision: 0.3704\n",
      "Recall: 0.4348\n",
      "\n",
      "Metrics for Class 2:\n",
      "Confusion Matrix:\n",
      "[[311  11]\n",
      " [ 16   1]]\n",
      "Precision: 0.0833\n",
      "Recall: 0.0588\n",
      "\n",
      "Metrics for Class 4:\n",
      "Confusion Matrix:\n",
      "[[338   0]\n",
      " [  1   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 5:\n",
      "Confusion Matrix:\n",
      "[[332   3]\n",
      " [  4   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 6:\n",
      "Confusion Matrix:\n",
      "[[334   2]\n",
      " [  3   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 7:\n",
      "Confusion Matrix:\n",
      "[[311   7]\n",
      " [ 15   6]]\n",
      "Precision: 0.4615\n",
      "Recall: 0.2857\n",
      "\n",
      "Metrics for Class 8:\n",
      "Confusion Matrix:\n",
      "[[236  53]\n",
      " [ 19  31]]\n",
      "Precision: 0.3690\n",
      "Recall: 0.6200\n",
      "\n",
      "Metrics for Class 9:\n",
      "Confusion Matrix:\n",
      "[[289  14]\n",
      " [ 29   7]]\n",
      "Precision: 0.3333\n",
      "Recall: 0.1944\n",
      "\n",
      "Metrics for Class 10:\n",
      "Confusion Matrix:\n",
      "[[331   3]\n",
      " [  5   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 11:\n",
      "Confusion Matrix:\n",
      "[[332   3]\n",
      " [  4   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 13:\n",
      "Confusion Matrix:\n",
      "[[330   2]\n",
      " [  7   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 14:\n",
      "Confusion Matrix:\n",
      "[[329   4]\n",
      " [  5   1]]\n",
      "Precision: 0.2000\n",
      "Recall: 0.1667\n",
      "\n",
      "Metrics for Class 15:\n",
      "Confusion Matrix:\n",
      "[[254  27]\n",
      " [ 39  19]]\n",
      "Precision: 0.4130\n",
      "Recall: 0.3276\n",
      "\n",
      "Metrics for Class 16:\n",
      "Confusion Matrix:\n",
      "[[333   3]\n",
      " [  3   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 17:\n",
      "Confusion Matrix:\n",
      "[[273  31]\n",
      " [ 26   9]]\n",
      "Precision: 0.2250\n",
      "Recall: 0.2571\n",
      "\n",
      "Metrics for Class 18:\n",
      "Confusion Matrix:\n",
      "[[336   2]\n",
      " [  1   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 19:\n",
      "Confusion Matrix:\n",
      "[[322   3]\n",
      " [ 14   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 20:\n",
      "Confusion Matrix:\n",
      "[[316   8]\n",
      " [ 14   1]]\n",
      "Precision: 0.1111\n",
      "Recall: 0.0667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Class 0:\n",
      "Confusion Matrix:\n",
      "[[282  21]\n",
      " [  8  28]]\n",
      "Precision: 0.5714\n",
      "Recall: 0.7778\n",
      "\n",
      "Metrics for Class 1:\n",
      "Confusion Matrix:\n",
      "[[303  13]\n",
      " [ 12  11]]\n",
      "Precision: 0.4583\n",
      "Recall: 0.4783\n",
      "\n",
      "Metrics for Class 2:\n",
      "Confusion Matrix:\n",
      "[[312  10]\n",
      " [ 16   1]]\n",
      "Precision: 0.0909\n",
      "Recall: 0.0588\n",
      "\n",
      "Metrics for Class 4:\n",
      "Confusion Matrix:\n",
      "[[338   0]\n",
      " [  1   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 5:\n",
      "Confusion Matrix:\n",
      "[[330   5]\n",
      " [  4   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 6:\n",
      "Confusion Matrix:\n",
      "[[335   1]\n",
      " [  3   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 7:\n",
      "Confusion Matrix:\n",
      "[[312   6]\n",
      " [ 15   6]]\n",
      "Precision: 0.5000\n",
      "Recall: 0.2857\n",
      "\n",
      "Metrics for Class 8:\n",
      "Confusion Matrix:\n",
      "[[252  37]\n",
      " [ 28  22]]\n",
      "Precision: 0.3729\n",
      "Recall: 0.4400\n",
      "\n",
      "Metrics for Class 9:\n",
      "Confusion Matrix:\n",
      "[[275  28]\n",
      " [ 26  10]]\n",
      "Precision: 0.2632\n",
      "Recall: 0.2778\n",
      "\n",
      "Metrics for Class 10:\n",
      "Confusion Matrix:\n",
      "[[334   0]\n",
      " [  5   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 11:\n",
      "Confusion Matrix:\n",
      "[[333   2]\n",
      " [  4   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 13:\n",
      "Confusion Matrix:\n",
      "[[331   1]\n",
      " [  7   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 14:\n",
      "Confusion Matrix:\n",
      "[[333   0]\n",
      " [  6   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 15:\n",
      "Confusion Matrix:\n",
      "[[228  53]\n",
      " [ 24  34]]\n",
      "Precision: 0.3908\n",
      "Recall: 0.5862\n",
      "\n",
      "Metrics for Class 16:\n",
      "Confusion Matrix:\n",
      "[[333   3]\n",
      " [  3   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 17:\n",
      "Confusion Matrix:\n",
      "[[282  22]\n",
      " [ 31   4]]\n",
      "Precision: 0.1538\n",
      "Recall: 0.1143\n",
      "\n",
      "Metrics for Class 18:\n",
      "Confusion Matrix:\n",
      "[[336   2]\n",
      " [  1   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 19:\n",
      "Confusion Matrix:\n",
      "[[318   7]\n",
      " [ 13   1]]\n",
      "Precision: 0.1250\n",
      "Recall: 0.0714\n",
      "\n",
      "Metrics for Class 20:\n",
      "Confusion Matrix:\n",
      "[[314  10]\n",
      " [ 14   1]]\n",
      "Precision: 0.0909\n",
      "Recall: 0.0667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='85' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [85/85 01:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Class 0:\n",
      "Confusion Matrix:\n",
      "[[270  33]\n",
      " [  4  32]]\n",
      "Precision: 0.4923\n",
      "Recall: 0.8889\n",
      "\n",
      "Metrics for Class 1:\n",
      "Confusion Matrix:\n",
      "[[290  26]\n",
      " [ 17   6]]\n",
      "Precision: 0.1875\n",
      "Recall: 0.2609\n",
      "\n",
      "Metrics for Class 2:\n",
      "Confusion Matrix:\n",
      "[[320   2]\n",
      " [ 17   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 4:\n",
      "Confusion Matrix:\n",
      "[[338   0]\n",
      " [  1   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 5:\n",
      "Confusion Matrix:\n",
      "[[327   8]\n",
      " [  4   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 6:\n",
      "Confusion Matrix:\n",
      "[[336   0]\n",
      " [  3   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 7:\n",
      "Confusion Matrix:\n",
      "[[316   2]\n",
      " [ 18   3]]\n",
      "Precision: 0.6000\n",
      "Recall: 0.1429\n",
      "\n",
      "Metrics for Class 8:\n",
      "Confusion Matrix:\n",
      "[[218  71]\n",
      " [ 19  31]]\n",
      "Precision: 0.3039\n",
      "Recall: 0.6200\n",
      "\n",
      "Metrics for Class 9:\n",
      "Confusion Matrix:\n",
      "[[292  11]\n",
      " [ 33   3]]\n",
      "Precision: 0.2143\n",
      "Recall: 0.0833\n",
      "\n",
      "Metrics for Class 10:\n",
      "Confusion Matrix:\n",
      "[[334   0]\n",
      " [  5   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 11:\n",
      "Confusion Matrix:\n",
      "[[335   0]\n",
      " [  4   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 13:\n",
      "Confusion Matrix:\n",
      "[[330   2]\n",
      " [  7   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 14:\n",
      "Confusion Matrix:\n",
      "[[332   1]\n",
      " [  6   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 15:\n",
      "Confusion Matrix:\n",
      "[[234  47]\n",
      " [ 30  28]]\n",
      "Precision: 0.3733\n",
      "Recall: 0.4828\n",
      "\n",
      "Metrics for Class 16:\n",
      "Confusion Matrix:\n",
      "[[335   1]\n",
      " [  3   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 17:\n",
      "Confusion Matrix:\n",
      "[[281  23]\n",
      " [ 29   6]]\n",
      "Precision: 0.2069\n",
      "Recall: 0.1714\n",
      "\n",
      "Metrics for Class 18:\n",
      "Confusion Matrix:\n",
      "[[338   0]\n",
      " [  1   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 19:\n",
      "Confusion Matrix:\n",
      "[[324   1]\n",
      " [ 14   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Metrics for Class 20:\n",
      "Confusion Matrix:\n",
      "[[322   2]\n",
      " [ 15   0]]\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Evaluation results:\n",
      "eval_loss: 2.2906\n",
      "eval_accuracy: 0.3215\n",
      "eval_f1: 0.3215\n",
      "eval_precision: 0.3215\n",
      "eval_recall: 0.3215\n",
      "eval_confusion_matrix: {'Class_0': [[270, 33], [4, 32]], 'Class_1': [[290, 26], [17, 6]], 'Class_2': [[320, 2], [17, 0]], 'Class_4': [[338, 0], [1, 0]], 'Class_5': [[327, 8], [4, 0]], 'Class_6': [[336, 0], [3, 0]], 'Class_7': [[316, 2], [18, 3]], 'Class_8': [[218, 71], [19, 31]], 'Class_9': [[292, 11], [33, 3]], 'Class_10': [[334, 0], [5, 0]], 'Class_11': [[335, 0], [4, 0]], 'Class_13': [[330, 2], [7, 0]], 'Class_14': [[332, 1], [6, 0]], 'Class_15': [[234, 47], [30, 28]], 'Class_16': [[335, 1], [3, 0]], 'Class_17': [[281, 23], [29, 6]], 'Class_18': [[338, 0], [1, 0]], 'Class_19': [[324, 1], [14, 0]], 'Class_20': [[322, 2], [15, 0]]}\n",
      "eval_runtime: 90.9329\n",
      "eval_samples_per_second: 3.7280\n",
      "eval_steps_per_second: 0.9350\n",
      "epoch: 2.9853\n",
      "\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▆██▆</td></tr><tr><td>eval/f1</td><td>▁▆██▆</td></tr><tr><td>eval/loss</td><td>▄▁▅█▁</td></tr><tr><td>eval/precision</td><td>▁▆██▆</td></tr><tr><td>eval/recall</td><td>▁▆██▆</td></tr><tr><td>eval/runtime</td><td>▇▁██▇</td></tr><tr><td>eval/samples_per_second</td><td>▂█▁▁▂</td></tr><tr><td>eval/steps_per_second</td><td>▁█▇▇█</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▁▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▂▂▃▃▁▁▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▄▄▄▃▃▃▄▄▄▄▅▄▆▇██▅▅▄▄▆▅▆▇▅▄▅▆▇▁▂▂▁▁▅▃█▃▁▅</td></tr><tr><td>train/learning_rate</td><td>▅██▇▇▅▅▄▄▃▂▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▇▆▆▆▅▆▆▅▅▅▅▅▅▅▅▅▇▇█▇█▄▃▄▃▄▃▃▄▃▄▁▂▂▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.32153</td></tr><tr><td>eval/f1</td><td>0.32153</td></tr><tr><td>eval/loss</td><td>2.29062</td></tr><tr><td>eval/precision</td><td>0.32153</td></tr><tr><td>eval/recall</td><td>0.32153</td></tr><tr><td>eval/runtime</td><td>90.9329</td></tr><tr><td>eval/samples_per_second</td><td>3.728</td></tr><tr><td>eval/steps_per_second</td><td>0.935</td></tr><tr><td>total_flos</td><td>8.070626800356557e+16</td></tr><tr><td>train/epoch</td><td>2.98525</td></tr><tr><td>train/global_step</td><td>507</td></tr><tr><td>train/grad_norm</td><td>30.74333</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6004</td></tr><tr><td>train_loss</td><td>1.84816</td></tr><tr><td>train_runtime</td><td>3328.5103</td></tr><tr><td>train_samples_per_second</td><td>1.221</td></tr><tr><td>train_steps_per_second</td><td>0.152</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">llama-classification-20250109</strong> at: <a href='https://wandb.ai/backpropagandists/llama-classification/runs/9ipou16t' target=\"_blank\">https://wandb.ai/backpropagandists/llama-classification/runs/9ipou16t</a><br> View project at: <a href='https://wandb.ai/backpropagandists/llama-classification' target=\"_blank\">https://wandb.ai/backpropagandists/llama-classification</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250109_230304-9ipou16t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose dataset to train on\n",
    "print(\"Select dataset for training:\")\n",
    "print(\"1. Full dataset\")\n",
    "print(\"2. UA dataset\")\n",
    "print(\"3. CC dataset\")\n",
    "choice = input(\"Enter your choice (1-3): \")\n",
    "\n",
    "if choice == \"1\":\n",
    "    print(\"\\nTraining on full dataset...\")\n",
    "    results, model, tokenizer, label_mapping = train_llama(df_normalized, base_path)\n",
    "elif choice == \"2\":\n",
    "    print(\"\\nTraining on UA dataset...\")\n",
    "    results, model, tokenizer, label_mapping = train_llama(df_normalized_ua, base_path)\n",
    "else:\n",
    "    print(\"\\nTraining on CC dataset...\")\n",
    "    results, model, tokenizer, label_mapping = train_llama(df_normalized_cc, base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing misclassifications in Training dataset...\n",
      "Model is on device: cuda:0\n",
      "\n",
      "Total samples to analyze: 1694\n",
      "\n",
      "Total misclassifications: 706\n",
      "Accuracy: 0.5832\n",
      "\n",
      "Misclassification distribution:\n",
      "predicted  0   1   2   5   7   8   9   13  14  15  16  17  18  19  20\n",
      "actual                                                               \n",
      "0           0   0   0   0   0   0   0   0   0  12   0   1   0   0   0\n",
      "1           6   0   0   0   0  16   1   0   0  17   0   7   0   0   0\n",
      "2           1   9   0   0   0  21   1   0   0  14   1   3   0   0   0\n",
      "3           1   0   0   0   1   0   0   0   0   0   0   0   0   0   0\n",
      "4           1   0   0   0   1   0   0   0   0   0   0   1   0   0   0\n",
      "5           5   0   0   0   1   0   0   1   0   3   0   0   0   0   0\n",
      "6          16   0   0   1   1   1   0   1   0   1   0   0   0   0   0\n",
      "7          22   0   1   8   0   1   1   3   0   5   0   0   1   0   0\n",
      "8           4   6   0   0   0   0   2   0   0  19   0  12   0   1   1\n",
      "9           1  20   1   0   0  51   0   0   1  24   0   8   0   1   0\n",
      "10          1   5   0   0   0   4   0   0   0   7   0   0   0   0   0\n",
      "11         19   0   0   0   1   2   0   2   0   0   0   0   0   0   0\n",
      "12          0   0   0   0   0   0   0   0   0   2   0   0   0   0   0\n",
      "13          1   1   0   0   0   5   1   0   0   5   0   1   0   0   0\n",
      "14          1   7   0   0   0   8   1   0   0   6   0   1   0   0   0\n",
      "15         21   8   1   3   3  25   5   1   0   0   0  18   0   0   0\n",
      "16          0   2   0   0   0   1   0   0   0   7   0   0   0   0   0\n",
      "17          2  12   1   0   0  38   2   0   0  31   0   0   0   0   0\n",
      "18          2   0   0   0   0   0   0   1   0   0   0   0   0   0   0\n",
      "19          1  11   0   0   0  14   1   0   1  15   0   2   0   0   1\n",
      "20          2   7   0   0   0  18   1   0   0  20   0   7   0   0   0\n",
      "\n",
      "Sample misclassifications:\n",
      "\n",
      "Example 1:\n",
      "Text: ['putin', 'mass', 'hivpositive', 'prisoner', 'choose', 'go', 'meatgrinder', 'frontline', 'rather', 'rot', 'jail', 'med', 'putin', 'mass', 'hivpositive', 'prisoner', 'choose', 'go', 'meatgrinder', 'fro\n",
      "Predicted: 15, Actual: 20\n",
      "Confidence: 0.4323\n",
      "\n",
      "Example 2:\n",
      "Text: ['north', 'korea', 'kim', 'jong', 'un', 'putin', 'xi', 'meet', 'beijing', 'october', 'say', 'kremlin', 'russian', 'president', 'vladimir', 'putin', 'meet', 'china', 'xi', 'jinping', 'talk', 'beijing',\n",
      "Predicted: 15, Actual: 17\n",
      "Confidence: 0.5843\n",
      "\n",
      "Example 3:\n",
      "Text: ['bloomberg', 'donate', 'million', 'beyond', 'carbon', 'campaign', 'former', 'new', 'york', 'city', 'mayor', 'michael', 'bloomberg', 'week', 'announce', 'intent', 'spend', 'million', 'campaign', 'repl\n",
      "Predicted: 7, Actual: 15\n",
      "Confidence: 0.5136\n",
      "\n",
      "Example 4:\n",
      "Text: ['boris', 'johnson', 'demand', 'putin', 'step', 'back', 'brink', 'say', 'russian', 'invasion', 'ukraine', 'bloodily', 'resist', 'boris', 'johnson', 'demand', 'putin', 'step', 'back', 'brink', 'showdow\n",
      "Predicted: 1, Actual: 20\n",
      "Confidence: 0.3686\n",
      "\n",
      "Example 5:\n",
      "Text: ['taiwanese', 'view', 'trustworthy', 'majority', 'trust', 'security', 'commitment', 'island', 'survey', 'majority', 'taiwanese', 'consider', 'unite', 'state', 'trustworthy', 'country', 'despite', 'exp\n",
      "Predicted: 15, Actual: 9\n",
      "Confidence: 0.6927\n"
     ]
    }
   ],
   "source": [
    "# Debug misclassifications\n",
    "if choice == \"1\":\n",
    "    misclass_df = debug_misclassifications(df_normalized, model, tokenizer, label_mapping)\n",
    "elif choice == \"2\":\n",
    "    misclass_df = debug_misclassifications(df_normalized_ua, model, tokenizer, label_mapping)\n",
    "else:\n",
    "    misclass_df = debug_misclassifications(df_normalized_cc,model, tokenizer, label_mapping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
