# NLP Project
This repository contains code and data for the Natural Language Processing and Information Extraction project at Vienna University of Technology. The project includes Jupyter notebooks, python files, configuration files and various datasets for training and evaluation. The project addresses task 3 subtask 2 from the excercise description - narrative classification.

## Comments on addressing tasks
- What text features can be used to train a feature-based discriminative classifier?
    - TF-IDF, BoW and Multilingual Embeddings for SVM-method
- Compare the results of a feature-based classifier with those produced by a deep-learning model of your choice (e.g., a BERT-based model or a model from the LLAMA herd).
    - Compared SVM (feature based) vs. BERT and LLama
- How were the data annotated? How would you frame the task of propaganda detection?
    - Was addressed in the qualitative analyses, e.g. svm_results.ipynb ("What is not propaganda?" -> Other-Other class)
- How can you make your classifierâ€™s decisions explainable to users?
    - Try to foster understanding with qualitative analysis
- Do you find any lexical or linguistic patterns in general in how particular narratives are transmitted (e.g., subjunctive mood)?
    - Again: Qualitative analyis (Performed for BERT and SVM)
- Challenging Compare the use of masked language models, such as BERT, and autoregressive models, such as LLAMA, for the task of analysis of narratives in online news.
    - Method comparison is done in the management report

Conforming to best practices in producing code, modularisation was used and PEP8 guidelines applied.

## Prerequisites
- Python 3.x
- Jupyter Notebook
- VS Code (optional)

## Installation
1. Clone the repository:
    ```sh
    git clone https://github.com/yourusername/nlp-project.git
    cd nlp-project
    ```

2. Install the required Python packages:
    ```sh
    pip install -r code/requirements.txt
    ```

## Usage 
1. Open any of the Jupyter notebooks containing:
    - code/ms1_1.ipynb addressed milestone 1
    - code/dl_methods/bert/bert_analysis.ipynb or code/dl_methods/bert/bert_pretraining.ipynb for the BERT-results
    - code/dl_methods/llama/llama.ipynb for the LLama results
    - code/non_dl_methods/svm_results.ipynb for the findings using the Support Vector Machines
2. Follow the instructions in the notebook to run the NLP tasks.

### VS Code Workspace
You can use the provided VS Code workspace configuration for an enhanced development experience. Open `nlp_ms1.code-workspace` in VS Code.

## Structure

### `code` contains:
#### dl_methods: 
  - `.py` modules containing main logic and `.ipynb` notebooks for calling the modules and communicating results.
  - Subfolders:
    - `bert`
    - `llama`
#### models:
  - Auxiliary files generated by or used by models (e.g., `.npy` files for multilingual embeddings used by SVMs to avoid creating them anew in each run).
#### modules:
  - `.py` modules for data preprocessing used by all methods:
    - Loading data
    - Text segmentation and normalization
    - Train-test-split
    - (CoNLL-U conversion and logging utilities)
  - Subfolder:
    - `modules_svm`: Modules specific to the SVM method
#### non_dl_methods:
- **`keyword_matching`**:
  - A simple approach using rudimentary keyword matching. 
  - Counts the most used words in each narrative and matches them to the test set.
  - *Note*: This method has poor accuracy and is not useful due to the many classes and underrepresentation of some classes.
- **`svm-results.ipynb`**:
  - A Jupyter Notebook documenting the application of one-vs-rest SVM to the problem, including:
    - Training different methods
    - Quantitative and qualitative analysis
- **`svm.py`**:
  - The SVM implementation used for results in the notebook.
#### `outputs` contains:
- **`analysis`**:
  - missclassifications file for LLama model
#### `ms1_1` contains:
- A Jupyter Notebook documenting the main findings relevant for Milestone 1 of the project:
  - The data preprocessing pipeline in the notebook was translated into `.py` files in the `modules` folder.
#### df_normalized_xx.csv:
  - Datasets with normalized inputs for each topic and combined.

### `CoNLL` contains:
- CoNLL files representing the data in CoNLL format.
  - *Note*: These files are currently not used since SVM, BERT, and LLama use different input representations.

### `info` contains:
- The task description
- Label taxonomy files
- Additional information files (if any)

### `training_data_04_December_release` contains:
- The newest data release (articles and annotations) specific to the problem.
- Folders for different languages:
  - `BG` = Bulgarian
  - `EN` = English
  - `HI` = Hindi
  - `PT` = Portuguese

### Final presentation:
- Technical presentation about findings
### Management summary:
- Findings summary for a non-technical audience

---

## License
This project is licensed under the MIT License.

##
JonasKruse and KrOnAsk are the same person, something went wrong with the user merging of GitHub accounts
