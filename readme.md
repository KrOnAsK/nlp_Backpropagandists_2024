# NLP Project
This repository contains code and data for the Natural Language Processing and Information Extraction project at Vienna University of Technology. The project includes Jupyter notebooks, python files, configuration files and various datasets for training and evaluation. The project addresses task 3 subtask 2 from the excercise description - narrative classification.

## Comments on addressing tasks
- What text features can be used to train a feature-based discriminative classifier?
    - TF-IDF, BoW and Multilingual Embeddings for SVM-method
- Compare the results of a feature-based classifier with those produced by a deep-learning model of your choice (e.g., a BERT-based model or a model from the LLAMA herd).
    - Compared SVM (feature based) vs. BERT and LLama
- How were the data annotated? How would you frame the task of propaganda detection?
    - Was addressed in the qualitative analyses, e.g. svm_results.ipynb ("What is not propaganda?" -> Other-Other class)
- How can you make your classifier’s decisions explainable to users?
    - Try to foster understanding with qualitative analysis
- Do you find any lexical or linguistic patterns in general in how particular narratives are transmitted (e.g., subjunctive mood)?
    - Again: Qualitative analyis (Performed for BERT and SVM)
- Challenging Compare the use of masked language models, such as BERT, and autoregressive models, such as LLAMA, for the task of analysis of narratives in online news.
    - Method comparison is done in the management report

## Prerequisites
- Python 3.x
- Jupyter Notebook
- VS Code (optional)

## Installation
1. Clone the repository:
    ```sh
    git clone https://github.com/yourusername/nlp-project.git
    cd nlp-project
    ```

2. Install the required Python packages:
    ```sh
    pip install -r code/requirements.txt
    ```

## Usage 
1. Open any of the Jupyter notebooks containing:
    - code/ms1_1.ipynb addressed milestone 1
    - code/dl_methods/bert/bert_analysis.ipynb or code/dl_methods/bert/bert_pretraining.ipynb for the BERT-results
    - code/dl_methods/llama/llama.ipynb for the LLama results
    - code/non_dl_methods/svm_results.ipynb for the findings using the Support Vector Machines
2. Follow the instructions in the notebook to run the NLP tasks.

### VS Code Workspace
You can use the provided VS Code workspace configuration for an enhanced development experience. Open `nlp_ms1.code-workspace` in VS Code.

## Structure
"code" contains:
- dl_methods: .py modules containing main logic and .ipynb notebooks for calling the modules and communicating results with subfolders:
    - -> bert
    - -> llama
- models: auxiliary files generated by or used by models, e.g. .npy files for the multilingual embeddings used by SVMs to avoid creating them anew in each run
- modules: 
    - .py modules for data preprocessing that is used by all methods (loading data, text segmentation and normalization, train-test-split, conllu conversion and logging utilities)
    - -> modules_svm: Modules specific to the SVM method
The `training_data_16_October_release` directory contains training data for various languages. The `CoNLL` directory contains output files in CoNLL format.
- non_dl_methods 
    - -> keyword_matching: simple approach to the problem using very rudimentary keyword matching by counting the most used words in each narrative and then matching that to the test set where we count the most used words again and try to match those to the "train" wordcounts -> very bad accuracy and also not really usefull because of the many classes and the severe underrepresentation of some classes
    - -> svm-results.ipynb: JN documenting the application of one-vs-rest SVM to the problem, including training different methods and quantitative and qualitative analysis
    - -> svm.py svm implementation used for results in .ipynb
- outputs -> analysis: ZU WELCHEM MODELL GEHÖRT DAS?
- ms1_1: JN documenting the main findings relevant for milestone 1 of the project
    - The data preprocessing pipeline in the JN was translated into the .py files in the "modules" folder
- df_normalized_xx.csv -> Datasets with normalized inputs for each topic and combined
"CoNLL" contains:
- CoNLL files representing the data in CoNLL format (currently not used since the SVM, BERT and LLama use different input representations)
"info" contains:
- The task description, label taxonomy files and possibly additional information files
"training_data_04_December_release" contains:
- The newest data release (articles and annotations) specific to the problem with folders for different languages (BG = bulgarian, EN = English, HI = Hindi, PT = Portuguese)

PLACEHOLDER MANAGEMENT SUMMARY + PRÄSI

## Outlook
Status 15.12.2024: The project will be continued mainly by adjusting the existing solutions to better address the specific challenges of the problem, for example:
- Find ways to deal with the severe underrepresentation of many classes
- Use additional languages for training
- Identifying a model that deals with these challenges better than the currently used models

## License
This project is licensed under the MIT License.

##
JonasKruse and KrOnAsk are the same person, something went wrong with the user merging of GitHub accounts
